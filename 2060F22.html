<!doctype html>
<html>
<head>
<meta charset="utf-8">
<title>Math 1951: Elements of Linear Algebra, Section 5, Winter 2023</title>
</head>
	


<body>

<p>Math 2060: Elements of Linear Algebra, Section 3, Winter 2023</p>
	
	<hr>
	
	<p>Please find the syllabus, Zoom link for tutoring sessions, exam information on the <a href="https://canvas.du.edu/courses/143807">Canvas Website</a>.&nbsp;</p>
	<p>Please find the textbook and slides in the <a href="https://denveru-my.sharepoint.com/:f:/g/personal/fei_qi_du_edu/EnGlnWlydoJDrbQnOUE2eyABH5RE0E4dtIcs3-hACgfzJg?e=GdeR3K">OneDrive Foler</a>. </p>
<hr>
	<p>Lecture Diaries:</p>

	<details>
	<summary>Lecture 1 on 9/13/2022.</summary>
	<ul>
		<p><span>We finished One.I.1 and are now half way through One.I.2. In the next lecture we will finish One.I.2 and One.1.3, and hopefully start Two.1.1 as planned in the schedule.&nbsp;</span></p>
        <p>The notations and terminologies in this textbook is slightly different from those in other standard textbooks. For example, other textbooks usually uses the notation <img class="equation_image" title="\rho_3 \mapsto \rho_3 + 2 \rho_1" src="https://canvas.du.edu/equation_images/%255Crho_3%2520%255Cmapsto%2520%255Crho_3%2520%252B%25202%2520%255Crho_1?scale=1" alt="LaTeX: \rho_3 \mapsto \rho_3 + 2 \rho_1" data-equation-content="\rho_3 \mapsto \rho_3 + 2 \rho_1" data-ignore-a11y-check="" /> for the operation that changes Row 3 by Row 3 Plus Two Times Row 1. This textbook compactified the notation and simply used <img class="equation_image" title="2\rho_1 + \rho_3" src="https://canvas.du.edu/equation_images/2%255Crho_1%2520%252B%2520%255Crho_3?scale=1" alt="LaTeX: 2\rho_1 + \rho_3" data-equation-content="2\rho_1 + \rho_3" data-ignore-a11y-check="" />, replacing the the row in the second place.&nbsp;</p>
        <p>The textbook contains many more examples than what I can go over in class. Please definitely go over them before attempting the exercises. <span>Homework problems in One.I.1 are simply those ticked exercises (1.17, 1.19, 1.21, 1.22, 1.24, 1.27, 1.32). The quiz problem on next Tuesday will be chosen from the ticked problems.&nbsp;</span></p>
        <p><span>A few students did not come and claim the extra credit. I have very bad memories and need to keep notes to everything. So please shoot me an email preferably today to inform me your name.&nbsp;&nbsp;</span></p>
	</ul>
	</details>

	<details>
	<summary>Lecture 2 on 9/15/2023.</summary>
		<ul>
            <p>We finished everything in One.I and introduced the reduced echelon form in One.III. I introduced it ahead of time because it is how I normally solve linear systems. It takes time to get used to such manipulations. Thus, it seems better for initial exposure to happen as soon as possible.&nbsp;</p>
            <p>Do not feel bad if you are confused. We will have more class time to understand the reduced echelon forms further. You can also find lots of further examples in One.III of the textbook. The author's videos will help as well.</p>
            <p>Homework problems for today will be ticked exercises in One.I.2 and One.I.3. Although officially, the quiz can range over all homework problems, it should not be difficult for you to narrow the range further, as some problems are not suitable for in-class quizzes. You may finish the quiz with or without the reduced echelon form. It doesn't matter that much in Week 2.</p>
            <p>For the following lecture on Tuesday, we will first discuss linear geometry in One.II before going into One.III. I am not very happy with the organization in the textbook. So my organization will be slightly different. But please do interrupt me as what people did today. That will help all of us.&nbsp;</p>
            <p>Reminder: Please make sure you claim your extra credit in time before I forget. My memory is bad because I had a bad habit of staying up late at night in college. I did not feel much then, but I am paying for it now. So please also take care of your body and make sure you get enough sleep, which is extraordinarily important for mathematics.&nbsp;</p>
            <p>Update (9/19): The quiz on 9/20 will exclude Exercise 3.21 (which I did not cover in class) and 3.24 (it's easy, but to write down the arguments appropriately is too demanding for this class)</p>
		</ul>
	</details>
	
	
	<details>
	<summary>Lecture 3 on 9/20/2022</summary>
		<ul>
            <p><span>We finished One.III.1 and are half-way through One.III.2. Homework for today will be the ticked exercises in One.III.1. </span><br /><br /><span>Note that we haven't started talking about One.II.1 Linear Geometry yet (though I said we would in the previous diary). I feel it better suited for the next lecture, before introducing the abstract definition of vector spaces in Two.I.1. That will bring some geometric intuitions to the abstract definition of vector spaces. </span></p>
        </ul>
	</details>

	<details><summary>Lecture 4 on 9/22/2023</summary>
		<ul>
            <p>Today we finished the discussion of One.III.2 and One.II.1. We justified the claim that two matrices are equivalent if and only if they have the same reduced echelon form, which follows from the theorem stating that each matrix is row equivalent to a UNIQUE reduced echelon form. I explained the proof of the theorem for 2-by-2 matrices by exhausting all possible reduced echelon forms. This explanation is not in the textbook. I am doing this because generally when you understand something on 2-by-2 matrices, you can generalize it to m-by-n matrices with induction. Since induction is not a prerequisite for this class, we will not cover the proofs in general. But you should understand the proof for 2-by-2 matrices.&nbsp;</p>
            <p>My organization and examples for One.II.1 are different from the textbook. Due to pedagogical concerns, Dr. Hefferon used more complicated and difficult-to-draw examples. But the content should be the same. I also planned to say something about One.II.2 but did not do so due to the limitation of time. I used the last five minutes going over the definition of a vector space in Two.I.1. Next lecture, I will repeat this definition before going over the examples in Two.I.1.</p>
            <p>Homework for this week will be ticked exercises in One.II.1, One.III.1 and One.III.2.</p>
            <p>Many thanks to Ms. Avarie Faulkner and Mr. Nicholas Wright, who pointed out mistakes in my board work. Many thanks also to Mr. Kolton Lee, who pointed out that my correction to the plane equation in class was still wrong. The correct equation is 4x + 8y + 5z = 20.&nbsp;</p>
        </ul>
	</details>
	
	<details><summary>Lecture 5 on 9/27/2022</summary>
	<ul>
		<p><span>We finished discussing Two.I.1. The section is very theoretical, but the verifications are straightforward. The ticked exercises at the end of the subsection will help you with further details. You are now ready to attempt them.</span></p>
        <p><span>The axiomatic formulation of vector spaces is lengthy. Still, you may feel its power in the lemma we showed at the end of the class: Once a set with two operations satisfies the ten axioms, all the lemmas deduced from the axioms automatically hold without further justifications. Philosophically, axiomatization is a <em>metaphysical </em>approach, contrary to <em>phenomenal</em>&nbsp;approach that focus more on the direct experience. Both approaches are important in science and engineering</span></p>
        <p><span>We are currently one lecture behind schedule. I will have to accelerate a little bit in the following lectures. But please do not hesitate to interrupt me if you are confused. It is my first time teaching this class. I will also need to learn from you how to teach the topics better. By interrupting me and reporting confusion, you are doing a great favor to both your peers and me.</span></p>
        <p><span>If you did not do well in the first quiz, please remember that only seven of your best quizzes will count in the final grading computation. If you work hard and make significant improvements, your low grades in the quiz last week will disappear.</span></p>
        <p><span>Please also don't hesitate to report any issues in your grading and ask for extra appointments if you cannot make it to my office hours. Typically I am busy on Tuesdays and Thursdays until 4 PM. I am booked for today from 4-5 PM to 6-7 PM. Tomorrow my schedule is open. </span><span>I prefer in-person meetings since that will allow me to see how you work and quickly identify your issues. I can meet on Zoom, but preferably you can write on an iPad or under a document camera (you may improvise one using a phone rack and your cell phone). </span></p>
    </ul></details>
	
	<details><summary>Lecture 6 on 9/29/2022</summary>
	<ul>
		<p><span>We finished Section 2.1.2 and started Section 2.2.1. The midterm next week will cover materials up to today's lecture. In particular, you may be asked to show that a set of vectors are linearly independent (since we did it in class today). Here are the related problems in the ticked exercises: 1.21(c), 1.22(a)(b)(c). They may appear in the quiz and the midterm next week, as well as the ticked problems in 2.1.1 and 2.1.2.</span></p>
        <p><span>We are currently 1.5 lectures behind schedule. Lab 3 also involves linear dependence and basis. I have asked your TA to introduce these topics during the labs. To give you some heads-up, a <strong>basis </strong>of a vector space V is just a sequence of vectors <img class="equation_image" title="\overrightarrow{e_1}, ..., \overrightarrow{e_n}" src="https://canvas.du.edu/equation_images/%255Coverrightarrow%257Be_1%257D%252C%2520...%252C%2520%255Coverrightarrow%257Be_n%257D?scale=1" alt="LaTeX: \overrightarrow{e_1}, ..., \overrightarrow{e_n}" data-equation-content="\overrightarrow{e_1}, ..., \overrightarrow{e_n}" data-ignore-a11y-check="" /> that are linearly independent and spanning V. We saw from today's lecture that V is spanned by <img class="equation_image" title="\{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" src="https://canvas.du.edu/equation_images/%255C%257B%255Coverrightarrow%257Be_1%257D%252C%2520...%252C%2520%255Coverrightarrow%257Be_n%257D%255C%257D?scale=1" alt="LaTeX: \{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" data-equation-content="\{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" data-ignore-a11y-check="" /> means every vector in V is a linear combination of <img class="equation_image" title="\overrightarrow{e_1}, ..., \overrightarrow{e_n}" src="https://canvas.du.edu/equation_images/%255Coverrightarrow%257Be_1%257D%252C%2520...%252C%2520%255Coverrightarrow%257Be_n%257D?scale=1" alt="LaTeX: \overrightarrow{e_1}, ..., \overrightarrow{e_n}" data-equation-content="\overrightarrow{e_1}, ..., \overrightarrow{e_n}" data-ignore-a11y-check="" />. We will see in the next lecture that if, in addition, <img class="equation_image" title="\{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" src="https://canvas.du.edu/equation_images/%255C%257B%255Coverrightarrow%257Be_1%257D%252C%2520...%252C%2520%255Coverrightarrow%257Be_n%257D%255C%257D?scale=1" alt="LaTeX: \{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" data-equation-content="\{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" data-ignore-a11y-check="" /> is linearly independent, then every vector in V is a <strong>UNIQUE linear combination</strong> of <img class="equation_image" title="\overrightarrow{e_1}, ..., \overrightarrow{e_n}" src="https://canvas.du.edu/equation_images/%255Coverrightarrow%257Be_1%257D%252C%2520...%252C%2520%255Coverrightarrow%257Be_n%257D?scale=1" alt="LaTeX: \overrightarrow{e_1}, ..., \overrightarrow{e_n}" data-equation-content="\overrightarrow{e_1}, ..., \overrightarrow{e_n}" data-ignore-a11y-check="" />. In other words, for every fixed vector <img class="equation_image" title="\overrightarrow{v}" src="https://canvas.du.edu/equation_images/%255Coverrightarrow%257Bv%257D?scale=1" alt="LaTeX: \overrightarrow{v}" data-equation-content="\overrightarrow{v}" data-ignore-a11y-check="" />, there exists UNIQUE numbers <img class="equation_image" title="x_1, ..., x_n\in \mathbb{R}" src="https://canvas.du.edu/equation_images/x_1%252C%2520...%252C%2520x_n%255Cin%2520%255Cmathbb%257BR%257D?scale=1" alt="LaTeX: x_1, ..., x_n\in \mathbb{R}" data-equation-content="x_1, ..., x_n\in \mathbb{R}" data-ignore-a11y-check="" />, such that <img class="equation_image" title="\overrightarrow{v} = x_1 \overrightarrow{e_1} + \cdots + x_n \overrightarrow{e_n}" src="https://canvas.du.edu/equation_images/%255Coverrightarrow%257Bv%257D%2520%253D%2520x_1%2520%255Coverrightarrow%257Be_1%257D%2520%252B%2520%255Ccdots%2520%252B%2520x_n%2520%255Coverrightarrow%257Be_n%257D?scale=1" alt="LaTeX: \overrightarrow{v} = x_1 \overrightarrow{e_1} + \cdots + x_n \overrightarrow{e_n}" data-equation-content="\overrightarrow{v} = x_1 \overrightarrow{e_1} + \cdots + x_n \overrightarrow{e_n}" data-ignore-a11y-check="" />. The numbers <img class="equation_image" title="x_1, ..., x_n" src="https://canvas.du.edu/equation_images/x_1%252C%2520...%252C%2520x_n?scale=1" alt="LaTeX: x_1, ..., x_n" data-equation-content="x_1, ..., x_n" data-ignore-a11y-check="" /> will be called&nbsp;<strong>coordinates</strong> of the vector <img class="equation_image" title="\overrightarrow{v}" src="https://canvas.du.edu/equation_images/%255Coverrightarrow%257Bv%257D?scale=1" alt="LaTeX: \overrightarrow{v}" data-equation-content="\overrightarrow{v}" data-ignore-a11y-check="" /> with respect to the basis <img class="equation_image" title="\{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" src="https://canvas.du.edu/equation_images/%255C%257B%255Coverrightarrow%257Be_1%257D%252C%2520...%252C%2520%255Coverrightarrow%257Be_n%257D%255C%257D?scale=1" alt="LaTeX: \{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" data-equation-content="\{\overrightarrow{e_1}, ..., \overrightarrow{e_n}\}" data-ignore-a11y-check="" />. Hefferon's textbook also uses the terminology of <strong>representation </strong>to describe the column vector consisting of the numbers <img class="equation_image" title="x_1, ..., x_n" src="https://canvas.du.edu/equation_images/x_1%252C%2520...%252C%2520x_n?scale=1" alt="LaTeX: x_1, ..., x_n" data-equation-content="x_1, ..., x_n" data-ignore-a11y-check="" />.&nbsp;</span></p>
        <p><span>The lectures next week will be more straightforward. I will spend less time discussing proofs and more time discussing examples.</span></p>
        <p><span>The core skill to be tested in Midterm 1 will be solving linear systems. Please make sure you are entirely comfortable in solving them. Many problems in linear algebra are essentially linear system problems, such as linear relations between vectors. You will need to know how to obtain a linear system from the context.</span></p>
        <p><span>I will announce more details once I finish drafting the midterm. Most likely, you will see an announcement by Saturday night. That should allow you sufficient time to study.&nbsp;</span></p>
        <p>If you feel that you are falling behind, please do not hesitate to ask for help. I can also accommodate weekend meetings.&nbsp;</p>
    </ul>
	</details>
	
	<details><summary>Lecture 7 on 10/04/2022</summary>
	<ul>
        <p>Today I tried to correct some mistakes in my teaching of the previous lectures while making many new mistakes.&nbsp;</p>
        <ol style="list-style-type: upper-roman;">
            <li>To solve a linear system using the Gauss-Jordan method, you need to&nbsp;
                <ol>
                    <li>Form the augmented matrix.</li>
                    <li>Use row operations to get an echelon form and figure out the leading variables.<br />2.1. If there exists a row looking like [0 0 ... 0 a] where a is a nonzero number, i.e., all but the last entry is zero, and the last entry is nonzero, then the system has no solution. <br />2.2. If such a row does not exist, proceed with Step 3.</li>
                    <li>Use more row operations to get the reduced echelon form by "pivoting" the leading variables. This means you should eliminate all the nonzero entries in the column of leading variables.</li>
                    <li>Read each row and express the leading variable in terms of the constant and the free variables.</li>
                    <li>Form the solution vector, then separate the terms according to the free variables to get the parameterized general solution.</li>
                </ol>
                In the previous classes, I combined Steps 4 and 5 and read the general solution directly from the reduced echelon form. Although this method is fast, it is too confusing and requires too much practice to clear the confusion. I now consider it wrong to teach this method.<br /><br />
            </li>
            <li><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">If a linear system is homogeneous, then you should know the following:</span>
                <ol>
                    <li style="list-style-type: none;">
                        <ol>
                            <li>You do not need to solve the system using the augmented matrix. Just perform the Gauss-Jordan method to the coefficient matrix. In Step 4, when you read the rows, write down the left-hand side as usual and set the right-hand side as 0. In Step 5, we simply ignore the particular solution [0; 0; ... 0].</li>
                            <li>The system has a unique solution if and only if all variables are leading variables. Equivalently, there does not exist any free variable. If the number of equations is the same as the number of variables, then the coefficient matrix is a square matrix that is also nonsingular. In the future, we will see that saying a square matrix to be nonsingular is equivalent to saying the matrix to be invertible and equivalent to saying that the determinant of the matrix is nonzero.</li>
                            <li>The set of solutions of a homogeneous system forms a vector space. <br />We proved this using matrix notation. However, I did not place enough emphasis on the matrix notation. Let me briefly recall the knowledge here.&nbsp;
                                <ol style="list-style-type: lower-alpha;">
                                    <li><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">Recall: The action (or multiplication) of a 1\times n matrix on (or by) an n-dimensional column vector is defined by the following formula<img class="equation_image" title="\begin{pmatrix}
                                        a_1 &amp; a_2 &amp; \cdots &amp; a_n
                                        \end{pmatrix}
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}&nbsp;
                                        = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250Aa_1%2520%2526%2520a_2%2520%2526%2520%255Ccdots%2520%2526%2520a_n%250A%255Cend%257Bpmatrix%257D%250A%255Cbegin%257Bpmatrix%257D%250Ax_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%250A%255Cend%257Bpmatrix%257D%25C2%25A0%250A%253D%2520a_1%2520x_1%2520%252B%2520a_2%2520x_2%2520%252B%2520%255Ccdots%2520%252B%2520a_n%2520x_n?scale=1" alt="LaTeX: \begin{pmatrix}
                                        a_1 &amp; a_2 &amp; \cdots &amp; a_n
                                        \end{pmatrix}
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}&nbsp;
                                        = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n" data-equation-content="\begin{pmatrix}
                                        a_1 &amp; a_2 &amp; \cdots &amp; a_n
                                        \end{pmatrix}
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}&nbsp;
                                        = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n" data-ignore-a11y-check="" /><br />The result is one single number. </span></li>
                                                                    <li>Recall: The action (or multiplication) of a m\times n matrix on (or by) an n-dimensional column vector is defined by the following formula<img class="equation_image" title="\begin{pmatrix}
                                        
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}&nbsp;
                                        =&nbsp;
                                        \begin{pmatrix}
                                        a_{11}x_1 + a_{12}x_2 +\cdots  +a_{1n}x_n \\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots + a_{mn}x_n
                                        \end{pmatrix}&nbsp;" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%255C%255C%250A%255Cend%257Bpmatrix%257D%250A%255Cbegin%257Bpmatrix%257D%250Ax_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%250A%255Cend%257Bpmatrix%257D%25C2%25A0%250A%253D%25C2%25A0%250A%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257Dx_1%2520%252B%2520a_%257B12%257Dx_2%2520%252B%255Ccdots%2520%2520%252Ba_%257B1n%257Dx_n%2520%255C%255C%25C2%25A0%250Aa_%257B21%257Dx_1%2520%252B%2520a_%257B22%257Dx_2%2520%252B%255Ccdots%2520%252B%2520a_%257B2n%257Dx_n%2520%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Aa_%257Bm1%257Dx_1%2520%252B%2520a_%257Bm2%257Dx_2%2520%252B%255Ccdots%2520%252B%2520a_%257Bmn%257Dx_n%250A%255Cend%257Bpmatrix%257D%25C2%25A0?scale=1" alt="LaTeX: \begin{pmatrix}
                                        
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}&nbsp;
                                        =&nbsp;
                                        \begin{pmatrix}
                                        a_{11}x_1 + a_{12}x_2 +\cdots  +a_{1n}x_n \\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots + a_{mn}x_n
                                        \end{pmatrix}&nbsp;" data-equation-content="\begin{pmatrix}
                                        
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}&nbsp;
                                        =&nbsp;
                                        \begin{pmatrix}
                                        a_{11}x_1 + a_{12}x_2 +\cdots  +a_{1n}x_n \\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots + a_{mn}x_n
                                        \end{pmatrix}&nbsp;" data-ignore-a11y-check="" /><br />Equivalently, we are multiplying each row of the matrix by the column vector. The multiplication of m rows gives m numbers. The right-hand side of the formula is simply the column vector formed by these m numbers.</li>
                                                                    <li>A linear system&nbsp;<br /><img class="equation_image" title="\left\{
                                        \begin{aligned}
                                        a_{11}x_1 + a_{12}x_2 +\cdots &amp; a_{1n}x_n = d_1\\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots &amp; a_{2n}x_n = d_2\\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots &amp; a_{mn}x_n = d_m
                                        \end{aligned}\right." src="https://canvas.du.edu/equation_images/%255Cleft%255C%257B%250A%255Cbegin%257Baligned%257D%250Aa_%257B11%257Dx_1%2520%252B%2520a_%257B12%257Dx_2%2520%252B%255Ccdots%2520%2526%2520a_%257B1n%257Dx_n%2520%253D%2520d_1%255C%255C%25C2%25A0%250Aa_%257B21%257Dx_1%2520%252B%2520a_%257B22%257Dx_2%2520%252B%255Ccdots%2520%2526%2520a_%257B2n%257Dx_n%2520%253D%2520d_2%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Aa_%257Bm1%257Dx_1%2520%252B%2520a_%257Bm2%257Dx_2%2520%252B%255Ccdots%2520%2526%2520a_%257Bmn%257Dx_n%2520%253D%2520d_m%250A%255Cend%257Baligned%257D%255Cright.?scale=1" alt="LaTeX: \left\{
                                        \begin{aligned}
                                        a_{11}x_1 + a_{12}x_2 +\cdots &amp; a_{1n}x_n = d_1\\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots &amp; a_{2n}x_n = d_2\\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots &amp; a_{mn}x_n = d_m
                                        \end{aligned}\right." data-equation-content="\left\{
                                        \begin{aligned}
                                        a_{11}x_1 + a_{12}x_2 +\cdots &amp; a_{1n}x_n = d_1\\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots &amp; a_{2n}x_n = d_2\\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots &amp; a_{mn}x_n = d_m
                                        \end{aligned}\right." data-ignore-a11y-check="" /><br />can be abbreviated as <img class="equation_image" title="A\vec{x} = \vec{d}" src="https://canvas.du.edu/equation_images/A%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A\vec{x} = \vec{d}" data-equation-content="A\vec{x} = \vec{d}" data-ignore-a11y-check="" />, where&nbsp;<br /><img class="equation_image" title="A = \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} = \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \vec{d}= \begin{pmatrix}
                                        d_1 \\ d_2 \\ \vdots \\ d_m \end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/A%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%255C%255C%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250Ax_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bd%257D%253D%2520%255Cbegin%257Bpmatrix%257D%250Ad_1%2520%255C%255C%2520d_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520d_m%2520%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: A = \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} = \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \vec{d}= \begin{pmatrix}
                                        d_1 \\ d_2 \\ \vdots \\ d_m \end{pmatrix}.&nbsp;" data-equation-content="A = \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} = \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \vec{d}= \begin{pmatrix}
                                        d_1 \\ d_2 \\ \vdots \\ d_m \end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /></li>
                                                                    <li><strong>(I forgot to emphasize in class that)</strong> The matrix multiplication is <strong>distributive </strong>with respect to vectors, meaning that&nbsp;<br /><img class="equation_image" title="A (\vec{x}+ \vec{y}) = A\vec{x} + A\vec{y}.&nbsp;" src="https://canvas.du.edu/equation_images/A%2520(%255Cvec%257Bx%257D%252B%2520%255Cvec%257By%257D)%2520%253D%2520A%255Cvec%257Bx%257D%2520%252B%2520A%255Cvec%257By%257D.%25C2%25A0?scale=1" alt="LaTeX: A (\vec{x}+ \vec{y}) = A\vec{x} + A\vec{y}.&nbsp;" data-equation-content="A (\vec{x}+ \vec{y}) = A\vec{x} + A\vec{y}.&nbsp;" data-ignore-a11y-check="" /><br />This is indeed easy to verify: Let <img class="equation_image" title="A = \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} = \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \vec{y}= \begin{pmatrix}
                                        y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/A%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%255C%255C%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250Ax_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257By%257D%253D%2520%255Cbegin%257Bpmatrix%257D%250Ay_1%2520%255C%255C%2520y_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520y_n%2520%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: A = \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} = \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \vec{y}= \begin{pmatrix}
                                        y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}.&nbsp;" data-equation-content="A = \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} = \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \vec{y}= \begin{pmatrix}
                                        y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /><br />Then&nbsp;<br /><img class="equation_image" title="\begin{aligned}
                                        A(\vec{x}+\vec{y}) &amp;=&nbsp;
                                        &nbsp;\begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix} \begin{pmatrix}
                                        x_1+y_1 \\ x_2+y_2 \\ \vdots \\ x_n+y_n \end{pmatrix} =&nbsp;\begin{pmatrix}
                                        a_{11}(x_1+y_1) + a_{12}(x_2+y_2) +\cdots + a_{1n}(x_n+y_n) \\&nbsp;
                                        a_{21}(x_1+y_1) + a_{22}(x_2+y_2) +\cdots + a_{2n}(x_n+y_n) \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}(x_1+y_1) + a_{m2}(x_2+y_2) +\cdots + a_{mn}(x_n+y_n)
                                        \end{pmatrix}\\
                                        &amp;=
                                        \begin{pmatrix}
                                        a_{11}x_1+a_{11}y_1 + a_{12}x_2+a_{12}y_2 +\cdots + a_{1n}x_n+a_{1n}y_n \\&nbsp;
                                        a_{21}x_1+a_{21}y_1 + a_{22}x_2+a_{22}y_2 +\cdots + a_{2n}x_n+a_{2n}y_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1+a_{m1}y_1 + a_{m2}x_2+a_{m2}y_2) +\cdots + a_{mn}x_n+a_{mn}y_n
                                        \end{pmatrix}\\
                                        &amp;=
                                        \begin{pmatrix}
                                        a_{11}x_1 + a_{12}x_2 +\cdots + a_{1n}x_n \\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots + a_{mn}x_n&nbsp;
                                        \end{pmatrix}+\begin{pmatrix}
                                        a_{11}y_1 + a_{12}y_2 +\cdots +a_{1n}y_n \\&nbsp;
                                        a_{21}y_1 + a_{22}y_2 +\cdots + a_{2n}y_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}y_1 + a_{m2}y_2 +\cdots + a_{mn}y_n&nbsp;
                                        \end{pmatrix}
                                        = A\vec{x} + A\vec{y}.&nbsp;
                                        \end{aligned}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Baligned%257D%250AA(%255Cvec%257Bx%257D%252B%255Cvec%257By%257D)%2520%2526%253D%25C2%25A0%250A%25C2%25A0%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%255C%255C%250A%255Cend%257Bpmatrix%257D%2520%255Cbegin%257Bpmatrix%257D%250Ax_1%252By_1%2520%255C%255C%2520x_2%252By_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%252By_n%2520%255Cend%257Bpmatrix%257D%2520%253D%25C2%25A0%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D(x_1%252By_1)%2520%252B%2520a_%257B12%257D(x_2%252By_2)%2520%252B%255Ccdots%2520%252B%2520a_%257B1n%257D(x_n%252By_n)%2520%255C%255C%25C2%25A0%250Aa_%257B21%257D(x_1%252By_1)%2520%252B%2520a_%257B22%257D(x_2%252By_2)%2520%252B%255Ccdots%2520%252B%2520a_%257B2n%257D(x_n%252By_n)%2520%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Aa_%257Bm1%257D(x_1%252By_1)%2520%252B%2520a_%257Bm2%257D(x_2%252By_2)%2520%252B%255Ccdots%2520%252B%2520a_%257Bmn%257D(x_n%252By_n)%250A%255Cend%257Bpmatrix%257D%255C%255C%250A%2526%253D%250A%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257Dx_1%252Ba_%257B11%257Dy_1%2520%252B%2520a_%257B12%257Dx_2%252Ba_%257B12%257Dy_2%2520%252B%255Ccdots%2520%252B%2520a_%257B1n%257Dx_n%252Ba_%257B1n%257Dy_n%2520%255C%255C%25C2%25A0%250Aa_%257B21%257Dx_1%252Ba_%257B21%257Dy_1%2520%252B%2520a_%257B22%257Dx_2%252Ba_%257B22%257Dy_2%2520%252B%255Ccdots%2520%252B%2520a_%257B2n%257Dx_n%252Ba_%257B2n%257Dy_n%2520%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Aa_%257Bm1%257Dx_1%252Ba_%257Bm1%257Dy_1%2520%252B%2520a_%257Bm2%257Dx_2%252Ba_%257Bm2%257Dy_2)%2520%252B%255Ccdots%2520%252B%2520a_%257Bmn%257Dx_n%252Ba_%257Bmn%257Dy_n%250A%255Cend%257Bpmatrix%257D%255C%255C%250A%2526%253D%250A%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257Dx_1%2520%252B%2520a_%257B12%257Dx_2%2520%252B%255Ccdots%2520%252B%2520a_%257B1n%257Dx_n%2520%255C%255C%25C2%25A0%250Aa_%257B21%257Dx_1%2520%252B%2520a_%257B22%257Dx_2%2520%252B%255Ccdots%2520%252B%2520a_%257B2n%257Dx_n%2520%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Aa_%257Bm1%257Dx_1%2520%252B%2520a_%257Bm2%257Dx_2%2520%252B%255Ccdots%2520%252B%2520a_%257Bmn%257Dx_n%25C2%25A0%250A%255Cend%257Bpmatrix%257D%252B%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257Dy_1%2520%252B%2520a_%257B12%257Dy_2%2520%252B%255Ccdots%2520%252Ba_%257B1n%257Dy_n%2520%255C%255C%25C2%25A0%250Aa_%257B21%257Dy_1%2520%252B%2520a_%257B22%257Dy_2%2520%252B%255Ccdots%2520%252B%2520a_%257B2n%257Dy_n%2520%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Aa_%257Bm1%257Dy_1%2520%252B%2520a_%257Bm2%257Dy_2%2520%252B%255Ccdots%2520%252B%2520a_%257Bmn%257Dy_n%25C2%25A0%250A%255Cend%257Bpmatrix%257D%250A%253D%2520A%255Cvec%257Bx%257D%2520%252B%2520A%255Cvec%257By%257D.%25C2%25A0%250A%255Cend%257Baligned%257D?scale=1" alt="LaTeX: \begin{aligned}
                                        A(\vec{x}+\vec{y}) &amp;=&nbsp;
                                        &nbsp;\begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix} \begin{pmatrix}
                                        x_1+y_1 \\ x_2+y_2 \\ \vdots \\ x_n+y_n \end{pmatrix} =&nbsp;\begin{pmatrix}
                                        a_{11}(x_1+y_1) + a_{12}(x_2+y_2) +\cdots + a_{1n}(x_n+y_n) \\&nbsp;
                                        a_{21}(x_1+y_1) + a_{22}(x_2+y_2) +\cdots + a_{2n}(x_n+y_n) \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}(x_1+y_1) + a_{m2}(x_2+y_2) +\cdots + a_{mn}(x_n+y_n)
                                        \end{pmatrix}\\
                                        &amp;=
                                        \begin{pmatrix}
                                        a_{11}x_1+a_{11}y_1 + a_{12}x_2+a_{12}y_2 +\cdots + a_{1n}x_n+a_{1n}y_n \\&nbsp;
                                        a_{21}x_1+a_{21}y_1 + a_{22}x_2+a_{22}y_2 +\cdots + a_{2n}x_n+a_{2n}y_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1+a_{m1}y_1 + a_{m2}x_2+a_{m2}y_2) +\cdots + a_{mn}x_n+a_{mn}y_n
                                        \end{pmatrix}\\
                                        &amp;=
                                        \begin{pmatrix}
                                        a_{11}x_1 + a_{12}x_2 +\cdots + a_{1n}x_n \\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots + a_{mn}x_n&nbsp;
                                        \end{pmatrix}+\begin{pmatrix}
                                        a_{11}y_1 + a_{12}y_2 +\cdots +a_{1n}y_n \\&nbsp;
                                        a_{21}y_1 + a_{22}y_2 +\cdots + a_{2n}y_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}y_1 + a_{m2}y_2 +\cdots + a_{mn}y_n&nbsp;
                                        \end{pmatrix}
                                        = A\vec{x} + A\vec{y}.&nbsp;
                                        \end{aligned}" data-equation-content="\begin{aligned}
                                        A(\vec{x}+\vec{y}) &amp;=&nbsp;
                                        &nbsp;\begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix} \begin{pmatrix}
                                        x_1+y_1 \\ x_2+y_2 \\ \vdots \\ x_n+y_n \end{pmatrix} =&nbsp;\begin{pmatrix}
                                        a_{11}(x_1+y_1) + a_{12}(x_2+y_2) +\cdots + a_{1n}(x_n+y_n) \\&nbsp;
                                        a_{21}(x_1+y_1) + a_{22}(x_2+y_2) +\cdots + a_{2n}(x_n+y_n) \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}(x_1+y_1) + a_{m2}(x_2+y_2) +\cdots + a_{mn}(x_n+y_n)
                                        \end{pmatrix}\\
                                        &amp;=
                                        \begin{pmatrix}
                                        a_{11}x_1+a_{11}y_1 + a_{12}x_2+a_{12}y_2 +\cdots + a_{1n}x_n+a_{1n}y_n \\&nbsp;
                                        a_{21}x_1+a_{21}y_1 + a_{22}x_2+a_{22}y_2 +\cdots + a_{2n}x_n+a_{2n}y_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1+a_{m1}y_1 + a_{m2}x_2+a_{m2}y_2) +\cdots + a_{mn}x_n+a_{mn}y_n
                                        \end{pmatrix}\\
                                        &amp;=
                                        \begin{pmatrix}
                                        a_{11}x_1 + a_{12}x_2 +\cdots + a_{1n}x_n \\&nbsp;
                                        a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}x_1 + a_{m2}x_2 +\cdots + a_{mn}x_n&nbsp;
                                        \end{pmatrix}+\begin{pmatrix}
                                        a_{11}y_1 + a_{12}y_2 +\cdots +a_{1n}y_n \\&nbsp;
                                        a_{21}y_1 + a_{22}y_2 +\cdots + a_{2n}y_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1}y_1 + a_{m2}y_2 +\cdots + a_{mn}y_n&nbsp;
                                        \end{pmatrix}
                                        = A\vec{x} + A\vec{y}.&nbsp;
                                        \end{aligned}" data-ignore-a11y-check="" /></li>
                                                                    <li><strong>(I forgot to emphasize in class that)</strong> The matrix multiplication <strong>commutes with scalar multiples</strong>, i.e., for every <img class="equation_image" title=" r\in \mathbb{R}," src="https://canvas.du.edu/equation_images/%2520r%255Cin%2520%255Cmathbb%257BR%257D%252C?scale=1" alt="LaTeX:  r\in \mathbb{R}," data-equation-content=" r\in \mathbb{R}," data-ignore-a11y-check="" />&nbsp;<br /><img class="equation_image" title=" A (r\cdot \vec{x}) = r \cdot (A \vec{x}). " src="https://canvas.du.edu/equation_images/%2520A%2520(r%255Ccdot%2520%255Cvec%257Bx%257D)%2520%253D%2520r%2520%255Ccdot%2520(A%2520%255Cvec%257Bx%257D).%2520?scale=1" alt="LaTeX:  A (r\cdot \vec{x}) = r \cdot (A \vec{x}). " data-equation-content=" A (r\cdot \vec{x}) = r \cdot (A \vec{x}). " data-ignore-a11y-check="" /><br />This is also easy. <br />Let <img class="equation_image" title="A=\begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} =&nbsp;
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/A%253D%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%255C%255C%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bx%257D%2520%253D%25C2%25A0%250A%255Cbegin%257Bpmatrix%257D%250Ax_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%250A%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: A=\begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} =&nbsp;
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}.&nbsp;" data-equation-content="A=\begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix}, \vec{x} =&nbsp;
                                        \begin{pmatrix}
                                        x_1 \\ x_2 \\ \vdots \\ x_n
                                        \end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /><br />Then&nbsp;<br /><img class="equation_image" title="\begin{aligned}
                                        A(r\cdot x) &amp;= \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix} \begin{pmatrix}
                                        rx_1 \\ rx_2 \\ \vdots \\ rx_n
                                        \end{pmatrix}&nbsp;
                                        = \begin{pmatrix}
                                        a_{11} (rx_1) + a_{12} (rx_2) +\cdots + a_{1n} (rx_n)\\&nbsp;
                                        a_{21} (rx_1)+ a_{22} (rx_2) +\cdots + a_{2n} (rx_n)\\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1} (rx_1) + a_{m2} (rx_2) +\cdots + a_{mn} (rx_n)
                                        \end{pmatrix}
                                        \\
                                        &amp;=
                                        \begin{pmatrix}
                                        r a_{11}x_1 + r a_{12}x_2 +\cdots + r a_{1n}x_n \\&nbsp;
                                        r a_{21}x_1 + r a_{22}x_2 +\cdots +r a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        r a_{m1}x_1 + r a_{m2}x_2 +\cdots + r a_{mn}x_n&nbsp;
                                        \end{pmatrix}
                                        = r\cdot \begin{pmatrix}
                                        r a_{11}x_1 + r a_{12}x_2 +\cdots + r a_{1n}x_n \\&nbsp;
                                        r a_{21}x_1 + r a_{22}x_2 +\cdots +r a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        r a_{m1}x_1 + r a_{m2}x_2 +\cdots + r a_{mn}x_n&nbsp;
                                        \end{pmatrix} = r\cdot (A\vec{x})
                                        \end{aligned}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Baligned%257D%250AA(r%255Ccdot%2520x)%2520%2526%253D%2520%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%255C%255C%250A%255Cend%257Bpmatrix%257D%2520%255Cbegin%257Bpmatrix%257D%250Arx_1%2520%255C%255C%2520rx_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520rx_n%250A%255Cend%257Bpmatrix%257D%25C2%25A0%250A%253D%2520%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520(rx_1)%2520%252B%2520a_%257B12%257D%2520(rx_2)%2520%252B%255Ccdots%2520%252B%2520a_%257B1n%257D%2520(rx_n)%255C%255C%25C2%25A0%250Aa_%257B21%257D%2520(rx_1)%252B%2520a_%257B22%257D%2520(rx_2)%2520%252B%255Ccdots%2520%252B%2520a_%257B2n%257D%2520(rx_n)%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Aa_%257Bm1%257D%2520(rx_1)%2520%252B%2520a_%257Bm2%257D%2520(rx_2)%2520%252B%255Ccdots%2520%252B%2520a_%257Bmn%257D%2520(rx_n)%250A%255Cend%257Bpmatrix%257D%250A%255C%255C%250A%2526%253D%250A%255Cbegin%257Bpmatrix%257D%250Ar%2520a_%257B11%257Dx_1%2520%252B%2520r%2520a_%257B12%257Dx_2%2520%252B%255Ccdots%2520%252B%2520r%2520a_%257B1n%257Dx_n%2520%255C%255C%25C2%25A0%250Ar%2520a_%257B21%257Dx_1%2520%252B%2520r%2520a_%257B22%257Dx_2%2520%252B%255Ccdots%2520%252Br%2520a_%257B2n%257Dx_n%2520%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Ar%2520a_%257Bm1%257Dx_1%2520%252B%2520r%2520a_%257Bm2%257Dx_2%2520%252B%255Ccdots%2520%252B%2520r%2520a_%257Bmn%257Dx_n%25C2%25A0%250A%255Cend%257Bpmatrix%257D%250A%253D%2520r%255Ccdot%2520%255Cbegin%257Bpmatrix%257D%250Ar%2520a_%257B11%257Dx_1%2520%252B%2520r%2520a_%257B12%257Dx_2%2520%252B%255Ccdots%2520%252B%2520r%2520a_%257B1n%257Dx_n%2520%255C%255C%25C2%25A0%250Ar%2520a_%257B21%257Dx_1%2520%252B%2520r%2520a_%257B22%257Dx_2%2520%252B%255Ccdots%2520%252Br%2520a_%257B2n%257Dx_n%2520%255C%255C%25C2%25A0%250A%255Cvdots%2520%255C%255C%25C2%25A0%250Ar%2520a_%257Bm1%257Dx_1%2520%252B%2520r%2520a_%257Bm2%257Dx_2%2520%252B%255Ccdots%2520%252B%2520r%2520a_%257Bmn%257Dx_n%25C2%25A0%250A%255Cend%257Bpmatrix%257D%2520%253D%2520r%255Ccdot%2520(A%255Cvec%257Bx%257D)%250A%255Cend%257Baligned%257D?scale=1" alt="LaTeX: \begin{aligned}
                                        A(r\cdot x) &amp;= \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix} \begin{pmatrix}
                                        rx_1 \\ rx_2 \\ \vdots \\ rx_n
                                        \end{pmatrix}&nbsp;
                                        = \begin{pmatrix}
                                        a_{11} (rx_1) + a_{12} (rx_2) +\cdots + a_{1n} (rx_n)\\&nbsp;
                                        a_{21} (rx_1)+ a_{22} (rx_2) +\cdots + a_{2n} (rx_n)\\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1} (rx_1) + a_{m2} (rx_2) +\cdots + a_{mn} (rx_n)
                                        \end{pmatrix}
                                        \\
                                        &amp;=
                                        \begin{pmatrix}
                                        r a_{11}x_1 + r a_{12}x_2 +\cdots + r a_{1n}x_n \\&nbsp;
                                        r a_{21}x_1 + r a_{22}x_2 +\cdots +r a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        r a_{m1}x_1 + r a_{m2}x_2 +\cdots + r a_{mn}x_n&nbsp;
                                        \end{pmatrix}
                                        = r\cdot \begin{pmatrix}
                                        r a_{11}x_1 + r a_{12}x_2 +\cdots + r a_{1n}x_n \\&nbsp;
                                        r a_{21}x_1 + r a_{22}x_2 +\cdots +r a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        r a_{m1}x_1 + r a_{m2}x_2 +\cdots + r a_{mn}x_n&nbsp;
                                        \end{pmatrix} = r\cdot (A\vec{x})
                                        \end{aligned}" data-equation-content="\begin{aligned}
                                        A(r\cdot x) &amp;= \begin{pmatrix}
                                        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
                                        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
                                        \vdots &amp; \vdots &amp; &amp; \vdots \\
                                        a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
                                        \end{pmatrix} \begin{pmatrix}
                                        rx_1 \\ rx_2 \\ \vdots \\ rx_n
                                        \end{pmatrix}&nbsp;
                                        = \begin{pmatrix}
                                        a_{11} (rx_1) + a_{12} (rx_2) +\cdots + a_{1n} (rx_n)\\&nbsp;
                                        a_{21} (rx_1)+ a_{22} (rx_2) +\cdots + a_{2n} (rx_n)\\&nbsp;
                                        \vdots \\&nbsp;
                                        a_{m1} (rx_1) + a_{m2} (rx_2) +\cdots + a_{mn} (rx_n)
                                        \end{pmatrix}
                                        \\
                                        &amp;=
                                        \begin{pmatrix}
                                        r a_{11}x_1 + r a_{12}x_2 +\cdots + r a_{1n}x_n \\&nbsp;
                                        r a_{21}x_1 + r a_{22}x_2 +\cdots +r a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        r a_{m1}x_1 + r a_{m2}x_2 +\cdots + r a_{mn}x_n&nbsp;
                                        \end{pmatrix}
                                        = r\cdot \begin{pmatrix}
                                        r a_{11}x_1 + r a_{12}x_2 +\cdots + r a_{1n}x_n \\&nbsp;
                                        r a_{21}x_1 + r a_{22}x_2 +\cdots +r a_{2n}x_n \\&nbsp;
                                        \vdots \\&nbsp;
                                        r a_{m1}x_1 + r a_{m2}x_2 +\cdots + r a_{mn}x_n&nbsp;
                                        \end{pmatrix} = r\cdot (A\vec{x})
                                        \end{aligned}" data-ignore-a11y-check="" /></li>
                                    <li>We are ready to show our claim using these properties. We want to show that the solutions of the equation<img class="equation_image" title=" A \vec{x} = \vec{0} " src="https://canvas.du.edu/equation_images/%2520A%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257B0%257D%2520?scale=1" alt="LaTeX:  A \vec{x} = \vec{0} " data-equation-content=" A \vec{x} = \vec{0} " data-ignore-a11y-check="" /> form a vector space. Equivalently, we wish to show that the subset <img class="equation_image" title="W = \left\{ \left.\vec{x} \in \mathbb{R}^n\right| A \vec{x} = \vec{0}\right\}" src="https://canvas.du.edu/equation_images/W%2520%253D%2520%255Cleft%255C%257B%2520%255Cleft.%255Cvec%257Bx%257D%2520%255Cin%2520%255Cmathbb%257BR%257D%255En%255Cright%257C%2520A%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257B0%257D%255Cright%255C%257D?scale=1" alt="LaTeX: W = \left\{ \left.\vec{x} \in \mathbb{R}^n\right| A \vec{x} = \vec{0}\right\}" data-equation-content="W = \left\{ \left.\vec{x} \in \mathbb{R}^n\right| A \vec{x} = \vec{0}\right\}" data-ignore-a11y-check="" />&nbsp;of <img class="equation_image" title=" \mathbb{R}^n " src="https://canvas.du.edu/equation_images/%2520%255Cmathbb%257BR%257D%255En%2520?scale=1" alt="LaTeX:  \mathbb{R}^n " data-equation-content=" \mathbb{R}^n " data-ignore-a11y-check="" /> is a vector subspace. <br />Using the lemma we discussed last Thursday, we only need to show that the subset W is closed under linear combinations of pairs of vectors.<br />In other words, we wish to show that for every <img class="equation_image" title="r_1, r_2\in \mathbb{R}" src="https://canvas.du.edu/equation_images/r_1%252C%2520r_2%255Cin%2520%255Cmathbb%257BR%257D?scale=1" alt="LaTeX: r_1, r_2\in \mathbb{R}" data-equation-content="r_1, r_2\in \mathbb{R}" data-ignore-a11y-check="" />, every <img class="equation_image" title="\vec{v_1}, \vec{v_2}\in W, r_1 \vec{v_1} + r_2 \vec{v_2} \in W" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv_1%257D%252C%2520%255Cvec%257Bv_2%257D%255Cin%2520W%252C%2520r_1%2520%255Cvec%257Bv_1%257D%2520%252B%2520r_2%2520%255Cvec%257Bv_2%257D%2520%255Cin%2520W?scale=1" alt="LaTeX: \vec{v_1}, \vec{v_2}\in W, r_1 \vec{v_1} + r_2 \vec{v_2} \in W" data-equation-content="\vec{v_1}, \vec{v_2}\in W, r_1 \vec{v_1} + r_2 \vec{v_2} \in W" data-ignore-a11y-check="" />,<br />i.e., <img class="equation_image" title="A(r_1\cdot \vec{v_1} + r_2 \cdot \vec{v_2}) = \vec{0}." src="https://canvas.du.edu/equation_images/A(r_1%255Ccdot%2520%255Cvec%257Bv_1%257D%2520%252B%2520r_2%2520%255Ccdot%2520%255Cvec%257Bv_2%257D)%2520%253D%2520%255Cvec%257B0%257D.?scale=1" alt="LaTeX: A(r_1\cdot \vec{v_1} + r_2 \cdot \vec{v_2}) = \vec{0}." data-equation-content="A(r_1\cdot \vec{v_1} + r_2 \cdot \vec{v_2}) = \vec{0}." data-ignore-a11y-check="" />. <br />From c and d, we see that&nbsp;<br /><img class="equation_image" title="A(r_1 \vec{v_1} + r_2 \vec{v_2}) = A(r_1 \cdot \vec{v_1}) + A (r_2 \cdot \vec{v_2}) = r_1 (A \vec{v_1}) + r_2 (A \vec{v_2}).&nbsp;" src="https://canvas.du.edu/equation_images/A(r_1%2520%255Cvec%257Bv_1%257D%2520%252B%2520r_2%2520%255Cvec%257Bv_2%257D)%2520%253D%2520A(r_1%2520%255Ccdot%2520%255Cvec%257Bv_1%257D)%2520%252B%2520A%2520(r_2%2520%255Ccdot%2520%255Cvec%257Bv_2%257D)%2520%253D%2520r_1%2520(A%2520%255Cvec%257Bv_1%257D)%2520%252B%2520r_2%2520(A%2520%255Cvec%257Bv_2%257D).%25C2%25A0?scale=1" alt="LaTeX: A(r_1 \vec{v_1} + r_2 \vec{v_2}) = A(r_1 \cdot \vec{v_1}) + A (r_2 \cdot \vec{v_2}) = r_1 (A \vec{v_1}) + r_2 (A \vec{v_2}).&nbsp;" data-equation-content="A(r_1 \vec{v_1} + r_2 \vec{v_2}) = A(r_1 \cdot \vec{v_1}) + A (r_2 \cdot \vec{v_2}) = r_1 (A \vec{v_1}) + r_2 (A \vec{v_2}).&nbsp;" data-ignore-a11y-check="" /><br />Note that <img class="equation_image" title=" \vec{v_1}\in W " src="https://canvas.du.edu/equation_images/%2520%255Cvec%257Bv_1%257D%255Cin%2520W%2520?scale=1" alt="LaTeX:  \vec{v_1}\in W " data-equation-content=" \vec{v_1}\in W " data-ignore-a11y-check="" /> means <img class="equation_image" title="A \vec{v_1} = \vec{0}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bv_1%257D%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: A \vec{v_1} = \vec{0}" data-equation-content="A \vec{v_1} = \vec{0}" data-ignore-a11y-check="" />, <img class="equation_image" title="\vec{v_2} \in W" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv_2%257D%2520%255Cin%2520W?scale=1" alt="LaTeX: \vec{v_2} \in W" data-equation-content="\vec{v_2} \in W" data-ignore-a11y-check="" /> means <img class="equation_image" title="A \vec{v_2} = \vec{0}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bv_2%257D%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: A \vec{v_2} = \vec{0}" data-equation-content="A \vec{v_2} = \vec{0}" data-ignore-a11y-check="" />. Thus <br /><img class="equation_image" title="A(r_1 \vec{v_1} + r_2 \vec{v_2}) = r_1 \cdot \vec{0} + r_2 \cdot \vec{0} = \vec{0}.&nbsp;" src="https://canvas.du.edu/equation_images/A(r_1%2520%255Cvec%257Bv_1%257D%2520%252B%2520r_2%2520%255Cvec%257Bv_2%257D)%2520%253D%2520r_1%2520%255Ccdot%2520%255Cvec%257B0%257D%2520%252B%2520r_2%2520%255Ccdot%2520%255Cvec%257B0%257D%2520%253D%2520%255Cvec%257B0%257D.%25C2%25A0?scale=1" alt="LaTeX: A(r_1 \vec{v_1} + r_2 \vec{v_2}) = r_1 \cdot \vec{0} + r_2 \cdot \vec{0} = \vec{0}.&nbsp;" data-equation-content="A(r_1 \vec{v_1} + r_2 \vec{v_2}) = r_1 \cdot \vec{0} + r_2 \cdot \vec{0} = \vec{0}.&nbsp;" data-ignore-a11y-check="" /><br />Thus <img class="equation_image" title="r_1 \vec{v_1} + r_2 \vec{v_2}" src="https://canvas.du.edu/equation_images/r_1%2520%255Cvec%257Bv_1%257D%2520%252B%2520r_2%2520%255Cvec%257Bv_2%257D?scale=1" alt="LaTeX: r_1 \vec{v_1} + r_2 \vec{v_2}" data-equation-content="r_1 \vec{v_1} + r_2 \vec{v_2}" data-ignore-a11y-check="" /> is an element in W, which is precisely what we want.</li>
                                </ol>
                            </li>
                            <li>The solutions to a nonhomogeneous linear system do NOT form a vector space. In other words, the set of solutions to <img class="equation_image" title="A \vec{x} = \vec{d}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A \vec{x} = \vec{d}" data-equation-content="A \vec{x} = \vec{d}" data-ignore-a11y-check="" /> fails to be a vector space if <img class="equation_image" title="\vec{d}\neq \vec{0}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bd%257D%255Cneq%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: \vec{d}\neq \vec{0}" data-equation-content="\vec{d}\neq \vec{0}" data-ignore-a11y-check="" />. <br />The shortest proof you may expect is to check that the solutions are not closed under scalar multiplication. In other words, let <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" /> be a solution to the system <img class="equation_image" title="A \vec{x} = \vec{d}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A \vec{x} = \vec{d}" data-equation-content="A \vec{x} = \vec{d}" data-ignore-a11y-check="" />, we want to show <img class="equation_image" title="r\cdot \vec{v}" src="https://canvas.du.edu/equation_images/r%255Ccdot%2520%255Cvec%257Bv%257D?scale=1" alt="LaTeX: r\cdot \vec{v}" data-equation-content="r\cdot \vec{v}" data-ignore-a11y-check="" /> is generally not a solution of the system. <br />We argue using 3.e: <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" /> is a solution of<img class="equation_image" title=" A\vec{x} = \vec{d} " src="https://canvas.du.edu/equation_images/%2520A%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D%2520?scale=1" alt="LaTeX:  A\vec{x} = \vec{d} " data-equation-content=" A\vec{x} = \vec{d} " data-ignore-a11y-check="" /> means <img class="equation_image" title="A \vec{v} = \vec{d}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bv%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A \vec{v} = \vec{d}" data-equation-content="A \vec{v} = \vec{d}" data-ignore-a11y-check="" />. Multiply both sides by the scalar r, we see that <img class="equation_image" title=" r \cdot (A\vec{v}) = r\vec{d}" src="https://canvas.du.edu/equation_images/%2520r%2520%255Ccdot%2520(A%255Cvec%257Bv%257D)%2520%253D%2520r%255Cvec%257Bd%257D?scale=1" alt="LaTeX:  r \cdot (A\vec{v}) = r\vec{d}" data-equation-content=" r \cdot (A\vec{v}) = r\vec{d}" data-ignore-a11y-check="" />. <br />But the left-hand-side, by 3.e, is precisely <img class="equation_image" title="A(r\cdot \vec{v})" src="https://canvas.du.edu/equation_images/A(r%255Ccdot%2520%255Cvec%257Bv%257D)?scale=1" alt="LaTeX: A(r\cdot \vec{v})" data-equation-content="A(r\cdot \vec{v})" data-ignore-a11y-check="" />. And the right-hand-side equals to <img class="equation_image" title="\vec{d}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bd%257D?scale=1" alt="LaTeX: \vec{d}" data-equation-content="\vec{d}" data-ignore-a11y-check="" /> only when <img class="equation_image" title="r=1" src="https://canvas.du.edu/equation_images/r%253D1?scale=1" alt="LaTeX: r=1" data-equation-content="r=1" data-ignore-a11y-check="" />. <br />So we showed that for every <img class="equation_image" title="r\neq 1" src="https://canvas.du.edu/equation_images/r%255Cneq%25201?scale=1" alt="LaTeX: r\neq 1" data-equation-content="r\neq 1" data-ignore-a11y-check="" />, <img class="equation_image" title="A (r\cdot \vec{v}) \neq \vec{d}" src="https://canvas.du.edu/equation_images/A%2520(r%255Ccdot%2520%255Cvec%257Bv%257D)%2520%255Cneq%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A (r\cdot \vec{v}) \neq \vec{d}" data-equation-content="A (r\cdot \vec{v}) \neq \vec{d}" data-ignore-a11y-check="" />. In other words, <img class="equation_image" title="r\cdot \vec{v}" src="https://canvas.du.edu/equation_images/r%255Ccdot%2520%255Cvec%257Bv%257D?scale=1" alt="LaTeX: r\cdot \vec{v}" data-equation-content="r\cdot \vec{v}" data-ignore-a11y-check="" /> is not a solution to <img class="equation_image" title="A\vec{x} = \vec{d}" src="https://canvas.du.edu/equation_images/A%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A\vec{x} = \vec{d}" data-equation-content="A\vec{x} = \vec{d}" data-ignore-a11y-check="" />. <br />So the solution set of <img class="equation_image" title="A\vec{x} = \vec{d}" src="https://canvas.du.edu/equation_images/A%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A\vec{x} = \vec{d}" data-equation-content="A\vec{x} = \vec{d}" data-ignore-a11y-check="" /> is not closed under scalar multiplication.<br /><br /></li>
                        </ol>
                    </li>
                </ol>
            </li>
            <li>After discussing this proof, we recall the definition of the span of a subset S (of an ambient vector space V), which is the set of linear combinations of vectors in the subset S. <br />In formal notations: <img class="equation_image" title="Span S = [S] = \{c_1 \vec{s_1} + c_2 \vec{s_2} + \cdots + c_n \vec{s_n} | c_1, ..., c_n \in \mathbb{R}, \vec{s_1}, ..., \vec{s_n}\in S\}" src="https://canvas.du.edu/equation_images/Span%2520S%2520%253D%2520%255BS%255D%2520%253D%2520%255C%257Bc_1%2520%255Cvec%257Bs_1%257D%2520%252B%2520c_2%2520%255Cvec%257Bs_2%257D%2520%252B%2520%255Ccdots%2520%252B%2520c_n%2520%255Cvec%257Bs_n%257D%2520%257C%2520c_1%252C%2520...%252C%2520c_n%2520%255Cin%2520%255Cmathbb%257BR%257D%252C%2520%255Cvec%257Bs_1%257D%252C%2520...%252C%2520%255Cvec%257Bs_n%257D%255Cin%2520S%255C%257D?scale=1" alt="LaTeX: Span S = [S] = \{c_1 \vec{s_1} + c_2 \vec{s_2} + \cdots + c_n \vec{s_n} | c_1, ..., c_n \in \mathbb{R}, \vec{s_1}, ..., \vec{s_n}\in S\}" data-equation-content="Span S = [S] = \{c_1 \vec{s_1} + c_2 \vec{s_2} + \cdots + c_n \vec{s_n} | c_1, ..., c_n \in \mathbb{R}, \vec{s_1}, ..., \vec{s_n}\in S\}" data-ignore-a11y-check="" /><br />Remark: Please make sure you distinguish the subsets and subspaces.
                <ul style="list-style-type: square;">
                    <li>A subset of a vector space can be any collection of vectors, while a subspace must be a set of vectors that is closed under linear combinations.</li>
                    <li>A subset can consist of any number of vectors, while a nonzero subspace always contains infinitely many vectors. Only the trivial subspace can consist of finitely many vectors (only one vector, <img class="equation_image" title="\vec{0}" src="https://canvas.du.edu/equation_images/%255Cvec%257B0%257D?scale=1" alt="LaTeX: \vec{0}" data-equation-content="\vec{0}" data-ignore-a11y-check="" />).</li>
                </ul>
                <p>Key facts regarding span that you should keep in mind:&nbsp;</p>
                <ol>
                    <li>Span S is always a subspace of V. This follows from the linear combination lemma.</li>
                    <li>(<strong>I forgot to emphasize in class</strong>) An alternative way to check if a subset (of a vector space) is a subspace is to express the subset as the span of something.&nbsp;<br />For example, you may check that the set of 2 x 2 matrices (which is a subset of the vector space consisting of 2 x 2 matrices)&nbsp;<br /><img class="equation_image" title="\left\{\left.\begin{pmatrix}
                    a &amp; b \\
                    a &amp; b
                    \end{pmatrix}\right| a, b\in \mathbb{R}\right\}" src="https://canvas.du.edu/equation_images/%255Cleft%255C%257B%255Cleft.%255Cbegin%257Bpmatrix%257D%250Aa%2520%2526%2520b%2520%255C%255C%250Aa%2520%2526%2520b%250A%255Cend%257Bpmatrix%257D%255Cright%257C%2520a%252C%2520b%255Cin%2520%255Cmathbb%257BR%257D%255Cright%255C%257D?scale=1" alt="LaTeX: \left\{\left.\begin{pmatrix}
                    a &amp; b \\
                    a &amp; b
                    \end{pmatrix}\right| a, b\in \mathbb{R}\right\}" data-equation-content="\left\{\left.\begin{pmatrix}
                    a &amp; b \\
                    a &amp; b
                    \end{pmatrix}\right| a, b\in \mathbb{R}\right\}" data-ignore-a11y-check="" /><br />is a vector space by parameterizing the set as<br /><img class="equation_image" title="\left\{\left.a \begin{pmatrix}
                    1 &amp; 0 \\
                    1 &amp; 0
                    \end{pmatrix} + b \begin{pmatrix}
                    0 &amp; 1 \\
                    0 &amp; 1
                    \end{pmatrix} \right| a, b\in \mathbb{R}\right\}" src="https://canvas.du.edu/equation_images/%255Cleft%255C%257B%255Cleft.a%2520%255Cbegin%257Bpmatrix%257D%250A1%2520%2526%25200%2520%255C%255C%250A1%2520%2526%25200%250A%255Cend%257Bpmatrix%257D%2520%252B%2520b%2520%255Cbegin%257Bpmatrix%257D%250A0%2520%2526%25201%2520%255C%255C%250A0%2520%2526%25201%250A%255Cend%257Bpmatrix%257D%2520%255Cright%257C%2520a%252C%2520b%255Cin%2520%255Cmathbb%257BR%257D%255Cright%255C%257D?scale=1" alt="LaTeX: \left\{\left.a \begin{pmatrix}
                    1 &amp; 0 \\
                    1 &amp; 0
                    \end{pmatrix} + b \begin{pmatrix}
                    0 &amp; 1 \\
                    0 &amp; 1
                    \end{pmatrix} \right| a, b\in \mathbb{R}\right\}" data-equation-content="\left\{\left.a \begin{pmatrix}
                    1 &amp; 0 \\
                    1 &amp; 0
                    \end{pmatrix} + b \begin{pmatrix}
                    0 &amp; 1 \\
                    0 &amp; 1
                    \end{pmatrix} \right| a, b\in \mathbb{R}\right\}" data-ignore-a11y-check="" /><br />Thus the subset is the span of <img class="equation_image" title="\left\{\begin{pmatrix}
                    1 &amp; 0 \\
                    1 &amp; 0
                    \end{pmatrix}, \begin{pmatrix}
                    0 &amp; 1 \\
                    0 &amp; 1
                    \end{pmatrix}\right\}" src="https://canvas.du.edu/equation_images/%255Cleft%255C%257B%255Cbegin%257Bpmatrix%257D%250A1%2520%2526%25200%2520%255C%255C%250A1%2520%2526%25200%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cbegin%257Bpmatrix%257D%250A0%2520%2526%25201%2520%255C%255C%250A0%2520%2526%25201%250A%255Cend%257Bpmatrix%257D%255Cright%255C%257D?scale=1" alt="LaTeX: \left\{\begin{pmatrix}
                    1 &amp; 0 \\
                    1 &amp; 0
                    \end{pmatrix}, \begin{pmatrix}
                    0 &amp; 1 \\
                    0 &amp; 1
                    \end{pmatrix}\right\}" data-equation-content="\left\{\begin{pmatrix}
                    1 &amp; 0 \\
                    1 &amp; 0
                    \end{pmatrix}, \begin{pmatrix}
                    0 &amp; 1 \\
                    0 &amp; 1
                    \end{pmatrix}\right\}" data-ignore-a11y-check="" />, which is a subspace. <br />Another example: Let V be the space of polynomials of degree at most 2. Let W be the set of polynomials <br /><img class="equation_image" title=" \{a + ax + ax^2 | a\in \mathbb{R}\}. " src="https://canvas.du.edu/equation_images/%2520%255C%257Ba%2520%252B%2520ax%2520%252B%2520ax%255E2%2520%257C%2520a%255Cin%2520%255Cmathbb%257BR%257D%255C%257D.%2520?scale=1" alt="LaTeX:  \{a + ax + ax^2 | a\in \mathbb{R}\}. " data-equation-content=" \{a + ax + ax^2 | a\in \mathbb{R}\}. " data-ignore-a11y-check="" /><br />Then W admits the parametrization&nbsp;<br /><img class="equation_image" title="W = \{a (1+x+x^2) | a\in \mathbb{R}\}&nbsp;." src="https://canvas.du.edu/equation_images/W%2520%253D%2520%255C%257Ba%2520(1%252Bx%252Bx%255E2)%2520%257C%2520a%255Cin%2520%255Cmathbb%257BR%257D%255C%257D%25C2%25A0.?scale=1" alt="LaTeX: W = \{a (1+x+x^2) | a\in \mathbb{R}\}&nbsp;." data-equation-content="W = \{a (1+x+x^2) | a\in \mathbb{R}\}&nbsp;." data-ignore-a11y-check="" /><br />and is thus equal to the span of <img class="equation_image" title="\{1+x+x^2\}" src="https://canvas.du.edu/equation_images/%255C%257B1%252Bx%252Bx%255E2%255C%257D?scale=1" alt="LaTeX: \{1+x+x^2\}" data-equation-content="\{1+x+x^2\}" data-ignore-a11y-check="" />. So W is a vector space. <br />A counter-example: Let V be the space of polynomials of degree at most 2. Let U be the set of polynomials of the form <br /><img class="equation_image" title="\{1 + ax + ax^2 | a \in \mathbb{R}\}." src="https://canvas.du.edu/equation_images/%255C%257B1%2520%252B%2520ax%2520%252B%2520ax%255E2%2520%257C%2520a%2520%255Cin%2520%255Cmathbb%257BR%257D%255C%257D.?scale=1" alt="LaTeX: \{1 + ax + ax^2 | a \in \mathbb{R}\}." data-equation-content="\{1 + ax + ax^2 | a \in \mathbb{R}\}." data-ignore-a11y-check="" />&nbsp;<br />Though U can be parametrized as <br /><img class="equation_image" title=" \{1 + a(1+x^2) | a\in \mathbb{R}\}, " src="https://canvas.du.edu/equation_images/%2520%255C%257B1%2520%252B%2520a(1%252Bx%255E2)%2520%257C%2520a%255Cin%2520%255Cmathbb%257BR%257D%255C%257D%252C%2520?scale=1" alt="LaTeX:  \{1 + a(1+x^2) | a\in \mathbb{R}\}, " data-equation-content=" \{1 + a(1+x^2) | a\in \mathbb{R}\}, " data-ignore-a11y-check="" /><br />U cannot be expressed as the span of any collection of vectors. <br />Indeed, you may check that U is not closed under scalar multiplication: <img class="equation_image" title="r + ra(1+x^2)" src="https://canvas.du.edu/equation_images/r%2520%252B%2520ra(1%252Bx%255E2)?scale=1" alt="LaTeX: r + ra(1+x^2)" data-equation-content="r + ra(1+x^2)" data-ignore-a11y-check="" /> is of form <img class="equation_image" title="1 + b(1+x^2)" src="https://canvas.du.edu/equation_images/1%2520%252B%2520b(1%252Bx%255E2)?scale=1" alt="LaTeX: 1 + b(1+x^2)" data-equation-content="1 + b(1+x^2)" data-ignore-a11y-check="" /> only when <img class="equation_image" title=" r = 1" src="https://canvas.du.edu/equation_images/%2520r%2520%253D%25201?scale=1" alt="LaTeX:  r = 1" data-equation-content=" r = 1" data-ignore-a11y-check="" />.</li>
                <li>To check if a given vector <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" /> is in the span of a given set S, we set up a linear system and see if it has solutions. <br />I did two examples in class:&nbsp;<br />(a) <img class="equation_image" title="\vec{v} = \begin{pmatrix}
        7 \\ 8 \\ 9
        \end{pmatrix},&nbsp;
        S = \left\{\begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix}, \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}\right\}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A7%2520%255C%255C%25208%2520%255C%255C%25209%250A%255Cend%257Bpmatrix%257D%252C%25C2%25A0%250AS%2520%253D%2520%255Cleft%255C%257B%255Cbegin%257Bpmatrix%257D%250A1%2520%255C%255C%25202%2520%255C%255C%25203%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cbegin%257Bpmatrix%257D%250A4%2520%255C%255C%25205%2520%255C%255C%25206%250A%255Cend%257Bpmatrix%257D%255Cright%255C%257D?scale=1" alt="LaTeX: \vec{v} = \begin{pmatrix}
        7 \\ 8 \\ 9
        \end{pmatrix},&nbsp;
        S = \left\{\begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix}, \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}\right\}" data-equation-content="\vec{v} = \begin{pmatrix}
        7 \\ 8 \\ 9
        \end{pmatrix},&nbsp;
        S = \left\{\begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix}, \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}\right\}" data-ignore-a11y-check="" /><br />You may solve the equation <img class="equation_image" title="\begin{pmatrix}
        7 \\ 8 \\ 9
        \end{pmatrix} = c_1 \begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix} + c_2 \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A7%2520%255C%255C%25208%2520%255C%255C%25209%250A%255Cend%257Bpmatrix%257D%2520%253D%2520c_1%2520%255Cbegin%257Bpmatrix%257D%250A1%2520%255C%255C%25202%2520%255C%255C%25203%250A%255Cend%257Bpmatrix%257D%2520%252B%2520c_2%2520%255Cbegin%257Bpmatrix%257D%250A4%2520%255C%255C%25205%2520%255C%255C%25206%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix}
        7 \\ 8 \\ 9
        \end{pmatrix} = c_1 \begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix} + c_2 \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}" data-equation-content="\begin{pmatrix}
        7 \\ 8 \\ 9
        \end{pmatrix} = c_1 \begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix} + c_2 \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}" data-ignore-a11y-check="" />and find that <img class="equation_image" title="c_1 = -1, c_2 = 2" src="https://canvas.du.edu/equation_images/c_1%2520%253D%2520-1%252C%2520c_2%2520%253D%25202?scale=1" alt="LaTeX: c_1 = -1, c_2 = 2" data-equation-content="c_1 = -1, c_2 = 2" data-ignore-a11y-check="" />.<br />(b) <img class="equation_image" title="\vec{v} = \begin{pmatrix}
        7 \\ 8 \\ 10
        \end{pmatrix},&nbsp;
        S = \left\{\begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix}, \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}\right\}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A7%2520%255C%255C%25208%2520%255C%255C%252010%250A%255Cend%257Bpmatrix%257D%252C%25C2%25A0%250AS%2520%253D%2520%255Cleft%255C%257B%255Cbegin%257Bpmatrix%257D%250A1%2520%255C%255C%25202%2520%255C%255C%25203%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cbegin%257Bpmatrix%257D%250A4%2520%255C%255C%25205%2520%255C%255C%25206%250A%255Cend%257Bpmatrix%257D%255Cright%255C%257D?scale=1" alt="LaTeX: \vec{v} = \begin{pmatrix}
        7 \\ 8 \\ 10
        \end{pmatrix},&nbsp;
        S = \left\{\begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix}, \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}\right\}" data-equation-content="\vec{v} = \begin{pmatrix}
        7 \\ 8 \\ 10
        \end{pmatrix},&nbsp;
        S = \left\{\begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix}, \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}\right\}" data-ignore-a11y-check="" /><br />You may solve the equation <img class="equation_image" title="\begin{pmatrix}
        7 \\ 8 \\ 10
        \end{pmatrix} = c_1 \begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix} + c_2 \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A7%2520%255C%255C%25208%2520%255C%255C%252010%250A%255Cend%257Bpmatrix%257D%2520%253D%2520c_1%2520%255Cbegin%257Bpmatrix%257D%250A1%2520%255C%255C%25202%2520%255C%255C%25203%250A%255Cend%257Bpmatrix%257D%2520%252B%2520c_2%2520%255Cbegin%257Bpmatrix%257D%250A4%2520%255C%255C%25205%2520%255C%255C%25206%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix}
        7 \\ 8 \\ 10
        \end{pmatrix} = c_1 \begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix} + c_2 \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}" data-equation-content="\begin{pmatrix}
        7 \\ 8 \\ 10
        \end{pmatrix} = c_1 \begin{pmatrix}
        1 \\ 2 \\ 3
        \end{pmatrix} + c_2 \begin{pmatrix}
        4 \\ 5 \\ 6
        \end{pmatrix}" data-ignore-a11y-check="" />and find that there exist no solutions for <img class="equation_image" title="c_1" src="https://canvas.du.edu/equation_images/c_1?scale=1" alt="LaTeX: c_1" data-equation-content="c_1" data-ignore-a11y-check="" /> and <img class="equation_image" title="c_2" src="https://canvas.du.edu/equation_images/c_2?scale=1" alt="LaTeX: c_2" data-equation-content="c_2" data-ignore-a11y-check="" />.&nbsp;<br /><br /></li>
                </ol>
            </li>
            <li style="list-style-type: upper-roman;">
                <p>Then we recall the definition of linear independence.&nbsp;<br />Intuitive definition: A subset S of a vector space V is linearly independent if any vector in S is not a linear combination of the others in S.&nbsp;<br />Rigorous definition: A subset S of a vector space V is linearly independent, if for any vector <img class="equation_image" title="\vec{s_1}, \vec{s_2}, ..., \vec{s_n}\in S" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D%252C%2520%255Cvec%257Bs_2%257D%252C%2520...%252C%2520%255Cvec%257Bs_n%257D%255Cin%2520S?scale=1" alt="LaTeX: \vec{s_1}, \vec{s_2}, ..., \vec{s_n}\in S" data-equation-content="\vec{s_1}, \vec{s_2}, ..., \vec{s_n}\in S" data-ignore-a11y-check="" />, the equation <br /><img class="equation_image" title="c_1 \vec{s_1} + c_2 \vec{s_2} + \cdots + c_n \vec{s_n} = \vec{0}" src="https://canvas.du.edu/equation_images/c_1%2520%255Cvec%257Bs_1%257D%2520%252B%2520c_2%2520%255Cvec%257Bs_2%257D%2520%252B%2520%255Ccdots%2520%252B%2520c_n%2520%255Cvec%257Bs_n%257D%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: c_1 \vec{s_1} + c_2 \vec{s_2} + \cdots + c_n \vec{s_n} = \vec{0}" data-equation-content="c_1 \vec{s_1} + c_2 \vec{s_2} + \cdots + c_n \vec{s_n} = \vec{0}" data-ignore-a11y-check="" /> <br />has the unique solution <img class="equation_image" title="c_1 = c_2 = \cdots = c_n = 0" src="https://canvas.du.edu/equation_images/c_1%2520%253D%2520c_2%2520%253D%2520%255Ccdots%2520%253D%2520c_n%2520%253D%25200?scale=1" alt="LaTeX: c_1 = c_2 = \cdots = c_n = 0" data-equation-content="c_1 = c_2 = \cdots = c_n = 0" data-ignore-a11y-check="" />.&nbsp;<br />Key facts to keep in mind:&nbsp;</p>
            </li>
        </ol>
        <ol>
            <li style="list-style-type: none;">
                <ol>
                    <li>If S is a finite set consisting of vectors <img class="equation_image" title="\vec{s_1}, ..., \vec{s_n}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D%252C%2520...%252C%2520%255Cvec%257Bs_n%257D?scale=1" alt="LaTeX: \vec{s_1}, ..., \vec{s_n}" data-equation-content="\vec{s_1}, ..., \vec{s_n}" data-ignore-a11y-check="" />, we can check the linear independence by studying the matrix formed by these (column) vectors.&nbsp;<br />1.1. We perform row operations to obtain an echelon form of the matrix. <br />&nbsp; &nbsp; &nbsp; &nbsp;<img class="equation_image" title="\vec{s_1}, ..., \vec{s_n}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D%252C%2520...%252C%2520%255Cvec%257Bs_n%257D?scale=1" alt="LaTeX: \vec{s_1}, ..., \vec{s_n}" data-equation-content="\vec{s_1}, ..., \vec{s_n}" data-ignore-a11y-check="" /> is linearly independent if and only if all variables are leading, equivalently, if there are no free variables.&nbsp;<br />1.2. If we want a linear relation between <img class="equation_image" title="\vec{s_1}, ...., \vec{s_n}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D%252C%2520....%252C%2520%255Cvec%257Bs_n%257D?scale=1" alt="LaTeX: \vec{s_1}, ...., \vec{s_n}" data-equation-content="\vec{s_1}, ...., \vec{s_n}" data-ignore-a11y-check="" />, then we need to solve the system and obtain a solution of <img class="equation_image" title="c_1, ..., c_n" src="https://canvas.du.edu/equation_images/c_1%252C%2520...%252C%2520c_n?scale=1" alt="LaTeX: c_1, ..., c_n" data-equation-content="c_1, ..., c_n" data-ignore-a11y-check="" /> that are not all zero.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Any such solution gives a linear relation. You may pick an arbitrary one.</li>
                    <li>The only new thing we went over today is the lemma and its corollaries:&nbsp;<br />2.1. The lemma states that adding a vector <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" /> into the spanning set S does not enlarge the span if and only if the vector is itself in the span. Formally, <img class="equation_image" title="[S \cup \{\vec{v}\}] = [S]" src="https://canvas.du.edu/equation_images/%255BS%2520%255Ccup%2520%255C%257B%255Cvec%257Bv%257D%255C%257D%255D%2520%253D%2520%255BS%255D?scale=1" alt="LaTeX: [S \cup \{\vec{v}\}] = [S]" data-equation-content="[S \cup \{\vec{v}\}] = [S]" data-ignore-a11y-check="" /> if and only <img class="equation_image" title="\vec{v} \in [S]" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%2520%255Cin%2520%255BS%255D?scale=1" alt="LaTeX: \vec{v} \in [S]" data-equation-content="\vec{v} \in [S]" data-ignore-a11y-check="" />.&nbsp;<br />2.2. Corollary: Removing a vector <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" /> from the spanning set S does not shrink the span [S] if and only if <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" /> is a linear combination of other elements in S. Formally, <img class="equation_image" title="[S \setminus \{\vec{v}\}] = [S]" src="https://canvas.du.edu/equation_images/%255BS%2520%255Csetminus%2520%255C%257B%255Cvec%257Bv%257D%255C%257D%255D%2520%253D%2520%255BS%255D?scale=1" alt="LaTeX: [S \setminus \{\vec{v}\}] = [S]" data-equation-content="[S \setminus \{\vec{v}\}] = [S]" data-ignore-a11y-check="" /> if and only if <img class="equation_image" title="\vec{v} \in [S \setminus\{\vec{v}\}]." src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%2520%255Cin%2520%255BS%2520%255Csetminus%255C%257B%255Cvec%257Bv%257D%255C%257D%255D.?scale=1" alt="LaTeX: \vec{v} \in [S \setminus\{\vec{v}\}]." data-equation-content="\vec{v} \in [S \setminus\{\vec{v}\}]." data-ignore-a11y-check="" />&nbsp;<br />Note: The textbook used the notation <img class="equation_image" title="S - \{\vec{v}\}" src="https://canvas.du.edu/equation_images/S%2520-%2520%255C%257B%255Cvec%257Bv%257D%255C%257D?scale=1" alt="LaTeX: S - \{\vec{v}\}" data-equation-content="S - \{\vec{v}\}" data-ignore-a11y-check="" /> to denote the subset of S that contains everything in S except for the vector <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" />. I personally prefer the backslash. But these two notations are both common.&nbsp;<br />2.3. Corollary (which is important). A set S is linearly independent if and only if for every <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\vec{v} \in [S]" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%2520%255Cin%2520%255BS%255D?scale=1" alt="LaTeX: \vec{v} \in [S]" data-equation-content="\vec{v} \in [S]" data-ignore-a11y-check="" />, removing <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" /> from the spanning set shrinks the span. Formally, for every <img class="equation_image" title="\vec{v}\in S, [S \setminus \{\vec{v}\}] \subsetneq [S]" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%255Cin%2520S%252C%2520%255BS%2520%255Csetminus%2520%255C%257B%255Cvec%257Bv%257D%255C%257D%255D%2520%255Csubsetneq%2520%255BS%255D?scale=1" alt="LaTeX: \vec{v}\in S, [S \setminus \{\vec{v}\}] \subsetneq [S]" data-equation-content="\vec{v}\in S, [S \setminus \{\vec{v}\}] \subsetneq [S]" data-ignore-a11y-check="" />.&nbsp;<br />2.4. (I will cover on Thursday) Corollary: If S is linearly independent and <img class="equation_image" title="\vec{v} \notin S" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%2520%255Cnotin%2520S?scale=1" alt="LaTeX: \vec{v} \notin S" data-equation-content="\vec{v} \notin S" data-ignore-a11y-check="" />, then <img class="equation_image" title="S \cup\{\vec{v}\}" src="https://canvas.du.edu/equation_images/S%2520%255Ccup%255C%257B%255Cvec%257Bv%257D%255C%257D?scale=1" alt="LaTeX: S \cup\{\vec{v}\}" data-equation-content="S \cup\{\vec{v}\}" data-ignore-a11y-check="" /> is linearly independent if and only if <img class="equation_image" title="\vec{v}\notin [S]" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%255Cnotin%2520%255BS%255D?scale=1" alt="LaTeX: \vec{v}\notin [S]" data-equation-content="\vec{v}\notin [S]" data-ignore-a11y-check="" />.&nbsp;<br />2.5. (I will cover on Thursday) In a vector space, any finite set has a linearly independent subset with the same span.&nbsp;<br />Remark: The same conclusion also holds for infinite sets, though one needs the axiom of choice to prove it.&nbsp;<br />2.6. (I will cover on Thursday) Corollary: A subset <img class="equation_image" title="S = \{\vec{s_1}, ..., \vec{s_n}\}" src="https://canvas.du.edu/equation_images/S%2520%253D%2520%255C%257B%255Cvec%257Bs_1%257D%252C%2520...%252C%2520%255Cvec%257Bs_n%257D%255C%257D?scale=1" alt="LaTeX: S = \{\vec{s_1}, ..., \vec{s_n}\}" data-equation-content="S = \{\vec{s_1}, ..., \vec{s_n}\}" data-ignore-a11y-check="" /> of a vector space is linearly dependent if and only if some <img class="equation_image" title="\vec{s_i}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_i%257D?scale=1" alt="LaTeX: \vec{s_i}" data-equation-content="\vec{s_i}" data-ignore-a11y-check="" /> is a linear combination of the vectors <img class="equation_image" title="\vec{s_1}, ..., \vec{s_{i-1}}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D%252C%2520...%252C%2520%255Cvec%257Bs_%257Bi-1%257D%257D?scale=1" alt="LaTeX: \vec{s_1}, ..., \vec{s_{i-1}}" data-equation-content="\vec{s_1}, ..., \vec{s_{i-1}}" data-ignore-a11y-check="" /> before it.&nbsp;<br />2.7. (I will cover on Thursday) Lemma: Any subset of a linearly independent subset is linear independent. Any superset of a linearly dependent subset is linearly dependent.&nbsp;<br />The proofs of the last four results here (namely, 2.4 to 2.7) are all quite stupid. I will not go over them. Instead, we shall try to get some intuitions from the examples.</li>
                </ol>
            </li>
        </ol>
    </ul></details>
	
	<details><summary>Lecture 8 on 10/06/2022</summary>
	<ul>
        <p>We finished discussing Two.II.1 and almost everything in Two.III.1. The only leftover concept is the definition of representation. You are now ready for all the ticked exercises in Two.III.1 except for 1.22. Next week's quiz problems will be from ticked exercises in Two.II.1 and Two.III.1.</p>
        <p>To catch up with the schedule and accelerate the progress, I used two whiteboard vectors and one space vector to illustrate the lemmas and corollaries regarding linear independent and dependent subsets, skipping textbook examples. I may continue the strategy until we fully catch up with the schedule. Please definitely go over those examples by yourself before attempting the exercises.&nbsp;</p>
        <p>I have decided to give a completion grade for the quiz this week. If you still do not get it, you may leave it later after the midterm tomorrow. The midterm tomorrow will be given in the lab from 9 AM to 9:50 AM. Please make sure you get enough sleep tonight. Suffering from a lack of sleep will increase your chances of making stupid mistakes and will potentially lower your performance in the exam. In the long run, habitually staying up late at night may damage your memory. I am a victim myself. I hope you don't feel the same regret as I do now when you reach my age.&nbsp;</p>	
    </ul></details>
	
	<details><summary>Lecture 9 on 10/11/2022</summary>
	<ul>
        <p><span>We started the class with a discussion of the quiz problem last week. Please keep in mind that a vector is not always represented by multiple numbers. In the quiz problem last week, as well as in any one-dimensional vector space, you only need one number to represent a vector. The properties you need to check for the quiz problem should all follow from properties of products and powers of positive numbers. </span><br /><br /><span>We also briefly recalled some theoretical part in linear system. You have seen that a linear system always has a solution if the echelon form of the augmented matrix does not contain a row of the form <img class="equation_image" title="[0 \quad \cdots \quad 0 \quad  | \quad  1]" src="https://canvas.du.edu/equation_images/%255B0%2520%255Cquad%2520%255Ccdots%2520%255Cquad%25200%2520%255Cquad%2520%2520%257C%2520%255Cquad%2520%25201%255D?scale=1" alt="LaTeX: [0 \quad \cdots \quad 0 \quad  | \quad  1]" data-equation-content="[0 \quad \cdots \quad 0 \quad  | \quad  1]" data-ignore-a11y-check="" />. It was my mistake to not have emphasized the following sufficient condition: the same holds if the echelon form of the coefficient matrix does not contain zero rows. </span><br /><br /><span>Application: For Problem 3 in today's quiz, after you checked the linear independence of the set of vectors, you know that the echelon form of the coefficient matrix for the linear system you use to check the spanning condition will contain no zero rows. Thus a solution exists and you do not need to further justify that. </span><br /><br /><span>Application: For Midterm Problem 2, the correct answer is that a solution exists for any choice of constants. Since I did not emphasize this point enough in class, I will need to consider how to assign partial credits appropriately. </span><br /><br /><span>Today, we finished the discussion of basis. You are now ready for ticked exercises in Section Two.III.2 except for 2.37. Part (c) of 2.37 will not be required in this course. </span><br /><br /><span>One important to keep in mind: for any /*nonzero*/ vector spaces there exists infinitely many choices of basis. So the number of basis is infinite. It is the number of vectors in a basis that is fixed. As we proved today, if a basis consists of n vectors, then any other basis will consist of n vectors. We define n to be the dimension of the vector space. </span><br /><br /><span>The same fact applies to infinite-dimensional vector spaces as well. For this course, we will restrict our attention to finite-dimensional vector spaces. Infinite-dimensional vector space did appear in exercises. In the quiz problem today, the vector space of function from <img class="equation_image" title="\mathbb{R}^+" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BR%257D%255E%252B?scale=1" alt="LaTeX: \mathbb{R}^+" data-equation-content="\mathbb{R}^+" data-ignore-a11y-check="" /> to <img class="equation_image" title="\mathbb{R}" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BR%257D?scale=1" alt="LaTeX: \mathbb{R}" data-equation-content="\mathbb{R}" data-ignore-a11y-check="" /> is infinite-dimensional. But we will only work on certain finite-dimensional subspace of such vector spaces. </span><br /><br /><span>(For students who are interested) Infinite-dimensional vector spaces are usually studied with additional structure (e.g. grading, topology, norm, inner product, etc.). The notion of basis also exists in such contexts (though in most cases not very useful). You will need the knowledge of cardinals and axiom of choice to understand the basics. You will see applications in functional analysis, partial differential equations and advanced abstract algebra. </span><br /><br /><span>Next lecture, we shall finish Chapter 2 and enter Chapter 3. I will still have to accelerate and might have to skip some computations in class. But still, please do not hesitate to stop me if you are confused. Please also claim your extra credit promptly via emails. </span></p>
    </ul>
	</details>
	
	<details><summary>Lecture 10 on 10/13/2022</summary><ul>
		<p><span>We finished Chapter 2. Homework for this week will be ticked exercises in Two.III.1, Two.III.2 and Two.III.3. Please go over them at the very least to prepare for the quiz next Tuesday. </span><br /><br /><span>Next week we shall start with homomorphisms and isomorphisms. On Tuesday I will go over Three.I and Three.II very quickly. I will only discuss the most important examples in those two sections, skipping all those that are not so important. Those example problems are appropriate as quiz and midterm problems. Please make sure you go over them by yourself. </span><br /><br /><span>We started the lecture with the leftovers from Two.III.1. Recall that a basis is a set of vectors that are linearly independent and spanning the vector space. We showed that any linearly independent set can be extended to a basis, and any spanning set can be shrunk to a basis. Using these two results, we proved that in an n-dimensional vector space, a set of n vectors is linearly independent if and only if it span the vector space. </span><br /><br /><span>Two side remarks that are not explicitly stated in the textbook: </span><br /><span>1. Every set of vectors containing the zero vector is automatically linearly dependent, since <img class="equation_image" title="0\cdot \vec{s_1} + \cdots + 0\cdot \vec{s_n} + 1 \cdot \vec{0} = \vec{0}" src="https://canvas.du.edu/equation_images/0%255Ccdot%2520%255Cvec%257Bs_1%257D%2520%252B%2520%255Ccdots%2520%252B%25200%255Ccdot%2520%255Cvec%257Bs_n%257D%2520%252B%25201%2520%255Ccdot%2520%255Cvec%257B0%257D%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: 0\cdot \vec{s_1} + \cdots + 0\cdot \vec{s_n} + 1 \cdot \vec{0} = \vec{0}" data-equation-content="0\cdot \vec{s_1} + \cdots + 0\cdot \vec{s_n} + 1 \cdot \vec{0} = \vec{0}" data-ignore-a11y-check="" />&nbsp;is a nontrivial relation among these vectors. </span><br /><span>2. We make the span of empty set as the trivial vector space <img class="equation_image" title="\{\vec{0}\}" src="https://canvas.du.edu/equation_images/%255C%257B%255Cvec%257B0%257D%255C%257D?scale=1" alt="LaTeX: \{\vec{0}\}" data-equation-content="\{\vec{0}\}" data-ignore-a11y-check="" />, so that the statement "the span of any set is a vector subspace" also applies to the empty set. At this moment, you may think of this convention as a logical nuisance. Justification for such nuisance will be seen in higher level classes. </span><br /><br /><span>We then got into Two.III.2. We started by the notion of row space and row rank of a matrix. We showed that if two matrices are row equivalent, then their row spaces are the same. To prove this result, we recall the following set theoretical pre-requisites: </span><br /><span>1. <img class="equation_image" title="A \subseteq B" src="https://canvas.du.edu/equation_images/A%2520%255Csubseteq%2520B?scale=1" alt="LaTeX: A \subseteq B" data-equation-content="A \subseteq B" data-ignore-a11y-check="" /> if and only if every element in A is also an element of B. </span><br /><span> Equivalently, for every <img class="equation_image" title="x\in A, x \in B" src="https://canvas.du.edu/equation_images/x%255Cin%2520A%252C%2520x%2520%255Cin%2520B?scale=1" alt="LaTeX: x\in A, x \in B" data-equation-content="x\in A, x \in B" data-ignore-a11y-check="" />.</span><br /><span>2. <img class="equation_image" title="A = B" src="https://canvas.du.edu/equation_images/A%2520%253D%2520B?scale=1" alt="LaTeX: A = B" data-equation-content="A = B" data-ignore-a11y-check="" /> if and only if <img class="equation_image" title="A \subseteq B" src="https://canvas.du.edu/equation_images/A%2520%255Csubseteq%2520B?scale=1" alt="LaTeX: A \subseteq B" data-equation-content="A \subseteq B" data-ignore-a11y-check="" /> and <img class="equation_image" title="B \subseteq A" src="https://canvas.du.edu/equation_images/B%2520%255Csubseteq%2520A?scale=1" alt="LaTeX: B \subseteq A" data-equation-content="B \subseteq A" data-ignore-a11y-check="" />. </span><br /><span>Had I emphasized these pre-requisites earlier, you may feel less confused on the proofs. This is sadly another pedagogical mistake I have made. Then the result follows from the fact that row vectors of two row-equivalent matrices are linear combinations of each other, together with the linear combination lemma. </span><br /><br /><span>In particular, since a matrix is row equivalent to an echelon form, and the nonzero rows in an echelon form are linearly independent, so Gauss's method essentially produces a basis for the row space of a matrix. The row rank is shown to be equal to the number of leading entries in an echelon form. We used the example </span><br /><span><img class="equation_image" title="M = \begin{pmatrix}
1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\\
-1 &amp; -1 &amp; 2 &amp; 2 &amp; 0 \\
2 &amp; 4 &amp; 5 &amp; 2 &amp; 9
\end{pmatrix}" src="https://canvas.du.edu/equation_images/M%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A1%2520%2526%25202%2520%2526%25201%2520%2526%25200%2520%2526%25203%255C%255C%250A-1%2520%2526%2520-1%2520%2526%25202%2520%2526%25202%2520%2526%25200%2520%255C%255C%250A2%2520%2526%25204%2520%2526%25205%2520%2526%25202%2520%2526%25209%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: M = \begin{pmatrix}
1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\\
-1 &amp; -1 &amp; 2 &amp; 2 &amp; 0 \\
2 &amp; 4 &amp; 5 &amp; 2 &amp; 9
\end{pmatrix}" data-equation-content="M = \begin{pmatrix}
1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\\
-1 &amp; -1 &amp; 2 &amp; 2 &amp; 0 \\
2 &amp; 4 &amp; 5 &amp; 2 &amp; 9
\end{pmatrix}" data-ignore-a11y-check="" /><br /></span><span>This matrix is row equivalent to the following echelon form</span><br /><span><img class="equation_image" title="\begin{pmatrix}
1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\\
0 &amp; 0 &amp; 3 &amp; 2 &amp; 3\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A1%2520%2526%25202%2520%2526%25201%2520%2526%25200%2520%2526%25203%255C%255C%250A0%2520%2526%25200%2520%2526%25203%2520%2526%25202%2520%2526%25203%255C%255C%250A0%2520%2526%25200%2520%2526%25200%2520%2526%25200%2520%2526%25200%250A%255Cend%257Bpmatrix%257D%250A?scale=1" alt="LaTeX: \begin{pmatrix}
1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\\
0 &amp; 0 &amp; 3 &amp; 2 &amp; 3\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
" data-equation-content="\begin{pmatrix}
1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\\
0 &amp; 0 &amp; 3 &amp; 2 &amp; 3\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
" data-ignore-a11y-check="" /><br /></span><span>and the following reduced echelon form</span><br /><span><img class="equation_image" title="\begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; - \frac 2 3 &amp; 2\\
0 &amp; 0 &amp; 1 &amp; \frac 2 3 &amp; 1\\ 
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A1%2520%2526%25202%2520%2526%25200%2520%2526%2520-%2520%255Cfrac%25202%25203%2520%2526%25202%255C%255C%250A0%2520%2526%25200%2520%2526%25201%2520%2526%2520%255Cfrac%25202%25203%2520%2526%25201%255C%255C%2520%250A0%2520%2526%25200%2520%2526%25200%2520%2526%25200%2520%2526%25200%250A%255Cend%257Bpmatrix%257D%250A?scale=1" alt="LaTeX: \begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; - \frac 2 3 &amp; 2\\
0 &amp; 0 &amp; 1 &amp; \frac 2 3 &amp; 1\\ 
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
" data-equation-content="\begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; - \frac 2 3 &amp; 2\\
0 &amp; 0 &amp; 1 &amp; \frac 2 3 &amp; 1\\ 
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
" data-ignore-a11y-check="" /><br /></span><span>The row space has basis</span><br /><img class="equation_image" title="\left\langle \begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\end{pmatrix}, \begin{pmatrix} 0 &amp; 0 &amp; 3 &amp; 2 &amp; 3\end{pmatrix}\right\rangle" src="https://canvas.du.edu/equation_images/%255Cleft%255Clangle%2520%255Cbegin%257Bpmatrix%257D%25201%2520%2526%25202%2520%2526%25201%2520%2526%25200%2520%2526%25203%255Cend%257Bpmatrix%257D%252C%2520%255Cbegin%257Bpmatrix%257D%25200%2520%2526%25200%2520%2526%25203%2520%2526%25202%2520%2526%25203%255Cend%257Bpmatrix%257D%255Cright%255Crangle?scale=1" alt="LaTeX: \left\langle \begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\end{pmatrix}, \begin{pmatrix} 0 &amp; 0 &amp; 3 &amp; 2 &amp; 3\end{pmatrix}\right\rangle" data-equation-content="\left\langle \begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; 0 &amp; 3\end{pmatrix}, \begin{pmatrix} 0 &amp; 0 &amp; 3 &amp; 2 &amp; 3\end{pmatrix}\right\rangle" data-ignore-a11y-check="" /><br /><span>and </span><br /><span><img class="equation_image" title="\left\langle \begin{pmatrix} 1 &amp; 2 &amp; 0 &amp; -\frac 2 3 &amp; 2\end{pmatrix}, \begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; \frac 2 3 &amp; 1\end{pmatrix}\right\rangle" src="https://canvas.du.edu/equation_images/%255Cleft%255Clangle%2520%255Cbegin%257Bpmatrix%257D%25201%2520%2526%25202%2520%2526%25200%2520%2526%2520-%255Cfrac%25202%25203%2520%2526%25202%255Cend%257Bpmatrix%257D%252C%2520%255Cbegin%257Bpmatrix%257D%25200%2520%2526%25200%2520%2526%25201%2520%2526%2520%255Cfrac%25202%25203%2520%2526%25201%255Cend%257Bpmatrix%257D%255Cright%255Crangle?scale=1" alt="LaTeX: \left\langle \begin{pmatrix} 1 &amp; 2 &amp; 0 &amp; -\frac 2 3 &amp; 2\end{pmatrix}, \begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; \frac 2 3 &amp; 1\end{pmatrix}\right\rangle" data-equation-content="\left\langle \begin{pmatrix} 1 &amp; 2 &amp; 0 &amp; -\frac 2 3 &amp; 2\end{pmatrix}, \begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; \frac 2 3 &amp; 1\end{pmatrix}\right\rangle" data-ignore-a11y-check="" /><br />The row rank is 2. </span><br /><br /><span>Then we continue with the notion of column space and column rank of a matrix. To obtain a basis for the column space, it suffices to perform row operations on the transpose. (I did not say in class that) This is essentially performing certain column operations. Probably due to the concerns regarding dual vector space, the textbook did not want to mention column operations at this stage as other usual linear algebra textbooks (unfortunately I don't exactly know why). </span><br /><br /><span>We then explained one of the most important lemmas: Row operations do not change the rank of the column. The proof basically follows from the idea that row operations on a linear system do not change the solution of the system. Thus any possible relations between the column vectors are not changed by the row operations. I decided not to go into further details. Instead, we studied the example of the matrix M above. Let <img class="equation_image" title="\vec{s_1}, ..., \vec{s_5}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D%252C%2520...%252C%2520%255Cvec%257Bs_5%257D?scale=1" alt="LaTeX: \vec{s_1}, ..., \vec{s_5}" data-equation-content="\vec{s_1}, ..., \vec{s_5}" data-ignore-a11y-check="" /> be the columns of the matrix M, i.e. </span><br /><img class="equation_image" title="\vec{s_1} = \begin{pmatrix}
1 \\
-1 \\
2 
\end{pmatrix},
\vec{s_2} = \begin{pmatrix}
2 \\
 -1 \\
4 
\end{pmatrix}, 
\vec{s_3} = \begin{pmatrix}
1 \\
2 \\
5 
\end{pmatrix}, \vec{s_4} = \begin{pmatrix}
0 \\
2 \\
2 
\end{pmatrix}, \vec{s_5} = \begin{pmatrix}
3\\ 0 \\ 9
\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A1%2520%255C%255C%250A-1%2520%255C%255C%250A2%2520%250A%255Cend%257Bpmatrix%257D%252C%250A%255Cvec%257Bs_2%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A2%2520%255C%255C%250A%2520-1%2520%255C%255C%250A4%2520%250A%255Cend%257Bpmatrix%257D%252C%2520%250A%255Cvec%257Bs_3%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A1%2520%255C%255C%250A2%2520%255C%255C%250A5%2520%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bs_4%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A0%2520%255C%255C%250A2%2520%255C%255C%250A2%2520%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bs_5%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A3%255C%255C%25200%2520%255C%255C%25209%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \vec{s_1} = \begin{pmatrix}
1 \\
-1 \\
2 
\end{pmatrix},
\vec{s_2} = \begin{pmatrix}
2 \\
 -1 \\
4 
\end{pmatrix}, 
\vec{s_3} = \begin{pmatrix}
1 \\
2 \\
5 
\end{pmatrix}, \vec{s_4} = \begin{pmatrix}
0 \\
2 \\
2 
\end{pmatrix}, \vec{s_5} = \begin{pmatrix}
3\\ 0 \\ 9
\end{pmatrix}" data-equation-content="\vec{s_1} = \begin{pmatrix}
1 \\
-1 \\
2 
\end{pmatrix},
\vec{s_2} = \begin{pmatrix}
2 \\
 -1 \\
4 
\end{pmatrix}, 
\vec{s_3} = \begin{pmatrix}
1 \\
2 \\
5 
\end{pmatrix}, \vec{s_4} = \begin{pmatrix}
0 \\
2 \\
2 
\end{pmatrix}, \vec{s_5} = \begin{pmatrix}
3\\ 0 \\ 9
\end{pmatrix}" data-ignore-a11y-check="" /></p>
<p><span>Let <img class="equation_image" title="\vec{t_1}, ..., \vec{t_5}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bt_1%257D%252C%2520...%252C%2520%255Cvec%257Bt_5%257D?scale=1" alt="LaTeX: \vec{t_1}, ..., \vec{t_5}" data-equation-content="\vec{t_1}, ..., \vec{t_5}" data-ignore-a11y-check="" /> be the columns of the reduced echelon form of M, i.e., </span><br /><img class="equation_image" title="\vec{t_1} = \begin{pmatrix}
1 \\
0 \\
0 
\end{pmatrix},
\vec{t_2} = \begin{pmatrix}
2 \\
0 \\
0
\end{pmatrix}, 
\vec{t_3} = \begin{pmatrix}
0 \\
1 \\
0 
\end{pmatrix}, \vec{t_4} = \begin{pmatrix}
-\frac 2 3 \\
\frac 2 3 \\
0 
\end{pmatrix}, \vec{t_5} = \begin{pmatrix}
2\\ 1 \\ 0
\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bt_1%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A1%2520%255C%255C%250A0%2520%255C%255C%250A0%2520%250A%255Cend%257Bpmatrix%257D%252C%250A%255Cvec%257Bt_2%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A2%2520%255C%255C%250A0%2520%255C%255C%250A0%250A%255Cend%257Bpmatrix%257D%252C%2520%250A%255Cvec%257Bt_3%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A0%2520%255C%255C%250A1%2520%255C%255C%250A0%2520%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bt_4%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A-%255Cfrac%25202%25203%2520%255C%255C%250A%255Cfrac%25202%25203%2520%255C%255C%250A0%2520%250A%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Bt_5%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A2%255C%255C%25201%2520%255C%255C%25200%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \vec{t_1} = \begin{pmatrix}
1 \\
0 \\
0 
\end{pmatrix},
\vec{t_2} = \begin{pmatrix}
2 \\
0 \\
0
\end{pmatrix}, 
\vec{t_3} = \begin{pmatrix}
0 \\
1 \\
0 
\end{pmatrix}, \vec{t_4} = \begin{pmatrix}
-\frac 2 3 \\
\frac 2 3 \\
0 
\end{pmatrix}, \vec{t_5} = \begin{pmatrix}
2\\ 1 \\ 0
\end{pmatrix}" data-equation-content="\vec{t_1} = \begin{pmatrix}
1 \\
0 \\
0 
\end{pmatrix},
\vec{t_2} = \begin{pmatrix}
2 \\
0 \\
0
\end{pmatrix}, 
\vec{t_3} = \begin{pmatrix}
0 \\
1 \\
0 
\end{pmatrix}, \vec{t_4} = \begin{pmatrix}
-\frac 2 3 \\
\frac 2 3 \\
0 
\end{pmatrix}, \vec{t_5} = \begin{pmatrix}
2\\ 1 \\ 0
\end{pmatrix}" data-ignore-a11y-check="" /><br /><span>Then clearly,<br /></span><img class="equation_image" title="\vec{t_2} = 2\vec{t_1}, \vec{t_4} = -\frac 2 3 \vec{t_1} + \frac 2 3 \vec{t_3}, \vec{t_5} = 2 \vec{t_1} + \vec{t_3}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bt_2%257D%2520%253D%25202%255Cvec%257Bt_1%257D%252C%2520%255Cvec%257Bt_4%257D%2520%253D%2520-%255Cfrac%25202%25203%2520%255Cvec%257Bt_1%257D%2520%252B%2520%255Cfrac%25202%25203%2520%255Cvec%257Bt_3%257D%252C%2520%255Cvec%257Bt_5%257D%2520%253D%25202%2520%255Cvec%257Bt_1%257D%2520%252B%2520%255Cvec%257Bt_3%257D?scale=1" alt="LaTeX: \vec{t_2} = 2\vec{t_1}, \vec{t_4} = -\frac 2 3 \vec{t_1} + \frac 2 3 \vec{t_3}, \vec{t_5} = 2 \vec{t_1} + \vec{t_3}" data-equation-content="\vec{t_2} = 2\vec{t_1}, \vec{t_4} = -\frac 2 3 \vec{t_1} + \frac 2 3 \vec{t_3}, \vec{t_5} = 2 \vec{t_1} + \vec{t_3}" data-ignore-a11y-check="" /><br /><span>You may check that the same relation holds for columns in M. More precisely,<br /><img class="equation_image" title="\vec{s_2} = 2\vec{s_1}, \vec{s_4} = -\frac 2 3 \vec{s_1} + \frac 2 3 \vec{s_3}, \vec{s_5} = 2 \vec{s_1} + \vec{s_3}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_2%257D%2520%253D%25202%255Cvec%257Bs_1%257D%252C%2520%255Cvec%257Bs_4%257D%2520%253D%2520-%255Cfrac%25202%25203%2520%255Cvec%257Bs_1%257D%2520%252B%2520%255Cfrac%25202%25203%2520%255Cvec%257Bs_3%257D%252C%2520%255Cvec%257Bs_5%257D%2520%253D%25202%2520%255Cvec%257Bs_1%257D%2520%252B%2520%255Cvec%257Bs_3%257D?scale=1" alt="LaTeX: \vec{s_2} = 2\vec{s_1}, \vec{s_4} = -\frac 2 3 \vec{s_1} + \frac 2 3 \vec{s_3}, \vec{s_5} = 2 \vec{s_1} + \vec{s_3}" data-equation-content="\vec{s_2} = 2\vec{s_1}, \vec{s_4} = -\frac 2 3 \vec{s_1} + \frac 2 3 \vec{s_3}, \vec{s_5} = 2 \vec{s_1} + \vec{s_3}" data-ignore-a11y-check="" />&nbsp;</span><br /><span>Thus the columns <img class="equation_image" title="\vec{s_1}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_1%257D?scale=1" alt="LaTeX: \vec{s_1}" data-equation-content="\vec{s_1}" data-ignore-a11y-check="" /> and <img class="equation_image" title="\vec{s_3}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bs_3%257D?scale=1" alt="LaTeX: \vec{s_3}" data-equation-content="\vec{s_3}" data-ignore-a11y-check="" /> form a basis for the column space of the matrix M, as well as the columns <img class="equation_image" title="\vec{t_1}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bt_1%257D?scale=1" alt="LaTeX: \vec{t_1}" data-equation-content="\vec{t_1}" data-ignore-a11y-check="" /> and&nbsp;<img class="equation_image" title="\vec{t_3}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bt_3%257D?scale=1" alt="LaTeX: \vec{t_3}" data-equation-content="\vec{t_3}" data-ignore-a11y-check="" />.</span><br /><br /><span>From the lemma above, we deduce an important theorem: the row rank and the column rank of the matrix are equal. We first check that this is the case for the reduced echelon form. For general matrices, we use row operations to reduce the problem to its reduced echelon form. More precisely, since row operations do not change the row rank, row rank of the matrix = row rank of its reduced echelon form, which is proved to be equal to the column rank of its reduced echelon form. The lemma implies that the column rank of the matrix equals to the column rank of its reduced echelon form, thereby finishing the proof. </span><br /><br /><span>This leads to the definition of the rank of the matrix. From the reduced echelon form, we also proved the important theorem that for the homogeneous linear system given by <img class="equation_image" title="A \vec{x} = \vec{0}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: A \vec{x} = \vec{0}" data-equation-content="A \vec{x} = \vec{0}" data-ignore-a11y-check="" />, rank A = r if and only if the dimension of the solution space is n-r. In particular, if rank A = n, then the solution space is zero-dimensional, i.e. the solution space is<img class="equation_image" title=" \{\vec{0}\}" src="https://canvas.du.edu/equation_images/%2520%255C%257B%255Cvec%257B0%257D%255C%257D?scale=1" alt="LaTeX:  \{\vec{0}\}" data-equation-content=" \{\vec{0}\}" data-ignore-a11y-check="" />. </span></p>
<p><span>In the case that the matrix A is a square matrix, i.e., number of rows and columns are equal, we have the following important corollary: rank A = n if and only if A is nonsingular, if and only if the rows of A are linearly independent, if and only if the columns of A are linearly independent, if and only if any linear system <img class="equation_image" title="A \vec{x} = \vec{d}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A \vec{x} = \vec{d}" data-equation-content="A \vec{x} = \vec{d}" data-ignore-a11y-check="" /> with coefficient matrix A has a unique solution. We will see in later parts of this course that if one of these condition happens, then the matrix A will be invertible. The unique solution to the system <img class="equation_image" title="A \vec{x} = \vec{d}" src="https://canvas.du.edu/equation_images/A%2520%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: A \vec{x} = \vec{d}" data-equation-content="A \vec{x} = \vec{d}" data-ignore-a11y-check="" /> is simply <img class="equation_image" title="\vec{x} = A^{-1} \vec{d}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bx%257D%2520%253D%2520A%255E%257B-1%257D%2520%255Cvec%257Bd%257D?scale=1" alt="LaTeX: \vec{x} = A^{-1} \vec{d}" data-equation-content="\vec{x} = A^{-1} \vec{d}" data-ignore-a11y-check="" />, where <img class="equation_image" title="A^{-1}" src="https://canvas.du.edu/equation_images/A%255E%257B-1%257D?scale=1" alt="LaTeX: A^{-1}" data-equation-content="A^{-1}" data-ignore-a11y-check="" /> is the inverse of A.&nbsp;</span></p>
	</ul></details>
	
	<details><summary>Lecture 11 on 10/18/2022</summary>
	<ul>
        <p><span>Today we started Chapter 3 and basically finished Three.I.1. We introduced the definitions homomorphisms, isomorphisms and automorphisms. We also stated an important lemma that for a map to be a homomorphism, all we need is to make sure it preserves linear combinations of pairs of vectors. </span><span>You are now ready for the ticked exercises in that subsection. </span><br /><br /><span>The main challenge is to prepare you for all the problems in Lab 4. I don't think there is any trouble for me to cover the pre-requisites for the first two examples. But to prepare you for the rest, I will have to finish Three.II and Three.III. I'll see how much I can do on Thursday.&nbsp;</span></p>
<p><span>Please remind me your extra credit claims.&nbsp;</span></p>
    </ul></details>
	
	<details><summary>Lecture 12 on 10/20/2022</summary>
	<ul>
        <p>We finished Three.I.2 and Three.II.1. I managed to cover the matrix representation of a homomorphism with respect to the standard bases of the domain and the codomain. In Three.III.1, we will cover the matrix representation with respect to bases that are not necessarily standard.&nbsp;</p>
<p>Technically, you are ready for Exercises 1 and 3 in Lab 4. But for Exercises 2 and 4, you will need range space and null space, which I failed to cover today.&nbsp;</p>
<p>Here is the summary together with the pedagogical mistakes I have made today:&nbsp;</p>
<p>We showed that isomorphism among vector spaces is an equivalence relation. We also showed that two vector spaces are isomorphic if and only if their dimensions are the same. So <img class="equation_image" title="\mathbb{R}^n" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BR%257D%255En?scale=1" alt="LaTeX: \mathbb{R}^n" data-equation-content="\mathbb{R}^n" data-ignore-a11y-check="" /> is the "canonical form" of the equivalence relation given by isomorphism.&nbsp;<br /><strong>(I forgot to emphasize this in class)</strong> The phrase canonical form is in quotes because it is not standard terminology. Typically canonical forms are for matrices, not for anything else. More appropriate terminology is a canonical representative.&nbsp;</p>
<p>We also discussed examples of homomorphisms. Here are the essential facts regarding a homomorphism: Let V be a vector space with a basis B. Let W be a vector space.</p>
<ol>
    <li>Any homomorphism from V to W is uniquely determined by its image of the basis vectors in B.</li>
    <li>Any map <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="f: B \to W" src="https://canvas.du.edu/equation_images/f%253A%2520B%2520%255Cto%2520W?scale=1" alt="LaTeX: f: B \to W" data-equation-content="f: B \to W" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> can be extended linearly to a unique homomorphism </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\hat{f}: V \to W" src="https://canvas.du.edu/equation_images/%255Chat%257Bf%257D%253A%2520V%2520%255Cto%2520W?scale=1" alt="LaTeX: \hat{f}: V \to W" data-equation-content="\hat{f}: V \to W" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">.</span></li>
    <li>If <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="V = \mathbb{R}^n, W = \mathbb{R}^m" src="https://canvas.du.edu/equation_images/V%2520%253D%2520%255Cmathbb%257BR%257D%255En%252C%2520W%2520%253D%2520%255Cmathbb%257BR%257D%255Em?scale=1" alt="LaTeX: V = \mathbb{R}^n, W = \mathbb{R}^m" data-equation-content="V = \mathbb{R}^n, W = \mathbb{R}^m" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">, </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="B = \langle \vec{e_1}, ..., \vec{e_n}\rangle" src="https://canvas.du.edu/equation_images/B%2520%253D%2520%255Clangle%2520%255Cvec%257Be_1%257D%252C%2520...%252C%2520%255Cvec%257Be_n%257D%255Crangle?scale=1" alt="LaTeX: B = \langle \vec{e_1}, ..., \vec{e_n}\rangle" data-equation-content="B = \langle \vec{e_1}, ..., \vec{e_n}\rangle" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> is the standard basis for V, i.e., <br /></span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\vec{e_1} = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \vec{e_2} = \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix}, ..., e_n = \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/%255Cvec%257Be_1%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257Be_2%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25201%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%252C%2520...%252C%2520e_n%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25201%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: \vec{e_1} = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \vec{e_2} = \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix}, ..., e_n = \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}.&nbsp;" data-equation-content="\vec{e_1} = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \vec{e_2} = \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix}, ..., e_n = \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /><br />If we know that <br /><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="f(\vec{e_1}) = \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}\end{pmatrix},&nbsp;
f(\vec{e_2}) = \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}\end{pmatrix}, ...,&nbsp;
f(\vec{e_n}) = \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}\end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/f(%255Cvec%257Be_1%257D)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520a_%257B11%257D%2520%255C%255C%2520a_%257B21%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_%257Bm1%257D%255Cend%257Bpmatrix%257D%252C%25C2%25A0%250Af(%255Cvec%257Be_2%257D)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520a_%257B12%257D%2520%255C%255C%2520a_%257B22%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_%257Bm2%257D%255Cend%257Bpmatrix%257D%252C%2520...%252C%25C2%25A0%250Af(%255Cvec%257Be_n%257D)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520a_%257B1n%257D%2520%255C%255C%2520a_%257B2n%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_%257Bmn%257D%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: f(\vec{e_1}) = \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}\end{pmatrix},&nbsp;
f(\vec{e_2}) = \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}\end{pmatrix}, ...,&nbsp;
f(\vec{e_n}) = \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}\end{pmatrix}.&nbsp;" data-equation-content="f(\vec{e_1}) = \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}\end{pmatrix},&nbsp;
f(\vec{e_2}) = \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}\end{pmatrix}, ...,&nbsp;
f(\vec{e_n}) = \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}\end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /><br />then the image of any vector <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%2520x_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}" data-equation-content="\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> is simply given by <br /></span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="f\left(\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix} \right) = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/f%255Cleft(%255Cbegin%257Bpmatrix%257D%2520x_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%255Cend%257Bpmatrix%257D%2520%255Cright)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%2520%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%250A%255Cend%257Bpmatrix%257D%2520%255Cbegin%257Bpmatrix%257D%2520x_1%2520%255C%255C%2520x_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520x_n%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: f\left(\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix} \right) = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}.&nbsp;" data-equation-content="f\left(\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix} \right) = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /><br />The matrix <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title=" \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix} " src="https://canvas.du.edu/equation_images/%2520%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%2520%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%250A%255Cend%257Bpmatrix%257D%2520?scale=1" alt="LaTeX:  \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix} " data-equation-content=" \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix} " data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> is called the matrix representation with respect to standard bases of </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\mathbb{R}^n" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BR%257D%255En?scale=1" alt="LaTeX: \mathbb{R}^n" data-equation-content="\mathbb{R}^n" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> and </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\mathbb{R}^m" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BR%257D%255Em?scale=1" alt="LaTeX: \mathbb{R}^m" data-equation-content="\mathbb{R}^m" data-ignore-a11y-check="" /><br /><strong>(I forgot to mention this in class)</strong> Generally speaking, if f is a map from A to B, we call <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="f(a)" src="https://canvas.du.edu/equation_images/f(a)?scale=1" alt="LaTeX: f(a)" data-equation-content="f(a)" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> the </span><span style="text-decoration: underline;">image</span><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> of </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="  a \in A" src="https://canvas.du.edu/equation_images/%2520%2520a%2520%255Cin%2520A?scale=1" alt="LaTeX:   a \in A" data-equation-content="  a \in A" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">. <br /></span><strong>(I forgot to mention this in class)</strong> Implicitly, we are also using the standard basis of the codomain <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\mathbb{R}^m" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BR%257D%255Em?scale=1" alt="LaTeX: \mathbb{R}^m" data-equation-content="\mathbb{R}^m" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> because the vectors </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="f(\vec{e_1}), ..., f(\vec{e_n})" src="https://canvas.du.edu/equation_images/f(%255Cvec%257Be_1%257D)%252C%2520...%252C%2520f(%255Cvec%257Be_n%257D)?scale=1" alt="LaTeX: f(\vec{e_1}), ..., f(\vec{e_n})" data-equation-content="f(\vec{e_1}), ..., f(\vec{e_n})" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> in the codomain are implicitly the representations with respect to the standard basis. <br /></span><strong>(I should also mention that)</strong> In general, if <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="B = \langle \vec{\beta_1}, ..., \vec{\beta_n}\rangle" src="https://canvas.du.edu/equation_images/B%2520%253D%2520%255Clangle%2520%255Cvec%257B%255Cbeta_1%257D%252C%2520...%252C%2520%255Cvec%257B%255Cbeta_n%257D%255Crangle?scale=1" alt="LaTeX: B = \langle \vec{\beta_1}, ..., \vec{\beta_n}\rangle" data-equation-content="B = \langle \vec{\beta_1}, ..., \vec{\beta_n}\rangle" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> is a basis for the domain V, D is a basis for the codomain W, and if <br /></span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\text{Rep}_{D}f(\vec{\beta_1}) = \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}\end{pmatrix},&nbsp;
\text{Rep}_Df(\vec{\beta_2}) = \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}\end{pmatrix}, ...,&nbsp;
\text{Rep}_{D}f(\vec{\beta_n}) = \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}\end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BD%257Df(%255Cvec%257B%255Cbeta_1%257D)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520a_%257B11%257D%2520%255C%255C%2520a_%257B21%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_%257Bm1%257D%255Cend%257Bpmatrix%257D%252C%25C2%25A0%250A%255Ctext%257BRep%257D_Df(%255Cvec%257B%255Cbeta_2%257D)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520a_%257B12%257D%2520%255C%255C%2520a_%257B22%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_%257Bm2%257D%255Cend%257Bpmatrix%257D%252C%2520...%252C%25C2%25A0%250A%255Ctext%257BRep%257D_%257BD%257Df(%255Cvec%257B%255Cbeta_n%257D)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520a_%257B1n%257D%2520%255C%255C%2520a_%257B2n%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_%257Bmn%257D%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: \text{Rep}_{D}f(\vec{\beta_1}) = \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}\end{pmatrix},&nbsp;
\text{Rep}_Df(\vec{\beta_2}) = \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}\end{pmatrix}, ...,&nbsp;
\text{Rep}_{D}f(\vec{\beta_n}) = \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}\end{pmatrix}.&nbsp;" data-equation-content="\text{Rep}_{D}f(\vec{\beta_1}) = \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}\end{pmatrix},&nbsp;
\text{Rep}_Df(\vec{\beta_2}) = \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}\end{pmatrix}, ...,&nbsp;
\text{Rep}_{D}f(\vec{\beta_n}) = \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}\end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /><br />Then the matrix <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B1n%257D%255C%255C%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%2520%255C%255C%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%2520%255C%255C%250Aa_%257Bm1%257D%2520%2526%2520a_%257Bm2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bmn%257D%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix}" data-equation-content="\begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{pmatrix}" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> is called the matrix representation with respect to the bases B and D, denoted </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\text{Rep}_{B,D}(f)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252CD%257D(f)?scale=1" alt="LaTeX: \text{Rep}_{B,D}(f)" data-equation-content="\text{Rep}_{B,D}(f)" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">.</span></li>
    <li>We defined endomorphisms as homomorphisms of a vector space to itself.
        <ol>
            <li><strong>(I forgot to mention in class)</strong> Recall that an automorphism is an isomorphism of a vector space to itself. All automorphisms are endomorphisms, but not vice versa.</li>
            <li><strong>(I forgot to mention in class)</strong> In old fashion languages:<br />linear maps = homomorphism,<br />invertible linear maps = isomorphism,<br />linear transformations = endomorphism, <br />invertible linear transformations = automorphism.<br />The textbook used old fashion language for endomorphisms. I don't understand why.</li>
            <li>The matrix representation of an endomorphism is a square matrix. Associated with an endomorphism are two extremely important concepts: eigenvalues and eigenvectors. We will end the course with these two concepts.</li>
        </ol>
    </li>
    <li>Unfortunately, all the examples of linear endomorphisms I gave in class are automorphisms. Here are two examples of endomorphisms that are not automorphisms:
        <ol>
            <li><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">Let <img class="equation_image" title="f: \mathbb{R}^2 \to \mathbb{R^2}" src="https://canvas.du.edu/equation_images/f%253A%2520%255Cmathbb%257BR%257D%255E2%2520%255Cto%2520%255Cmathbb%257BR%255E2%257D?scale=1" alt="LaTeX: f: \mathbb{R}^2 \to \mathbb{R^2}" data-equation-content="f: \mathbb{R}^2 \to \mathbb{R^2}" data-ignore-a11y-check="" /> be the projection to the x-axis. More precisely, <img class="equation_image" title="f\left(\begin{pmatrix} x\\y\end{pmatrix}\right) = \begin{pmatrix} x\\ 0\end{pmatrix}" src="https://canvas.du.edu/equation_images/f%255Cleft(%255Cbegin%257Bpmatrix%257D%2520x%255C%255Cy%255Cend%257Bpmatrix%257D%255Cright)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520x%255C%255C%25200%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: f\left(\begin{pmatrix} x\\y\end{pmatrix}\right) = \begin{pmatrix} x\\ 0\end{pmatrix}" data-equation-content="f\left(\begin{pmatrix} x\\y\end{pmatrix}\right) = \begin{pmatrix} x\\ 0\end{pmatrix}" data-ignore-a11y-check="" />. Then f is an endomorphism that is neither one-to-one nor onto. </span></li>
            <li>We may regard the differential operator d/dx as an endomorphism on <img class="equation_image" title="\mathcal{P}_n" src="https://canvas.du.edu/equation_images/%255Cmathcal%257BP%257D_n?scale=1" alt="LaTeX: \mathcal{P}_n" data-equation-content="\mathcal{P}_n" data-ignore-a11y-check="" /> (the image of any polynomial is still a polynomial of degree at most n). Then d/dx is neither one-to-one nor onto.&nbsp;</li>
        </ol>
    </li>
</ol>
    </ul></details>
	
	<details><summary>Lecture 13 on 10/25/2022
</summary>
	<ul>
        <p><span>We finished the discussion of range space and null space of a homomorphism. We also stated the definition of matrix representations in the most general sense. The progress is still too slow, though necessary, as the notion of range space and null space is quite abstract and should be explained with enough examples. You are now ready for the exercises in Three.II.2 and Three.III.1. </span><br /><br /><span>The rank-nullity theorem is one of the most important theorems in linear algebra. I skipped the proof because it will be clear once we know how to interpret a homomorphim in term of matrices and linear systems, the theorem is equivalent to the following:</span><br /><span>1. the number of leading entries in an echelon form + the number of non-leading entries in an echelon form = the number of columns in a matrix. </span><br /><span>2. the dimension of the solution space of <img class="equation_image" title="A\vec{x} = \vec{0}" src="https://canvas.du.edu/equation_images/A%255Cvec%257Bx%257D%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: A\vec{x} = \vec{0}" data-equation-content="A\vec{x} = \vec{0}" data-ignore-a11y-check="" /> is a subspace of dimension n - rank(A), where n is the number of entries in <img class="equation_image" title="\vec{x}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bx%257D?scale=1" alt="LaTeX: \vec{x}" data-equation-content="\vec{x}" data-ignore-a11y-check="" />. </span><br /><br /><span>The last theorem we stated in class plays a fundamental role in everything, namely, </span><br /><span><img class="equation_image" title="\text{Rep}_D(h(\vec{v}) = \text{Rep}_{B, D}(h) \text{Rep}_{B}(\vec{v})" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_D(h(%255Cvec%257Bv%257D)%2520%253D%2520%255Ctext%257BRep%257D_%257BB%252C%2520D%257D(h)%2520%255Ctext%257BRep%257D_%257BB%257D(%255Cvec%257Bv%257D)?scale=1" alt="LaTeX: \text{Rep}_D(h(\vec{v}) = \text{Rep}_{B, D}(h) \text{Rep}_{B}(\vec{v})" data-equation-content="\text{Rep}_D(h(\vec{v}) = \text{Rep}_{B, D}(h) \text{Rep}_{B}(\vec{v})" data-ignore-a11y-check="" /><br /></span><span>I did not have the time to supplement an example in class. Let me include one here. </span><br /><br /><span>Consider the projection map <img class="equation_image" title=" \pi: \mathbb{R}^2 \to \mathbb{R}^2" src="https://canvas.du.edu/equation_images/%2520%255Cpi%253A%2520%255Cmathbb%257BR%257D%255E2%2520%255Cto%2520%255Cmathbb%257BR%257D%255E2?scale=1" alt="LaTeX:  \pi: \mathbb{R}^2 \to \mathbb{R}^2" data-equation-content=" \pi: \mathbb{R}^2 \to \mathbb{R}^2" data-ignore-a11y-check="" /> given by<br /><img class="equation_image" title=" \begin{pmatrix} x \\ y \end{pmatrix} \mapsto \begin{pmatrix} x \\ 0 \end{pmatrix}. " src="https://canvas.du.edu/equation_images/%2520%255Cbegin%257Bpmatrix%257D%2520x%2520%255C%255C%2520y%2520%255Cend%257Bpmatrix%257D%2520%255Cmapsto%2520%255Cbegin%257Bpmatrix%257D%2520x%2520%255C%255C%25200%2520%255Cend%257Bpmatrix%257D.%2520?scale=1" alt="LaTeX:  \begin{pmatrix} x \\ y \end{pmatrix} \mapsto \begin{pmatrix} x \\ 0 \end{pmatrix}. " data-equation-content=" \begin{pmatrix} x \\ y \end{pmatrix} \mapsto \begin{pmatrix} x \\ 0 \end{pmatrix}. " data-ignore-a11y-check="" />&nbsp;<br /></span><span>We have deduced in class that the matrix representation with respect to the basis <img class="equation_image" title="B = \left\langle \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \begin{pmatrix} -1 \\ 1 \end{pmatrix}\right\rangle" src="https://canvas.du.edu/equation_images/B%2520%253D%2520%255Cleft%255Clangle%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cbegin%257Bpmatrix%257D%2520-1%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%255Cright%255Crangle?scale=1" alt="LaTeX: B = \left\langle \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \begin{pmatrix} -1 \\ 1 \end{pmatrix}\right\rangle" data-equation-content="B = \left\langle \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \begin{pmatrix} -1 \\ 1 \end{pmatrix}\right\rangle" data-ignore-a11y-check="" /> of the domain and the basis <img class="equation_image" title="D = \left\langle \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 2 \\ 2 \end{pmatrix}\right\rangle" src="https://canvas.du.edu/equation_images/D%2520%253D%2520%255Cleft%255Clangle%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cbegin%257Bpmatrix%257D%25202%2520%255C%255C%25202%2520%255Cend%257Bpmatrix%257D%255Cright%255Crangle?scale=1" alt="LaTeX: D = \left\langle \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 2 \\ 2 \end{pmatrix}\right\rangle" data-equation-content="D = \left\langle \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 2 \\ 2 \end{pmatrix}\right\rangle" data-ignore-a11y-check="" /> of the codomain, which is the matrix <img class="equation_image" title="\begin{pmatrix} -1 &amp; 1 \\ 1/2 &amp; -1/2 \end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%2520-1%2520%2526%25201%2520%255C%255C%25201%252F2%2520%2526%2520-1%252F2%2520%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix} -1 &amp; 1 \\ 1/2 &amp; -1/2 \end{pmatrix}" data-equation-content="\begin{pmatrix} -1 &amp; 1 \\ 1/2 &amp; -1/2 \end{pmatrix}" data-ignore-a11y-check="" /></span><span>. <br />Now consider the vector <img class="equation_image" title="\vec{v} = \begin{pmatrix} 2 \\ 5 \end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25202%2520%255C%255C%25205%2520%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \vec{v} = \begin{pmatrix} 2 \\ 5 \end{pmatrix}" data-equation-content="\vec{v} = \begin{pmatrix} 2 \\ 5 \end{pmatrix}" data-ignore-a11y-check="" />. By solving the linear system</span><br /><span><img class="equation_image" title=" \begin{pmatrix} 2 \\ 5 \end{pmatrix} = c_1 \begin{pmatrix} 1 \\ 1 \end{pmatrix} + c_2 \begin{pmatrix} -1 \\ 1 \end{pmatrix}, " src="https://canvas.du.edu/equation_images/%2520%255Cbegin%257Bpmatrix%257D%25202%2520%255C%255C%25205%2520%255Cend%257Bpmatrix%257D%2520%253D%2520c_1%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%2520%252B%2520c_2%2520%255Cbegin%257Bpmatrix%257D%2520-1%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%252C%2520?scale=1" alt="LaTeX:  \begin{pmatrix} 2 \\ 5 \end{pmatrix} = c_1 \begin{pmatrix} 1 \\ 1 \end{pmatrix} + c_2 \begin{pmatrix} -1 \\ 1 \end{pmatrix}, " data-equation-content=" \begin{pmatrix} 2 \\ 5 \end{pmatrix} = c_1 \begin{pmatrix} 1 \\ 1 \end{pmatrix} + c_2 \begin{pmatrix} -1 \\ 1 \end{pmatrix}, " data-ignore-a11y-check="" /><br />we see that <br /></span><span><img class="equation_image" title=" \text{Rep}_B(\vec{v}) = \begin{pmatrix} 3.5 \\ 1.5 \end{pmatrix}. " src="https://canvas.du.edu/equation_images/%2520%255Ctext%257BRep%257D_B(%255Cvec%257Bv%257D)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25203.5%2520%255C%255C%25201.5%2520%255Cend%257Bpmatrix%257D.%2520?scale=1" alt="LaTeX:  \text{Rep}_B(\vec{v}) = \begin{pmatrix} 3.5 \\ 1.5 \end{pmatrix}. " data-equation-content=" \text{Rep}_B(\vec{v}) = \begin{pmatrix} 3.5 \\ 1.5 \end{pmatrix}. " data-ignore-a11y-check="" /><br />Using the theorem, we conclude that </span><br /><img class="equation_image" title="\text{Rep}_D(\pi(\vec{v})) = \begin{pmatrix} -1 &amp; 1 \\ 1/2 &amp; -1/2 \end{pmatrix} \begin{pmatrix} 3.5 \\ 1.5 \end{pmatrix} = \begin{pmatrix} -2 \\ 1 \end{pmatrix}." src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_D(%255Cpi(%255Cvec%257Bv%257D))%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520-1%2520%2526%25201%2520%255C%255C%25201%252F2%2520%2526%2520-1%252F2%2520%255Cend%257Bpmatrix%257D%2520%255Cbegin%257Bpmatrix%257D%25203.5%2520%255C%255C%25201.5%2520%255Cend%257Bpmatrix%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520-2%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D.?scale=1" alt="LaTeX: \text{Rep}_D(\pi(\vec{v})) = \begin{pmatrix} -1 &amp; 1 \\ 1/2 &amp; -1/2 \end{pmatrix} \begin{pmatrix} 3.5 \\ 1.5 \end{pmatrix} = \begin{pmatrix} -2 \\ 1 \end{pmatrix}." data-equation-content="\text{Rep}_D(\pi(\vec{v})) = \begin{pmatrix} -1 &amp; 1 \\ 1/2 &amp; -1/2 \end{pmatrix} \begin{pmatrix} 3.5 \\ 1.5 \end{pmatrix} = \begin{pmatrix} -2 \\ 1 \end{pmatrix}." data-ignore-a11y-check="" /><br /><span>This means that </span><br /><img class="equation_image" title="\pi(\vec{v}) = -2 \begin{pmatrix} 0 \\ 1 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 2 \end{pmatrix} = \begin{pmatrix} 2 \\ 0\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cpi(%255Cvec%257Bv%257D)%2520%253D%2520-2%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%2520%252B%25201%2520%255Cbegin%257Bpmatrix%257D%25202%2520%255C%255C%25202%2520%255Cend%257Bpmatrix%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25202%2520%255C%255C%25200%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \pi(\vec{v}) = -2 \begin{pmatrix} 0 \\ 1 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 2 \end{pmatrix} = \begin{pmatrix} 2 \\ 0\end{pmatrix}" data-equation-content="\pi(\vec{v}) = -2 \begin{pmatrix} 0 \\ 1 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 2 \end{pmatrix} = \begin{pmatrix} 2 \\ 0\end{pmatrix}" data-ignore-a11y-check="" /><br />which is precisely what we are supposed to get.&nbsp;</p>
    </ul></details>
	
	<details><summary>Lecture 14 on 10/27/2022
</summary>
	<ul>
        <p><span>We finished the discussion of Three.III.2 and started Three.IV.2. Please spend enough time working on problems in Three.III.2, which contains the most important technique in studying linear maps. The midterm next Friday will also be heavily focused on this section. </span><br /><br /><span>Midterm 2 will cover everything up to Three.III.2. More details will be announced later. I will also compile a conceptual guide to assist your review. </span><br /><br /><span>We are currently three lectures behind schedule. To catch up with the schedule, the quiz next Tuesday will again be 10 minutes, with only one problem chosen from the ticked exercises in Three.II.2, Three.III.1 and Three.III.2. I will also accelerate by covering only the essential knowledge in Three.IV.1 and focus more on Three.IV.2 and Three.IV.3. </span></p>
    </ul></details>
	
	<details><summary>Lecture 15 on 11/1/2022
</summary>
	<ul>
        <p>We finished the discussion of Three.IV.1, Three.IV.2, and almost all of Three.IV.3. We established the isomorphism between the space of linear maps from <img class="equation_image" title="V" src="https://canvas.du.edu/equation_images/V?scale=1" alt="LaTeX: V" data-equation-content="V" data-ignore-a11y-check="" /> to <img class="equation_image" title="W" src="https://canvas.du.edu/equation_images/W?scale=1" alt="LaTeX: W" data-equation-content="W" data-ignore-a11y-check="" /> and the space of matrices of size <img class="equation_image" title="\dim(W) \times \dim(V)" src="https://canvas.du.edu/equation_images/%255Cdim(W)%2520%255Ctimes%2520%255Cdim(V)?scale=1" alt="LaTeX: \dim(W) \times \dim(V)" data-equation-content="\dim(W) \times \dim(V)" data-ignore-a11y-check="" />. Note that the isomorphism depends on the choice of bases of <img class="equation_image" title="V" src="https://canvas.du.edu/equation_images/V?scale=1" alt="LaTeX: V" data-equation-content="V" data-ignore-a11y-check="" /> and <img class="equation_image" title="W" src="https://canvas.du.edu/equation_images/W?scale=1" alt="LaTeX: W" data-equation-content="W" data-ignore-a11y-check="" />. Different choices of bases result in different isomorphisms. We also discussed matrix-matrix multiplication and used it to represent compositions of linear maps. Then we discussed the left and right multiplications of the (i,j)-entry matrix <img class="equation_image" title="e_{ij}" src="https://canvas.du.edu/equation_images/e_%257Bij%257D?scale=1" alt="LaTeX: e_{ij}" data-equation-content="e_{ij}" data-ignore-a11y-check="" />, the diagonal matrix, and showed what they do to a matrix. We also realized the row operations as left multiplications of certain matrices.&nbsp;</p>
<p>One pedagogical mistake I made today: I mentioned that matrix-matrix multiplications are not commutative but forgot to mention that matrix-matrix multiplications are associative. It is technical, though possible, to work it out using the formula. But now we know that matrix multiplications represent compositions of linear maps, we can approach the associativity more conceptually. Indeed, the associativity of matrix-matrix multiplications simply follows from the associativity of compositions.&nbsp;</p>
<p>In greater detail, let <img class="equation_image" title="h: V_1 \to V_2, g: V_2 \to V_3, f: V_3 \to V_4" src="https://canvas.du.edu/equation_images/h%253A%2520V_1%2520%255Cto%2520V_2%252C%2520g%253A%2520V_2%2520%255Cto%2520V_3%252C%2520f%253A%2520V_3%2520%255Cto%2520V_4?scale=1" alt="LaTeX: h: V_1 \to V_2, g: V_2 \to V_3, f: V_3 \to V_4" data-equation-content="h: V_1 \to V_2, g: V_2 \to V_3, f: V_3 \to V_4" data-ignore-a11y-check="" /> be linear maps. The associativity of the composition of maps says&nbsp;<br /><img class="equation_image" title="(f\circ g)\circ h = f \circ (g \circ h)." src="https://canvas.du.edu/equation_images/(f%255Ccirc%2520g)%255Ccirc%2520h%2520%253D%2520f%2520%255Ccirc%2520(g%2520%255Ccirc%2520h).?scale=1" alt="LaTeX: (f\circ g)\circ h = f \circ (g \circ h)." data-equation-content="(f\circ g)\circ h = f \circ (g \circ h)." data-ignore-a11y-check="" /><br />Why? Fix an arbitrary <img class="equation_image" title="\vec{v}\in V_1" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%255Cin%2520V_1?scale=1" alt="LaTeX: \vec{v}\in V_1" data-equation-content="\vec{v}\in V_1" data-ignore-a11y-check="" />. Left-hand-side applied to <img class="equation_image" title=" \vec{v}\in V_1 " src="https://canvas.du.edu/equation_images/%2520%255Cvec%257Bv%257D%255Cin%2520V_1%2520?scale=1" alt="LaTeX:  \vec{v}\in V_1 " data-equation-content=" \vec{v}\in V_1 " data-ignore-a11y-check="" /> simply give<br /><img class="equation_image" title="((f\circ g)\circ h)(\vec{v}) = (f\circ g)(h(\vec{v})) = f(g(h(\vec{v}))).&nbsp;" src="https://canvas.du.edu/equation_images/((f%255Ccirc%2520g)%255Ccirc%2520h)(%255Cvec%257Bv%257D)%2520%253D%2520(f%255Ccirc%2520g)(h(%255Cvec%257Bv%257D))%2520%253D%2520f(g(h(%255Cvec%257Bv%257D))).%25C2%25A0?scale=1" alt="LaTeX: ((f\circ g)\circ h)(\vec{v}) = (f\circ g)(h(\vec{v})) = f(g(h(\vec{v}))).&nbsp;" data-equation-content="((f\circ g)\circ h)(\vec{v}) = (f\circ g)(h(\vec{v})) = f(g(h(\vec{v}))).&nbsp;" data-ignore-a11y-check="" /><br />Right-hand-side applied to <img class="equation_image" title="\vec{v}\in V_1" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D%255Cin%2520V_1?scale=1" alt="LaTeX: \vec{v}\in V_1" data-equation-content="\vec{v}\in V_1" data-ignore-a11y-check="" /> simply give<br /><img class="equation_image" title="(f\circ (g\circ h))(\vec{v}) = f((g\circ h)(\vec{v})) = f(g(h(\vec{v}))).&nbsp;
" src="https://canvas.du.edu/equation_images/(f%255Ccirc%2520(g%255Ccirc%2520h))(%255Cvec%257Bv%257D)%2520%253D%2520f((g%255Ccirc%2520h)(%255Cvec%257Bv%257D))%2520%253D%2520f(g(h(%255Cvec%257Bv%257D))).%25C2%25A0%250A?scale=1" alt="LaTeX: (f\circ (g\circ h))(\vec{v}) = f((g\circ h)(\vec{v})) = f(g(h(\vec{v}))).&nbsp;
" data-equation-content="(f\circ (g\circ h))(\vec{v}) = f((g\circ h)(\vec{v})) = f(g(h(\vec{v}))).&nbsp;
" data-ignore-a11y-check="" /><br />Thus they are equal.&nbsp;</p>
<p>Now let <img class="equation_image" title="A" src="https://canvas.du.edu/equation_images/A?scale=1" alt="LaTeX: A" data-equation-content="A" data-ignore-a11y-check="" /> be a matrix of size <img class="equation_image" title="m\times r_1" src="https://canvas.du.edu/equation_images/m%255Ctimes%2520r_1?scale=1" alt="LaTeX: m\times r_1" data-equation-content="m\times r_1" data-ignore-a11y-check="" />, <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" /> be a matrix of size <img class="equation_image" title="r_1 \times r_2" src="https://canvas.du.edu/equation_images/r_1%2520%255Ctimes%2520r_2?scale=1" alt="LaTeX: r_1 \times r_2" data-equation-content="r_1 \times r_2" data-ignore-a11y-check="" />, and <img class="equation_image" title="C" src="https://canvas.du.edu/equation_images/C?scale=1" alt="LaTeX: C" data-equation-content="C" data-ignore-a11y-check="" /> be a matrix of size <img class="equation_image" title="r_2 \times n" src="https://canvas.du.edu/equation_images/r_2%2520%255Ctimes%2520n?scale=1" alt="LaTeX: r_2 \times n" data-equation-content="r_2 \times n" data-ignore-a11y-check="" />. Pick the standard bases <img class="equation_image" title="\mathcal{E}_{m}, \mathcal{E}_{r_1}, \mathcal{E}_{r_2}, \mathcal{E}_{n}" src="https://canvas.du.edu/equation_images/%255Cmathcal%257BE%257D_%257Bm%257D%252C%2520%255Cmathcal%257BE%257D_%257Br_1%257D%252C%2520%255Cmathcal%257BE%257D_%257Br_2%257D%252C%2520%255Cmathcal%257BE%257D_%257Bn%257D?scale=1" alt="LaTeX: \mathcal{E}_{m}, \mathcal{E}_{r_1}, \mathcal{E}_{r_2}, \mathcal{E}_{n}" data-equation-content="\mathcal{E}_{m}, \mathcal{E}_{r_1}, \mathcal{E}_{r_2}, \mathcal{E}_{n}" data-ignore-a11y-check="" /> respectively for <img class="equation_image" title=" \mathbb{R}^{m}, \mathbb{R}^{r_1}, \mathbb{R}^{r_2}, \mathbb{R}^{n}" src="https://canvas.du.edu/equation_images/%2520%255Cmathbb%257BR%257D%255E%257Bm%257D%252C%2520%255Cmathbb%257BR%257D%255E%257Br_1%257D%252C%2520%255Cmathbb%257BR%257D%255E%257Br_2%257D%252C%2520%255Cmathbb%257BR%257D%255E%257Bn%257D?scale=1" alt="LaTeX:  \mathbb{R}^{m}, \mathbb{R}^{r_1}, \mathbb{R}^{r_2}, \mathbb{R}^{n}" data-equation-content=" \mathbb{R}^{m}, \mathbb{R}^{r_1}, \mathbb{R}^{r_2}, \mathbb{R}^{n}" data-ignore-a11y-check="" />. Then <img class="equation_image" title="A" src="https://canvas.du.edu/equation_images/A?scale=1" alt="LaTeX: A" data-equation-content="A" data-ignore-a11y-check="" /> is representing a linear map <img class="equation_image" title="f: \mathbb{R}^{r_1} \to \mathbb{R}^{m}" src="https://canvas.du.edu/equation_images/f%253A%2520%255Cmathbb%257BR%257D%255E%257Br_1%257D%2520%255Cto%2520%255Cmathbb%257BR%257D%255E%257Bm%257D?scale=1" alt="LaTeX: f: \mathbb{R}^{r_1} \to \mathbb{R}^{m}" data-equation-content="f: \mathbb{R}^{r_1} \to \mathbb{R}^{m}" data-ignore-a11y-check="" />, <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" /> is representing a linear map <img class="equation_image" title="g: \mathbb{R}^{r_2} \to \mathbb{R}^{r_1}" src="https://canvas.du.edu/equation_images/g%253A%2520%255Cmathbb%257BR%257D%255E%257Br_2%257D%2520%255Cto%2520%255Cmathbb%257BR%257D%255E%257Br_1%257D?scale=1" alt="LaTeX: g: \mathbb{R}^{r_2} \to \mathbb{R}^{r_1}" data-equation-content="g: \mathbb{R}^{r_2} \to \mathbb{R}^{r_1}" data-ignore-a11y-check="" />, <img class="equation_image" title="C" src="https://canvas.du.edu/equation_images/C?scale=1" alt="LaTeX: C" data-equation-content="C" data-ignore-a11y-check="" /> is representing a linear map <img class="equation_image" title="\mathbb{R}^{n} \to \mathbb{R}^{r_2}" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BR%257D%255E%257Bn%257D%2520%255Cto%2520%255Cmathbb%257BR%257D%255E%257Br_2%257D?scale=1" alt="LaTeX: \mathbb{R}^{n} \to \mathbb{R}^{r_2}" data-equation-content="\mathbb{R}^{n} \to \mathbb{R}^{r_2}" data-ignore-a11y-check="" />. Since <img class="equation_image" title="(f\circ g)\circ h = f\circ (g\circ h)" src="https://canvas.du.edu/equation_images/(f%255Ccirc%2520g)%255Ccirc%2520h%2520%253D%2520f%255Ccirc%2520(g%255Ccirc%2520h)?scale=1" alt="LaTeX: (f\circ g)\circ h = f\circ (g\circ h)" data-equation-content="(f\circ g)\circ h = f\circ (g\circ h)" data-ignore-a11y-check="" />, we have <img class="equation_image" title="(AB)C = A(BC)" src="https://canvas.du.edu/equation_images/(AB)C%2520%253D%2520A(BC)?scale=1" alt="LaTeX: (AB)C = A(BC)" data-equation-content="(AB)C = A(BC)" data-ignore-a11y-check="" />.&nbsp;</p>
<p>Three particular cases:<br />1. <img class="equation_image" title="C" src="https://canvas.du.edu/equation_images/C?scale=1" alt="LaTeX: C" data-equation-content="C" data-ignore-a11y-check="" /> is a column vector <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" />. Then this says that on the vector <img class="equation_image" title="\vec{v}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bv%257D?scale=1" alt="LaTeX: \vec{v}" data-equation-content="\vec{v}" data-ignore-a11y-check="" />, the consecutive left multiplication first by <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" /> and then by <img class="equation_image" title="A" src="https://canvas.du.edu/equation_images/A?scale=1" alt="LaTeX: A" data-equation-content="A" data-ignore-a11y-check="" /> results in the same column vector as the left multiplication of the matrix <img class="equation_image" title="AB" src="https://canvas.du.edu/equation_images/AB?scale=1" alt="LaTeX: AB" data-equation-content="AB" data-ignore-a11y-check="" />.&nbsp;<br />2. <img class="equation_image" title="A" src="https://canvas.du.edu/equation_images/A?scale=1" alt="LaTeX: A" data-equation-content="A" data-ignore-a11y-check="" /> is a row vector <img class="equation_image" title="\vec{w}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bw%257D?scale=1" alt="LaTeX: \vec{w}" data-equation-content="\vec{w}" data-ignore-a11y-check="" />. Then this says that on the vector <img class="equation_image" title="\vec{w}" src="https://canvas.du.edu/equation_images/%255Cvec%257Bw%257D?scale=1" alt="LaTeX: \vec{w}" data-equation-content="\vec{w}" data-ignore-a11y-check="" />, the consecutive right multiplication first by <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" />&nbsp;and then by <img class="equation_image" title="C" src="https://canvas.du.edu/equation_images/C?scale=1" alt="LaTeX: C" data-equation-content="C" data-ignore-a11y-check="" /> results in the same column vector as the right multiplication of the matrix <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" /><img class="equation_image" title="C" src="https://canvas.du.edu/equation_images/C?scale=1" alt="LaTeX: C" data-equation-content="C" data-ignore-a11y-check="" />.&nbsp;<br />3. <img class="equation_image" title="A" src="https://canvas.du.edu/equation_images/A?scale=1" alt="LaTeX: A" data-equation-content="A" data-ignore-a11y-check="" />, <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" />, and <img class="equation_image" title="C" src="https://canvas.du.edu/equation_images/C?scale=1" alt="LaTeX: C" data-equation-content="C" data-ignore-a11y-check="" /> are square matrices of size n\times n. We have <img class="equation_image" title="(AB)C = A(BC)" src="https://canvas.du.edu/equation_images/(AB)C%2520%253D%2520A(BC)?scale=1" alt="LaTeX: (AB)C = A(BC)" data-equation-content="(AB)C = A(BC)" data-ignore-a11y-check="" />. In higher-level mathematics, together with the properties of the identity matrix, the vector space of square matrices of size n\times n forms a ring. It might be the only source of examples for noncommutative rings in undergraduate-level abstract algebra.&nbsp;</p>
    </ul></details>
	
	<details><summary>Lecture 16 on 11/3/2022</summary>
	<ul>
        <p>We finished the discussion of the Three.IV.4 and Three.V.1 in the textbook. We began the discussion of Three.V.2 at the end of the lecture and introduced the formula&nbsp;<br /><img class="equation_image" title="\text{Rep}_{\hat{B}, \hat{D}}(h) = \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) \text{Rep}_{\hat{B}, B}
            " src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520%255Chat%257BD%257D%257D(h)%2520%253D%2520%255Ctext%257BRep%257D_%257BD%252C%2520%255Chat%257BD%257D%257D(id)%2520%255Ctext%257BRep%257D_%257BB%252C%2520D%257D(h)%2520%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520B%257D%250A?scale=1" alt="LaTeX: \text{Rep}_{\hat{B}, \hat{D}}(h) = \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) \text{Rep}_{\hat{B}, B}
            " data-equation-content="\text{Rep}_{\hat{B}, \hat{D}}(h) = \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) \text{Rep}_{\hat{B}, B}
            " data-ignore-a11y-check="" /><br />for a homomorphism <img class="equation_image" title="h: V\to W" src="https://canvas.du.edu/equation_images/h%253A%2520V%255Cto%2520W?scale=1" alt="LaTeX: h: V\to W" data-equation-content="h: V\to W" data-ignore-a11y-check="" /> between two vector spaces, where <img class="equation_image" title="B, \hat{B}" src="https://canvas.du.edu/equation_images/B%252C%2520%255Chat%257BB%257D?scale=1" alt="LaTeX: B, \hat{B}" data-equation-content="B, \hat{B}" data-ignore-a11y-check="" /> are bases for V, <img class="equation_image" title="D, \hat{D}" src="https://canvas.du.edu/equation_images/D%252C%2520%255Chat%257BD%257D?scale=1" alt="LaTeX: D, \hat{D}" data-equation-content="D, \hat{D}" data-ignore-a11y-check="" /> are bases for W. But I did not provide sufficient justification for the formula. We shall start from there next Tuesday.&nbsp;</p>
            <p>Following Tuesday's quiz will consist of two problems on matrix multiplications and finding the inverses. The computations will be light enough to finish in 10 minutes. I understand that you have been spending all your time dealing with the midterm this week. But please do attempt the related ticked exercises in the textbook. Should you have any confusion, do not hesitate to report it. I will be available either on Zoom or in person during the weekend.&nbsp;</p>
            <p>A pedagogical mistake I have made in class: Although I have mentioned in class that the matrix <img class="equation_image" title="\text{Rep}_{B, \hat{B}}(id)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252C%2520%255Chat%257BB%257D%257D(id)?scale=1" alt="LaTeX: \text{Rep}_{B, \hat{B}}(id)" data-equation-content="\text{Rep}_{B, \hat{B}}(id)" data-ignore-a11y-check="" /> is the inverse to <img class="equation_image" title="\text{Rep}_{\hat{B}, B}(id)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520B%257D(id)?scale=1" alt="LaTeX: \text{Rep}_{\hat{B}, B}(id)" data-equation-content="\text{Rep}_{\hat{B}, B}(id)" data-ignore-a11y-check="" />, I did not provide a comprehensive argument. I should provide one: Recall that for two linear maps <img class="equation_image" title="g: W\to U" src="https://canvas.du.edu/equation_images/g%253A%2520W%255Cto%2520U?scale=1" alt="LaTeX: g: W\to U" data-equation-content="g: W\to U" data-ignore-a11y-check="" /> and <img class="equation_image" title="h: V\to W" src="https://canvas.du.edu/equation_images/h%253A%2520V%255Cto%2520W?scale=1" alt="LaTeX: h: V\to W" data-equation-content="h: V\to W" data-ignore-a11y-check="" />, if <img class="equation_image" title="B, C, D" src="https://canvas.du.edu/equation_images/B%252C%2520C%252C%2520D?scale=1" alt="LaTeX: B, C, D" data-equation-content="B, C, D" data-ignore-a11y-check="" /> are bases for <img class="equation_image" title="V, W, U" src="https://canvas.du.edu/equation_images/V%252C%2520W%252C%2520U?scale=1" alt="LaTeX: V, W, U" data-equation-content="V, W, U" data-ignore-a11y-check="" /> respectively, then&nbsp;<br /><img class="equation_image" title="\text{Rep}_{B, D}(g \circ h) = \text{Rep}_{C, D}(g) \text{Rep}_{B, C}(h)
            " src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252C%2520D%257D(g%2520%255Ccirc%2520h)%2520%253D%2520%255Ctext%257BRep%257D_%257BC%252C%2520D%257D(g)%2520%255Ctext%257BRep%257D_%257BB%252C%2520C%257D(h)%250A?scale=1" alt="LaTeX: \text{Rep}_{B, D}(g \circ h) = \text{Rep}_{C, D}(g) \text{Rep}_{B, C}(h)
            " data-equation-content="\text{Rep}_{B, D}(g \circ h) = \text{Rep}_{C, D}(g) \text{Rep}_{B, C}(h)
            " data-ignore-a11y-check="" /><br />Now specialize to the case where <img class="equation_image" title="V=W=U, g=h=id, C = \hat{B}" src="https://canvas.du.edu/equation_images/V%253DW%253DU%252C%2520g%253Dh%253Did%252C%2520C%2520%253D%2520%255Chat%257BB%257D?scale=1" alt="LaTeX: V=W=U, g=h=id, C = \hat{B}" data-equation-content="V=W=U, g=h=id, C = \hat{B}" data-ignore-a11y-check="" /> and <img class="equation_image" title="D = B" src="https://canvas.du.edu/equation_images/D%2520%253D%2520B?scale=1" alt="LaTeX: D = B" data-equation-content="D = B" data-ignore-a11y-check="" />. Then we have&nbsp;<br /><img class="equation_image" title="\text{Rep}_{B, B}(id \circ id) = \text{Rep}_{\hat{B}, B}(id) \text{Rep}_{B, \hat{B}}(id)
            " src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252C%2520B%257D(id%2520%255Ccirc%2520id)%2520%253D%2520%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520B%257D(id)%2520%255Ctext%257BRep%257D_%257BB%252C%2520%255Chat%257BB%257D%257D(id)%250A?scale=1" alt="LaTeX: \text{Rep}_{B, B}(id \circ id) = \text{Rep}_{\hat{B}, B}(id) \text{Rep}_{B, \hat{B}}(id)
            " data-equation-content="\text{Rep}_{B, B}(id \circ id) = \text{Rep}_{\hat{B}, B}(id) \text{Rep}_{B, \hat{B}}(id)
            " data-ignore-a11y-check="" /><br />Note that <img class="equation_image" title="id \circ id = id" src="https://canvas.du.edu/equation_images/id%2520%255Ccirc%2520id%2520%253D%2520id?scale=1" alt="LaTeX: id \circ id = id" data-equation-content="id \circ id = id" data-ignore-a11y-check="" />, so the left-hand-side is <img class="equation_image" title="\text{Rep}_{B, B}(id)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252C%2520B%257D(id)?scale=1" alt="LaTeX: \text{Rep}_{B, B}(id)" data-equation-content="\text{Rep}_{B, B}(id)" data-ignore-a11y-check="" />. I will leave it to you to check that <img class="equation_image" title="\text{Rep}_{B, B}(id)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252C%2520B%257D(id)?scale=1" alt="LaTeX: \text{Rep}_{B, B}(id)" data-equation-content="\text{Rep}_{B, B}(id)" data-ignore-a11y-check="" /> is precisely the identity matrix (it's easily seen if you follow the definition of the representation). Thus the claim is proved.&nbsp;</p>
    </ul></details>
	
	<details><summary>Lecture 17 on 11/8/2022
</summary>
	<ul>
        <p>We covered Three.V.2 today and started Chapter 4. The main problem we solved today is how two representations of a linear map are related. More precisely, given a linear map <img class="equation_image" title="h" src="https://canvas.du.edu/equation_images/h?scale=1" alt="LaTeX: h" data-equation-content="h" data-ignore-a11y-check="" /> and two bases <img class="equation_image" title="B, \hat{B}" src="https://canvas.du.edu/equation_images/B%252C%2520%255Chat%257BB%257D?scale=1" alt="LaTeX: B, \hat{B}" data-equation-content="B, \hat{B}" data-ignore-a11y-check="" /> of the domain and two bases of the codomain <img class="equation_image" title="D, \hat{D}" src="https://canvas.du.edu/equation_images/D%252C%2520%255Chat%257BD%257D?scale=1" alt="LaTeX: D, \hat{D}" data-equation-content="D, \hat{D}" data-ignore-a11y-check="" />, there will be two matrix representations <img class="equation_image" title="\text{Rep}_{B, D}(h)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252C%2520D%257D(h)?scale=1" alt="LaTeX: \text{Rep}_{B, D}(h)" data-equation-content="\text{Rep}_{B, D}(h)" data-ignore-a11y-check="" /> and <img class="equation_image" title="\text{Rep}_{\hat{B}, \hat{D}}" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520%255Chat%257BD%257D%257D?scale=1" alt="LaTeX: \text{Rep}_{\hat{B}, \hat{D}}" data-equation-content="\text{Rep}_{\hat{B}, \hat{D}}" data-ignore-a11y-check="" /> for the linear map. These two matrix representations are related by the change of basis matrices <img class="equation_image" title="\text{Rep}_{B, \hat{B}}(id)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BB%252C%2520%255Chat%257BB%257D%257D(id)?scale=1" alt="LaTeX: \text{Rep}_{B, \hat{B}}(id)" data-equation-content="\text{Rep}_{B, \hat{B}}(id)" data-ignore-a11y-check="" /> and <img class="equation_image" title="\text{Rep}_{D, \hat{D}}(id)" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257BD%252C%2520%255Chat%257BD%257D%257D(id)?scale=1" alt="LaTeX: \text{Rep}_{D, \hat{D}}(id)" data-equation-content="\text{Rep}_{D, \hat{D}}(id)" data-ignore-a11y-check="" />. The formula can be summarized as<br /><img class="equation_image" title="\text{Rep}_{\hat{B}, \hat{D}} = \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) \text{Rep}_{\hat{D}, D}(id)
= \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) (\text{Rep}_{D, \hat{D}}(id))^{-1}" src="https://canvas.du.edu/equation_images/%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520%255Chat%257BD%257D%257D%2520%253D%2520%255Ctext%257BRep%257D_%257BD%252C%2520%255Chat%257BD%257D%257D(id)%2520%255Ctext%257BRep%257D_%257BB%252C%2520D%257D(h)%2520%255Ctext%257BRep%257D_%257B%255Chat%257BD%257D%252C%2520D%257D(id)%250A%253D%2520%255Ctext%257BRep%257D_%257BD%252C%2520%255Chat%257BD%257D%257D(id)%2520%255Ctext%257BRep%257D_%257BB%252C%2520D%257D(h)%2520(%255Ctext%257BRep%257D_%257BD%252C%2520%255Chat%257BD%257D%257D(id))%255E%257B-1%257D?scale=1" alt="LaTeX: \text{Rep}_{\hat{B}, \hat{D}} = \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) \text{Rep}_{\hat{D}, D}(id)
= \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) (\text{Rep}_{D, \hat{D}}(id))^{-1}" data-equation-content="\text{Rep}_{\hat{B}, \hat{D}} = \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) \text{Rep}_{\hat{D}, D}(id)
= \text{Rep}_{D, \hat{D}}(id) \text{Rep}_{B, D}(h) (\text{Rep}_{D, \hat{D}}(id))^{-1}" data-ignore-a11y-check="" /><br />We went through an example in class and showed that they are the same. Will asked the question why we still want to perform matrix multiplications. My answer in class omitted an important fact that matrix multiplications are more convenient for a computer to carry out. The relation is both theoretically important and convenient in practice.&nbsp;</p>
<p>The formula above motivates the definition of matrix equivalence: Two matrices <img class="equation_image" title="H" src="https://canvas.du.edu/equation_images/H?scale=1" alt="LaTeX: H" data-equation-content="H" data-ignore-a11y-check="" /> and <img class="equation_image" title="\hat{H}" src="https://canvas.du.edu/equation_images/%255Chat%257BH%257D?scale=1" alt="LaTeX: \hat{H}" data-equation-content="\hat{H}" data-ignore-a11y-check="" /> are matrix equivalent if for some nonsingular matrices <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" /> and <img class="equation_image" title="D" src="https://canvas.du.edu/equation_images/D?scale=1" alt="LaTeX: D" data-equation-content="D" data-ignore-a11y-check="" />, <img class="equation_image" title="\hat{H} = B H D" src="https://canvas.du.edu/equation_images/%255Chat%257BH%257D%2520%253D%2520B%2520H%2520D?scale=1" alt="LaTeX: \hat{H} = B H D" data-equation-content="\hat{H} = B H D" data-ignore-a11y-check="" />. Note that nonsingular matrices are products of elementary matrices. So left multiplication by <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" /> is equivalent to performing a series of row operations, while right multiplication by <img class="equation_image" title="D" src="https://canvas.du.edu/equation_images/D?scale=1" alt="LaTeX: D" data-equation-content="D" data-ignore-a11y-check="" /> is equivalent to performing a series of column operations. So two matrices are matrix equivalent if one passes to another via the row operations AND column operations. Recall that row operations can reduce a matrix into the reduced echelon form. Now with column operations, we can reduce it further to a matrix of the form&nbsp;<br /><img class="equation_image" title="\begin{pmatrix}
&nbsp;I_{k\times k} &amp; \text{zeros}\\
\text{zeros} &amp; \text{zeros}
\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%25C2%25A0I_%257Bk%255Ctimes%2520k%257D%2520%2526%2520%255Ctext%257Bzeros%257D%255C%255C%250A%255Ctext%257Bzeros%257D%2520%2526%2520%255Ctext%257Bzeros%257D%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix}
&nbsp;I_{k\times k} &amp; \text{zeros}\\
\text{zeros} &amp; \text{zeros}
\end{pmatrix}" data-equation-content="\begin{pmatrix}
&nbsp;I_{k\times k} &amp; \text{zeros}\\
\text{zeros} &amp; \text{zeros}
\end{pmatrix}" data-ignore-a11y-check="" /><br />This matrix is called the block partial-identity matrix by the textbook and serves the role of canonical form for the matrix equivalence relation. In particular, we have come to the conclusion that every homomorphism can be represented by a block partial-identity matrix with respect to some bases of the domain and the codomain.&nbsp;</p>
<p>We started the discussion of determinants as a "measure" for a matrix to be nonsingular. Geometrically, the determinant of the matrix is precisely the "<strong>oriented volume</strong>" of the parallelogram formed by its row vectors (or column vectors). But to start with, we will define the determinant as a function satisfying the following conditions:&nbsp;<br />1. Adding k times i-th row to j-th row won't change the determinant.&nbsp;<br />2. Swapping two rows generates a negative sign.&nbsp;<br />3. Rescaling a row changes the determinant by the scalar.&nbsp;<br />4. Determinant of the identity matrix is 1.&nbsp;<br />These conditions allow us to compute the determinant using row operations. We went over an example in class purely using these conditions. Next lecture, we will introduce more shortcuts to compute the determinants.&nbsp;</p>
<p>I should have mentioned in class that all these four conditions have natural geometric meanings. The first condition simply says that translating the parallelogram along one direction does not change its volume. The second condition says swapping two vectors changes the orientation. The third condition simply says rescaling a side leads to rescaling the volume. The fourth condition simply says the parallelogram formed by the standard basis is one.&nbsp;</p>
    </ul></details>
	
	<details><summary>Lecture 18 on 11/10/2022</summary>
	<ul>
        <p><span data-preserver-spaces="true">Today we finished all Chapter IV at an insane pace because of the accumulated pedagogical mistakes made in the earlier part of classes. I had to skip the geometric interpretations of the determinants and Cramer's Rule. But I decided to include the confusing Laplace expansion since we shall use it extensively in the computation of eigenvalues.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Homework Exercises for this week will be Three.V.1, Three.V.2, Four.I.2, and Four.III.1. These are the relevant sections in the final exam.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Since many students missed the lecture today, I am posting everything I discussed in class here and will supplement some examples I failed to cover in class:&nbsp;</span></p>
<p><span data-preserver-spaces="true">The determinant is a function <img class="equation_image" title="\det: \mathcal{M}_{n\times n} \to \mathbb{R}" src="https://canvas.du.edu/equation_images/%255Cdet%253A%2520%255Cmathcal%257BM%257D_%257Bn%255Ctimes%2520n%257D%2520%255Cto%2520%255Cmathbb%257BR%257D?scale=1" alt="LaTeX: \det: \mathcal{M}_{n\times n} \to \mathbb{R}" data-equation-content="\det: \mathcal{M}_{n\times n} \to \mathbb{R}" data-ignore-a11y-check="" /> satisfying:<br /></span><span data-preserver-spaces="true">(a) <img class="equation_image" title="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ k\vec{\rho}_i + \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix}&nbsp;" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520k%255Cvec%257B%255Crho%257D_i%2520%252B%2520%255Cvec%257B%255Crho%257D_j%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_j%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D%25C2%25A0?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ k\vec{\rho}_i + \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix}&nbsp;" data-equation-content="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ k\vec{\rho}_i + \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix}&nbsp;" data-ignore-a11y-check="" /><br />In words: adding k times a row to another row does not change the determinant. <br /></span>(b) <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ k \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = k\cdot&nbsp; \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}&nbsp;" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520k%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520k%255Ccdot%25C2%25A0%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D%25C2%25A0?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ k \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = k\cdot&nbsp; \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}&nbsp;" data-equation-content="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ k \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = k\cdot&nbsp; \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}&nbsp;" data-ignore-a11y-check="" /><br />In words, rescaling a row by k results in rescaling the determinant by k. <br />(c) <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_j%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520-%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_j%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" data-equation-content="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" data-ignore-a11y-check="" /><br />In words, swapping two rows generates a negative movement. <br />(d) <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\det I_{n\times n} = 1" src="https://canvas.du.edu/equation_images/%255Cdet%2520I_%257Bn%255Ctimes%2520n%257D%2520%253D%25201?scale=1" alt="LaTeX: \det I_{n\times n} = 1" data-equation-content="\det I_{n\times n} = 1" data-ignore-a11y-check="" />. <br />In words, the determinant of the identity matrix is 1.</p>
<p>These properties allow us to compute the determinants using Gaussian reduction. A theorem guarantees that the determinant function satisfying all these conditions exists uniquely. The proof is not difficult but involves some deep knowledge of the sign of permutation matrices. We shall not be too worried about it.&nbsp;</p>
<p><strong><span data-preserver-spaces="true">Notation:</span></strong><span data-preserver-spaces="true"> For matrix A, we may denote the determinant as <img class="equation_image" title="\det(A)" src="https://canvas.du.edu/equation_images/%255Cdet(A)?scale=1" alt="LaTeX: \det(A)" data-equation-content="\det(A)" data-ignore-a11y-check="" /> or <img class="equation_image" title="|A|" src="https://canvas.du.edu/equation_images/%257CA%257C?scale=1" alt="LaTeX: |A|" data-equation-content="|A|" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Here are the properties you need to keep in mind:&nbsp;</span></p>
<ol>
    <li><span data-preserver-spaces="true">If a matrix contains a zero row, then the determinant is zero. <br /></span>Proof: <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\det\begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
\vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}&nbsp;
= \det \begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
2\cdot \vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}
= 2\cdot \det \begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
\vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}&nbsp;" src="https://canvas.du.edu/equation_images/%255Cdet%255Cbegin%257Bpmatrix%257D%25C2%25A0%250A%255Cvec%257B%255Crho%257D_1%255C%255C%250A%255Cvdots%2520%255C%255C%250A%255Cvec%257B0%257D%2520%255C%255C%250A%255Cvdots%2520%255C%255C%250A%255Cvec%257B%255Crho%257D_n%250A%255Cend%257Bpmatrix%257D%25C2%25A0%250A%253D%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%25C2%25A0%250A%255Cvec%257B%255Crho%257D_1%255C%255C%250A%255Cvdots%2520%255C%255C%250A2%255Ccdot%2520%255Cvec%257B0%257D%2520%255C%255C%250A%255Cvdots%2520%255C%255C%250A%255Cvec%257B%255Crho%257D_n%250A%255Cend%257Bpmatrix%257D%250A%253D%25202%255Ccdot%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%25C2%25A0%250A%255Cvec%257B%255Crho%257D_1%255C%255C%250A%255Cvdots%2520%255C%255C%250A%255Cvec%257B0%257D%2520%255C%255C%250A%255Cvdots%2520%255C%255C%250A%255Cvec%257B%255Crho%257D_n%250A%255Cend%257Bpmatrix%257D%25C2%25A0?scale=1" alt="LaTeX: \det\begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
\vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}&nbsp;
= \det \begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
2\cdot \vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}
= 2\cdot \det \begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
\vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}&nbsp;" data-equation-content="\det\begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
\vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}&nbsp;
= \det \begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
2\cdot \vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}
= 2\cdot \det \begin{pmatrix}&nbsp;
\vec{\rho}_1\\
\vdots \\
\vec{0} \\
\vdots \\
\vec{\rho}_n
\end{pmatrix}&nbsp;" data-ignore-a11y-check="" /><br />So the determinant of such a matrix is simply twice itself. But the only number <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="a" src="https://canvas.du.edu/equation_images/a?scale=1" alt="LaTeX: a" data-equation-content="a" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> satisfying </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="a = 2a" src="https://canvas.du.edu/equation_images/a%2520%253D%25202a?scale=1" alt="LaTeX: a = 2a" data-equation-content="a = 2a" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> is </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="a = 0" src="https://canvas.du.edu/equation_images/a%2520%253D%25200?scale=1" alt="LaTeX: a = 0" data-equation-content="a = 0" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">.</span></li>
    <li>If a matrix contains two identical rows, its determinant is zero. <br />Proof: Note that <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_j%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520-%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_j%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" data-equation-content="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho}_j \\ \vdots \\ \vec{\rho}_i \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" data-ignore-a11y-check="" />. Say <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title=" \vec{\rho}_i=\vec{\rho}_j = \vec{\rho}" src="https://canvas.du.edu/equation_images/%2520%255Cvec%257B%255Crho%257D_i%253D%255Cvec%257B%255Crho%257D_j%2520%253D%2520%255Cvec%257B%255Crho%257D?scale=1" alt="LaTeX:  \vec{\rho}_i=\vec{\rho}_j = \vec{\rho}" data-equation-content=" \vec{\rho}_i=\vec{\rho}_j = \vec{\rho}" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">. Then we have </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = -\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520-%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Crho%257D_1%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520%255Cvec%257B%255Crho%257D_n%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = -\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" data-equation-content="\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho}_n\end{pmatrix} = -\det \begin{pmatrix} \vec{\rho}_1 \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho} \\ \vdots \\ \vec{\rho}_n\end{pmatrix}" data-ignore-a11y-check="" />. <br />So the determinant of the matrix equals its opposite. But the only number satisfying <img class="equation_image" title="a = -a" src="https://canvas.du.edu/equation_images/a%2520%253D%2520-a?scale=1" alt="LaTeX: a = -a" data-equation-content="a = -a" data-ignore-a11y-check="" /> is <img class="equation_image" title="a = 0" src="https://canvas.du.edu/equation_images/a%2520%253D%25200?scale=1" alt="LaTeX: a = 0" data-equation-content="a = 0" data-ignore-a11y-check="" />.&nbsp;</li>
    <li><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" data-preserver-spaces="true">(Forgot to mention in class) The determinant of a matrix is nonzero if and only if the matrix is nonsingular. <br /></span>Proof: The row operations described in (a), (b), and (c) reduce the determinant of a matrix as a nonzero scalar multiple of the determinant of its reduced echelon form. If the reduced echelon form contains no zero rows, then it must be an identity matrix. Thus the determinant is nonzero. Otherwise, the reduced echelon form has a zero row. Then from 1, the determinant must be zero.</li>
    <li>The determinant of an upper-triangular matrix is the product of its diagonal elements. Similarly, the determinant of an upper-triangular matrix is the product of its diagonal elements<br />Proof: Consider the determinant <img class="equation_image" title="\begin{vmatrix}

t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1,n-1} &amp; t_{1n}\\

0 &amp; t_{22} &amp; \cdots &amp; t_{2,n-1} &amp; t_{2n}\\

\vdots &amp; \vdots &amp; &amp; \vdots\\

0 &amp; 0 &amp; \cdots &amp; t_{n-1, n-1} &amp; t_{n-1, n}\\

0 &amp; 0 &amp; \cdots &amp; 0 &amp; t_{nn}

\end{vmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bvmatrix%257D%250A%250At_%257B11%257D%2520%2526%2520t_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520t_%257B1%252Cn-1%257D%2520%2526%2520t_%257B1n%257D%255C%255C%250A%250A0%2520%2526%2520t_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520t_%257B2%252Cn-1%257D%2520%2526%2520t_%257B2n%257D%255C%255C%250A%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%255C%255C%250A%250A0%2520%2526%25200%2520%2526%2520%255Ccdots%2520%2526%2520t_%257Bn-1%252C%2520n-1%257D%2520%2526%2520t_%257Bn-1%252C%2520n%257D%255C%255C%250A%250A0%2520%2526%25200%2520%2526%2520%255Ccdots%2520%2526%25200%2520%2526%2520t_%257Bnn%257D%250A%250A%255Cend%257Bvmatrix%257D?scale=1" alt="LaTeX: \begin{vmatrix}

t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1,n-1} &amp; t_{1n}\\

0 &amp; t_{22} &amp; \cdots &amp; t_{2,n-1} &amp; t_{2n}\\

\vdots &amp; \vdots &amp; &amp; \vdots\\

0 &amp; 0 &amp; \cdots &amp; t_{n-1, n-1} &amp; t_{n-1, n}\\

0 &amp; 0 &amp; \cdots &amp; 0 &amp; t_{nn}

\end{vmatrix}" data-equation-content="\begin{vmatrix}

t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1,n-1} &amp; t_{1n}\\

0 &amp; t_{22} &amp; \cdots &amp; t_{2,n-1} &amp; t_{2n}\\

\vdots &amp; \vdots &amp; &amp; \vdots\\

0 &amp; 0 &amp; \cdots &amp; t_{n-1, n-1} &amp; t_{n-1, n}\\

0 &amp; 0 &amp; \cdots &amp; 0 &amp; t_{nn}

\end{vmatrix}" data-ignore-a11y-check="" />. <br />(I did not emphasize this in class) If t_{11}, ..., t_{nn} are all nonzero, we rescale each row to see that the determinant is <img class="equation_image" title="t_{11}t_{22}\cdots t_{n-1, n-1}t_{nn}\begin{vmatrix}

1 &amp; t_{12}/t_{11} &amp; \cdots &amp; t_{1,n-1}/t_{11} &amp; t_{1n}/ t_{11} \\

0 &amp; 1 &amp; \cdots &amp; t_{2,n-1}/t_{22} &amp; t_{2n}/t_{22}\\

\vdots &amp; \vdots &amp; &amp; \vdots\\

0 &amp; 0 &amp; \cdots &amp; 1 &amp; t_{n-1, n}/ t_{n-1, n-1}\\

0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1

\end{vmatrix}" src="https://canvas.du.edu/equation_images/t_%257B11%257Dt_%257B22%257D%255Ccdots%2520t_%257Bn-1%252C%2520n-1%257Dt_%257Bnn%257D%255Cbegin%257Bvmatrix%257D%250A%250A1%2520%2526%2520t_%257B12%257D%252Ft_%257B11%257D%2520%2526%2520%255Ccdots%2520%2526%2520t_%257B1%252Cn-1%257D%252Ft_%257B11%257D%2520%2526%2520t_%257B1n%257D%252F%2520t_%257B11%257D%2520%255C%255C%250A%250A0%2520%2526%25201%2520%2526%2520%255Ccdots%2520%2526%2520t_%257B2%252Cn-1%257D%252Ft_%257B22%257D%2520%2526%2520t_%257B2n%257D%252Ft_%257B22%257D%255C%255C%250A%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%255C%255C%250A%250A0%2520%2526%25200%2520%2526%2520%255Ccdots%2520%2526%25201%2520%2526%2520t_%257Bn-1%252C%2520n%257D%252F%2520t_%257Bn-1%252C%2520n-1%257D%255C%255C%250A%250A0%2520%2526%25200%2520%2526%2520%255Ccdots%2520%2526%25200%2520%2526%25201%250A%250A%255Cend%257Bvmatrix%257D?scale=1" alt="LaTeX: t_{11}t_{22}\cdots t_{n-1, n-1}t_{nn}\begin{vmatrix}

1 &amp; t_{12}/t_{11} &amp; \cdots &amp; t_{1,n-1}/t_{11} &amp; t_{1n}/ t_{11} \\

0 &amp; 1 &amp; \cdots &amp; t_{2,n-1}/t_{22} &amp; t_{2n}/t_{22}\\

\vdots &amp; \vdots &amp; &amp; \vdots\\

0 &amp; 0 &amp; \cdots &amp; 1 &amp; t_{n-1, n}/ t_{n-1, n-1}\\

0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1

\end{vmatrix}" data-equation-content="t_{11}t_{22}\cdots t_{n-1, n-1}t_{nn}\begin{vmatrix}

1 &amp; t_{12}/t_{11} &amp; \cdots &amp; t_{1,n-1}/t_{11} &amp; t_{1n}/ t_{11} \\

0 &amp; 1 &amp; \cdots &amp; t_{2,n-1}/t_{22} &amp; t_{2n}/t_{22}\\

\vdots &amp; \vdots &amp; &amp; \vdots\\

0 &amp; 0 &amp; \cdots &amp; 1 &amp; t_{n-1, n}/ t_{n-1, n-1}\\

0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1

\end{vmatrix}" data-ignore-a11y-check="" />. <br />Using the row operations of type (a), we see that the determinant of the rescaled matrix is precisely 1. So the claim is proved when all the diagonal elements are nonzero. <br />(I forgot to mention) If one of the <img class="equation_image" title="t_{ii}" src="https://canvas.du.edu/equation_images/t_%257Bii%257D?scale=1" alt="LaTeX: t_{ii}" data-equation-content="t_{ii}" data-ignore-a11y-check="" /> is zero, then we can check that the i-row to the n-th row is linearly dependent. So an echelon of the matrix will have a zero row. Thus the matrix is singular, and the determinant will be zero. <br />Example:<br /><img class="equation_image" title="\begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix} =&nbsp;

\begin{vmatrix}

3 &amp; 1 &amp; 2 \\

5 &amp; 4 &amp; -1\\

1 &amp; -2 &amp; -7

\end{vmatrix}

=-\begin{vmatrix}

1 &amp; -2 &amp; -7\\

5 &amp; 4 &amp; -1\\

3 &amp; 1 &amp; 2&nbsp;

\end{vmatrix}

=-\begin{vmatrix}

1 &amp; -2 &amp; -7\\

0 &amp; 14 &amp; 34 \\

0 &amp; 7 &amp; 23

\end{vmatrix}

=-2\begin{vmatrix}

1 &amp; -2 &amp; -1\\

0 &amp; 7 &amp; 17 \\

0 &amp; 7 &amp; 23

\end{vmatrix}&nbsp;

=-2\begin{vmatrix}

1 &amp; -2 &amp; -1\\

0 &amp; 7 &amp; 17 \\

0 &amp; 0 &amp; 6

\end{vmatrix}= -2\cdot 1 \cdot 7 \cdot (-6) = 84.&nbsp;" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bvmatrix%257D%250A%250A3%2520%2526%25201%2520%2526%25202%255C%255C%250A%250A5%2520%2526%25204%2520%2526%2520-1%255C%255C%250A%250A7%2520%2526%25200%2520%2526%2520-3%250A%250A%255Cend%257Bvmatrix%257D%2520%253D%25C2%25A0%250A%250A%255Cbegin%257Bvmatrix%257D%250A%250A3%2520%2526%25201%2520%2526%25202%2520%255C%255C%250A%250A5%2520%2526%25204%2520%2526%2520-1%255C%255C%250A%250A1%2520%2526%2520-2%2520%2526%2520-7%250A%250A%255Cend%257Bvmatrix%257D%250A%250A%253D-%255Cbegin%257Bvmatrix%257D%250A%250A1%2520%2526%2520-2%2520%2526%2520-7%255C%255C%250A%250A5%2520%2526%25204%2520%2526%2520-1%255C%255C%250A%250A3%2520%2526%25201%2520%2526%25202%25C2%25A0%250A%250A%255Cend%257Bvmatrix%257D%250A%250A%253D-%255Cbegin%257Bvmatrix%257D%250A%250A1%2520%2526%2520-2%2520%2526%2520-7%255C%255C%250A%250A0%2520%2526%252014%2520%2526%252034%2520%255C%255C%250A%250A0%2520%2526%25207%2520%2526%252023%250A%250A%255Cend%257Bvmatrix%257D%250A%250A%253D-2%255Cbegin%257Bvmatrix%257D%250A%250A1%2520%2526%2520-2%2520%2526%2520-1%255C%255C%250A%250A0%2520%2526%25207%2520%2526%252017%2520%255C%255C%250A%250A0%2520%2526%25207%2520%2526%252023%250A%250A%255Cend%257Bvmatrix%257D%25C2%25A0%250A%250A%253D-2%255Cbegin%257Bvmatrix%257D%250A%250A1%2520%2526%2520-2%2520%2526%2520-1%255C%255C%250A%250A0%2520%2526%25207%2520%2526%252017%2520%255C%255C%250A%250A0%2520%2526%25200%2520%2526%25206%250A%250A%255Cend%257Bvmatrix%257D%253D%2520-2%255Ccdot%25201%2520%255Ccdot%25207%2520%255Ccdot%2520(-6)%2520%253D%252084.%25C2%25A0?scale=1" alt="LaTeX: \begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix} =&nbsp;

\begin{vmatrix}

3 &amp; 1 &amp; 2 \\

5 &amp; 4 &amp; -1\\

1 &amp; -2 &amp; -7

\end{vmatrix}

=-\begin{vmatrix}

1 &amp; -2 &amp; -7\\

5 &amp; 4 &amp; -1\\

3 &amp; 1 &amp; 2&nbsp;

\end{vmatrix}

=-\begin{vmatrix}

1 &amp; -2 &amp; -7\\

0 &amp; 14 &amp; 34 \\

0 &amp; 7 &amp; 23

\end{vmatrix}

=-2\begin{vmatrix}

1 &amp; -2 &amp; -1\\

0 &amp; 7 &amp; 17 \\

0 &amp; 7 &amp; 23

\end{vmatrix}&nbsp;

=-2\begin{vmatrix}

1 &amp; -2 &amp; -1\\

0 &amp; 7 &amp; 17 \\

0 &amp; 0 &amp; 6

\end{vmatrix}= -2\cdot 1 \cdot 7 \cdot (-6) = 84.&nbsp;" data-equation-content="\begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix} =&nbsp;

\begin{vmatrix}

3 &amp; 1 &amp; 2 \\

5 &amp; 4 &amp; -1\\

1 &amp; -2 &amp; -7

\end{vmatrix}

=-\begin{vmatrix}

1 &amp; -2 &amp; -7\\

5 &amp; 4 &amp; -1\\

3 &amp; 1 &amp; 2&nbsp;

\end{vmatrix}

=-\begin{vmatrix}

1 &amp; -2 &amp; -7\\

0 &amp; 14 &amp; 34 \\

0 &amp; 7 &amp; 23

\end{vmatrix}

=-2\begin{vmatrix}

1 &amp; -2 &amp; -1\\

0 &amp; 7 &amp; 17 \\

0 &amp; 7 &amp; 23

\end{vmatrix}&nbsp;

=-2\begin{vmatrix}

1 &amp; -2 &amp; -1\\

0 &amp; 7 &amp; 17 \\

0 &amp; 0 &amp; 6

\end{vmatrix}= -2\cdot 1 \cdot 7 \cdot (-6) = 84.&nbsp;" data-ignore-a11y-check="" /></li>
    <li>(Better be placed here) The determinant is multiplicative, i.e., for two matrices A, B, <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="|AB| = |A|\cdot |B|" src="https://canvas.du.edu/equation_images/%257CAB%257C%2520%253D%2520%257CA%257C%255Ccdot%2520%257CB%257C?scale=1" alt="LaTeX: |AB| = |A|\cdot |B|" data-equation-content="|AB| = |A|\cdot |B|" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">. The proof is better understood using the geometric interpretation. Please watch the 3blue1brown videos for more details. </span></li>
    <li>The determinant is not linear. Generally speaking, det(A+B)\neq det(A)+det(B). But the determinant is multilinear, in the sense that for each i = 1, ..., n, <br /><img class="equation_image" title="\det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\rho}_i + \vec{\mu}_i\\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix} = \det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\rho}_i \\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix} + \det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\mu}_i\\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%250A%250A%255Cvec%257B%255Crho%257D_1%2520%255C%255C%250A%250A%255Cvdots%2520%255C%255C%250A%250A%255Cvec%257B%255Crho%257D_i%2520%252B%2520%255Cvec%257B%255Cmu%257D_i%255C%255C%250A%250A%255Cvdots%2520%255C%255C%250A%250A%255Cvec%257B%255Crho%257D_n%255C%255C%250A%250A%255Cend%257Bpmatrix%257D%2520%253D%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%250A%250A%255Cvec%257B%255Crho%257D_1%2520%255C%255C%250A%250A%255Cvdots%2520%255C%255C%250A%250A%255Cvec%257B%255Crho%257D_i%2520%255C%255C%250A%250A%255Cvdots%2520%255C%255C%250A%250A%255Cvec%257B%255Crho%257D_n%255C%255C%250A%250A%255Cend%257Bpmatrix%257D%2520%252B%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%250A%250A%255Cvec%257B%255Crho%257D_1%2520%255C%255C%250A%250A%255Cvdots%2520%255C%255C%250A%250A%255Cvec%257B%255Cmu%257D_i%255C%255C%250A%250A%255Cvdots%2520%255C%255C%250A%250A%255Cvec%257B%255Crho%257D_n%255C%255C%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\rho}_i + \vec{\mu}_i\\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix} = \det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\rho}_i \\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix} + \det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\mu}_i\\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix}" data-equation-content="\det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\rho}_i + \vec{\mu}_i\\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix} = \det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\rho}_i \\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix} + \det \begin{pmatrix}

\vec{\rho}_1 \\

\vdots \\

\vec{\mu}_i\\

\vdots \\

\vec{\rho}_n\\

\end{pmatrix}" data-ignore-a11y-check="" /><br />Also, we generally have <img class="equation_image" title="det(kA) = k^n det(A)" src="https://canvas.du.edu/equation_images/det(kA)%2520%253D%2520k%255En%2520det(A)?scale=1" alt="LaTeX: det(kA) = k^n det(A)" data-equation-content="det(kA) = k^n det(A)" data-ignore-a11y-check="" />, since the scalar product of A rescales each row. Pulling out the coefficients for each row will result in n copies of k.</li>
    <li>Using the multilinearity, we can obtain the permutation expansion of a determinant. For example, the determinant can be expanded as<img class="equation_image" title="\begin{vmatrix}

a_{11} &amp; a_{12} &amp; a_{13} \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} = \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}\\

\text{since the first row }= \begin{pmatrix} a_{11} &amp; 0 &amp; 0 \end{pmatrix} +&nbsp;

\begin{pmatrix} 0 &amp; a_{12} &amp; 0 \end{pmatrix} + \begin{pmatrix} 0 &amp; 0 &amp; a_{13} \end{pmatrix}. \\

\text{Continuing with the second row}\\

&nbsp;= \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} +\begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}&nbsp;\\

+ \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}&nbsp;\\

+ \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}\\

= a_{11}a_{22}a_{33} - a_{11}a_{23}a_{31} - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bvmatrix%257D%250A%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%2520a_%257B13%257D%2520%255C%255C%250A%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520a_%257B23%257D%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%253D%2520%255Cbegin%257Bvmatrix%257D%250A%250Aa_%257B11%257D%2520%2526%25200%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520a_%257B23%257D%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%2520a_%257B12%257D%2520%2526%25200%255C%255C%250A%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520a_%257B23%257D%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%25200%2520%2526%2520a_%257B13%257D%2520%255C%255C%250A%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520a_%257B23%257D%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%255C%255C%250A%250A%255Ctext%257Bsince%2520the%2520first%2520row%2520%257D%253D%2520%255Cbegin%257Bpmatrix%257D%2520a_%257B11%257D%2520%2526%25200%2520%2526%25200%2520%255Cend%257Bpmatrix%257D%2520%252B%25C2%25A0%250A%250A%255Cbegin%257Bpmatrix%257D%25200%2520%2526%2520a_%257B12%257D%2520%2526%25200%2520%255Cend%257Bpmatrix%257D%2520%252B%2520%255Cbegin%257Bpmatrix%257D%25200%2520%2526%25200%2520%2526%2520a_%257B13%257D%2520%255Cend%257Bpmatrix%257D.%2520%255C%255C%250A%250A%255Ctext%257BContinuing%2520with%2520the%2520second%2520row%257D%255C%255C%250A%250A%25C2%25A0%253D%2520%255Cbegin%257Bvmatrix%257D%250A%250Aa_%257B11%257D%2520%2526%25200%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B21%257D%2520%2526%25200%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250Aa_%257B11%257D%2520%2526%25200%2520%2526%25200%2520%255C%255C%250A%250A0%2520%2526%2520a_%257B22%257D%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%255Cbegin%257Bvmatrix%257D%250A%250Aa_%257B11%257D%2520%2526%25200%2520%2526%25200%2520%255C%255C%250A%250A0%2520%2526%25200%2520%2526%2520a_%257B23%257D%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%25C2%25A0%255C%255C%250A%250A%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%2520a_%257B12%257D%2520%2526%25200%255C%255C%250A%250Aa_%257B21%257D%2520%2526%25200%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%2520a_%257B12%257D%2520%2526%25200%255C%255C%250A%250A0%2520%2526%2520a_%257B22%257D%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%2520a_%257B12%257D%2520%2526%25200%255C%255C%250A%250A0%2520%2526%25200%2520%2526%2520a_%257B23%257D%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%25C2%25A0%255C%255C%250A%250A%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%25200%2520%2526%2520a_%257B13%257D%2520%255C%255C%250A%250Aa_%257B21%257D%2520%2526%25200%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%25200%2520%2526%2520a_%257B13%257D%2520%255C%255C%250A%250A0%2520%2526%2520a_%257B22%257D%2520%2526%25200%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%252B%2520%255Cbegin%257Bvmatrix%257D%250A%250A0%2520%2526%25200%2520%2526%2520a_%257B13%257D%2520%255C%255C%250A%250A0%2520%2526%25200%2520%2526%2520a_%257B23%257D%2520%255C%255C%250A%250Aa_%257B31%257D%2520%2526%2520a_%257B32%257D%2520%2526%2520a_%257B33%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%255C%255C%250A%250A%253D%2520a_%257B11%257Da_%257B22%257Da_%257B33%257D%2520-%2520a_%257B11%257Da_%257B23%257Da_%257B31%257D%2520-%2520a_%257B12%257Da_%257B21%257Da_%257B33%257D%2520%252B%2520a_%257B12%257Da_%257B23%257Da_%257B31%257D%2520%252B%2520a_%257B13%257Da_%257B21%257Da_%257B32%257D%2520-%2520a_%257B13%257Da_%257B22%257Da_%257B31%257D?scale=1" alt="LaTeX: \begin{vmatrix}

a_{11} &amp; a_{12} &amp; a_{13} \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} = \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}\\

\text{since the first row }= \begin{pmatrix} a_{11} &amp; 0 &amp; 0 \end{pmatrix} +&nbsp;

\begin{pmatrix} 0 &amp; a_{12} &amp; 0 \end{pmatrix} + \begin{pmatrix} 0 &amp; 0 &amp; a_{13} \end{pmatrix}. \\

\text{Continuing with the second row}\\

&nbsp;= \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} +\begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}&nbsp;\\

+ \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}&nbsp;\\

+ \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}\\

= a_{11}a_{22}a_{33} - a_{11}a_{23}a_{31} - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}" data-equation-content="\begin{vmatrix}

a_{11} &amp; a_{12} &amp; a_{13} \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} = \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

a_{21} &amp; a_{22} &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}\\

\text{since the first row }= \begin{pmatrix} a_{11} &amp; 0 &amp; 0 \end{pmatrix} +&nbsp;

\begin{pmatrix} 0 &amp; a_{12} &amp; 0 \end{pmatrix} + \begin{pmatrix} 0 &amp; 0 &amp; a_{13} \end{pmatrix}. \\

\text{Continuing with the second row}\\

&nbsp;= \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} +\begin{vmatrix}

a_{11} &amp; 0 &amp; 0 \\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}&nbsp;\\

+ \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; a_{12} &amp; 0\\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}&nbsp;\\

+ \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

a_{21} &amp; 0 &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

0 &amp; a_{22} &amp; 0 \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix} + \begin{vmatrix}

0 &amp; 0 &amp; a_{13} \\

0 &amp; 0 &amp; a_{23} \\

a_{31} &amp; a_{32} &amp; a_{33} \\

\end{vmatrix}\\

= a_{11}a_{22}a_{33} - a_{11}a_{23}a_{31} - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}" data-ignore-a11y-check="" /><br /><br />In general, <img class="equation_image" title="\begin{vmatrix}

a_{11} &amp; a_{12} &amp;\cdots &amp; a_{1n} \\

a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\

\vdots &amp; \vdots &amp; &amp;\vdots \\

a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} \\

\end{vmatrix} = \sum\limits_{\substack{\phi: \{1, ..., n\} \to \{1, ... n\}\\ \text{permuation}}} a_{1\phi(1)} a_{2\phi(2)} \cdots a_{n\phi(n)} |P_\phi|" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bvmatrix%257D%250A%250Aa_%257B11%257D%2520%2526%2520a_%257B12%257D%2520%2526%255Ccdots%2520%2526%2520a_%257B1n%257D%2520%255C%255C%250A%250Aa_%257B21%257D%2520%2526%2520a_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257B2n%257D%2520%255C%255C%250A%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%255Cvdots%2520%255C%255C%250A%250Aa_%257Bn1%257D%2520%2526%2520a_%257Bn2%257D%2520%2526%2520%255Ccdots%2520%2526%2520a_%257Bnn%257D%2520%255C%255C%250A%250A%255Cend%257Bvmatrix%257D%2520%253D%2520%255Csum%255Climits_%257B%255Csubstack%257B%255Cphi%253A%2520%255C%257B1%252C%2520...%252C%2520n%255C%257D%2520%255Cto%2520%255C%257B1%252C%2520...%2520n%255C%257D%255C%255C%2520%255Ctext%257Bpermuation%257D%257D%257D%2520a_%257B1%255Cphi(1)%257D%2520a_%257B2%255Cphi(2)%257D%2520%255Ccdots%2520a_%257Bn%255Cphi(n)%257D%2520%257CP_%255Cphi%257C?scale=1" alt="LaTeX: \begin{vmatrix}

a_{11} &amp; a_{12} &amp;\cdots &amp; a_{1n} \\

a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\

\vdots &amp; \vdots &amp; &amp;\vdots \\

a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} \\

\end{vmatrix} = \sum\limits_{\substack{\phi: \{1, ..., n\} \to \{1, ... n\}\\ \text{permuation}}} a_{1\phi(1)} a_{2\phi(2)} \cdots a_{n\phi(n)} |P_\phi|" data-equation-content="\begin{vmatrix}

a_{11} &amp; a_{12} &amp;\cdots &amp; a_{1n} \\

a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\

\vdots &amp; \vdots &amp; &amp;\vdots \\

a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} \\

\end{vmatrix} = \sum\limits_{\substack{\phi: \{1, ..., n\} \to \{1, ... n\}\\ \text{permuation}}} a_{1\phi(1)} a_{2\phi(2)} \cdots a_{n\phi(n)} |P_\phi|" data-ignore-a11y-check="" /><br />where <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="P_{\phi}" src="https://canvas.du.edu/equation_images/P_%257B%255Cphi%257D?scale=1" alt="LaTeX: P_{\phi}" data-equation-content="P_{\phi}" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> is the matrix whose i-th row is the row vector </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="(0, ..., 0, 1, 0, ..., 0)" src="https://canvas.du.edu/equation_images/(0%252C%2520...%252C%25200%252C%25201%252C%25200%252C%2520...%252C%25200)?scale=1" alt="LaTeX: (0, ..., 0, 1, 0, ..., 0)" data-equation-content="(0, ..., 0, 1, 0, ..., 0)" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;"> with the 1 located in the </span><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\phi(i)" src="https://canvas.du.edu/equation_images/%255Cphi(i)?scale=1" alt="LaTeX: \phi(i)" data-equation-content="\phi(i)" data-ignore-a11y-check="" /><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">-th column. <br /><br /></span>I still need to mention this formula in this course since it is theoretically important in higher-level mathematics. But in practice, we rarely compute the determinant in such a way.</li>
    <li><img class="equation_image" title="\det(A) = \det(A^T)" src="https://canvas.du.edu/equation_images/%255Cdet(A)%2520%253D%2520%255Cdet(A%255ET)?scale=1" alt="LaTeX: \det(A) = \det(A^T)" data-equation-content="\det(A) = \det(A^T)" data-ignore-a11y-check="" />. The permutation expansion proves this, but the argument is quite technical. An important consequence is that one can define determinants using column vectors. More precisely, the determinant is a function<img class="equation_image" title=" \det: \mathcal{M}_{n\times n} \to \mathbb{R} " src="https://canvas.du.edu/equation_images/%2520%255Cdet%253A%2520%255Cmathcal%257BM%257D_%257Bn%255Ctimes%2520n%257D%2520%255Cto%2520%255Cmathbb%257BR%257D%2520?scale=1" alt="LaTeX:  \det: \mathcal{M}_{n\times n} \to \mathbb{R} " data-equation-content=" \det: \mathcal{M}_{n\times n} \to \mathbb{R} " data-ignore-a11y-check="" /> satisfying:<br />(a) <img class="equation_image" title="\det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; k\vec{c}_i + \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix}&nbsp;" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257Bc%257D_1%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_i%2520%2526%2520%255Ccdots%2520%2526%2520k%255Cvec%257Bc%257D_i%2520%252B%2520%255Cvec%257Bc%257D_j%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257Bc%257D_1%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_i%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_j%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_n%255Cend%257Bpmatrix%257D%25C2%25A0?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; k\vec{c}_i + \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix}&nbsp;" data-equation-content="\det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; k\vec{c}_i + \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix}&nbsp;" data-ignore-a11y-check="" /><br />In words: adding k times a column to another column does not change the determinant. <br />(b) <img class="equation_image" title="\det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; k \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = k\cdot  \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix}&nbsp;" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257Bc%257D_1%2520%2526%2520%255Ccdots%2520%2526%2520k%2520%255Cvec%257Bc%257D_i%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520k%255Ccdot%2520%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257Bc%257D_1%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_i%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_n%255Cend%257Bpmatrix%257D%25C2%25A0?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; k \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = k\cdot  \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix}&nbsp;" data-equation-content="\det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; k \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = k\cdot  \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix}&nbsp;" data-ignore-a11y-check="" /><br />In words, rescaling a column by k results in rescaling the determinant by k. <br />(c) <img class="equation_image" title="\det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257Bc%257D_1%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_i%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_j%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_n%255Cend%257Bpmatrix%257D%2520%253D%2520-%2520%255Cdet%2520%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257Bc%257D_1%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_j%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_i%2520%2526%2520%255Ccdots%2520%2526%2520%255Cvec%257Bc%257D_n%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix}" data-equation-content="\det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_n\end{pmatrix} = - \det \begin{pmatrix} \vec{c}_1 &amp; \cdots &amp; \vec{c}_j &amp; \cdots &amp; \vec{c}_i &amp; \cdots &amp; \vec{c}_n\end{pmatrix}" data-ignore-a11y-check="" /><br />In words, swapping two columns generates a negative movement. <br />(d) <img class="equation_image" title="\det I_{n\times n} = 1" src="https://canvas.du.edu/equation_images/%255Cdet%2520I_%257Bn%255Ctimes%2520n%257D%2520%253D%25201?scale=1" alt="LaTeX: \det I_{n\times n} = 1" data-equation-content="\det I_{n\times n} = 1" data-ignore-a11y-check="" /><br />In words, the determinant of the identity matrix is 1.</li>
    <li>Laplace expansion is commonly used to compute a determinant corresponding to the characteristic polynomial (to be seen later).
        <ol>
            <li>We first define the (i,j)-minor of a <img class="equation_image" title="n\times n" src="https://canvas.du.edu/equation_images/n%255Ctimes%2520n?scale=1" alt="LaTeX: n\times n" data-equation-content="n\times n" data-ignore-a11y-check="" />-matrix as the <img class="equation_image" title="(n-1)\times(n-1)" src="https://canvas.du.edu/equation_images/(n-1)%255Ctimes(n-1)?scale=1" alt="LaTeX: (n-1)\times(n-1)" data-equation-content="(n-1)\times(n-1)" data-ignore-a11y-check="" />-matrix obtained by deleting the i-th row and j-th column. Then we define the (i,j)-cofactor as the determinant of the (i,j)-minor multiplied by <img class="equation_image" title="(-1)^{i+j}" src="https://canvas.du.edu/equation_images/(-1)%255E%257Bi%252Bj%257D?scale=1" alt="LaTeX: (-1)^{i+j}" data-equation-content="(-1)^{i+j}" data-ignore-a11y-check="" />. The (i,j)-cofactor of a matrix T is denoted by <img class="equation_image" title="T_{ij}" src="https://canvas.du.edu/equation_images/T_%257Bij%257D?scale=1" alt="LaTeX: T_{ij}" data-equation-content="T_{ij}" data-ignore-a11y-check="" />.</li>
            <li>Theorem: For <img class="equation_image" title="T = \begin{pmatrix}&nbsp;

t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1n} \\&nbsp;

t_{21} &amp; t_{22} &amp; \cdots &amp; t_{2n} \\

\vdots &amp; \vdots &amp; &amp; \vdots\\

t_{n1} &amp; t_{n2} &amp; \cdots &amp; t_{nn}&nbsp;

\end{pmatrix}" src="https://canvas.du.edu/equation_images/T%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25C2%25A0%250A%250At_%257B11%257D%2520%2526%2520t_%257B12%257D%2520%2526%2520%255Ccdots%2520%2526%2520t_%257B1n%257D%2520%255C%255C%25C2%25A0%250A%250At_%257B21%257D%2520%2526%2520t_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520t_%257B2n%257D%2520%255C%255C%250A%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%255C%255C%250A%250At_%257Bn1%257D%2520%2526%2520t_%257Bn2%257D%2520%2526%2520%255Ccdots%2520%2526%2520t_%257Bnn%257D%25C2%25A0%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: T = \begin{pmatrix}&nbsp;

t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1n} \\&nbsp;

t_{21} &amp; t_{22} &amp; \cdots &amp; t_{2n} \\

\vdots &amp; \vdots &amp; &amp; \vdots\\

t_{n1} &amp; t_{n2} &amp; \cdots &amp; t_{nn}&nbsp;

\end{pmatrix}" data-equation-content="T = \begin{pmatrix}&nbsp;

t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1n} \\&nbsp;

t_{21} &amp; t_{22} &amp; \cdots &amp; t_{2n} \\

\vdots &amp; \vdots &amp; &amp; \vdots\\

t_{n1} &amp; t_{n2} &amp; \cdots &amp; t_{nn}&nbsp;

\end{pmatrix}" data-ignore-a11y-check="" />, we have<br /><img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\begin{aligned}
\det T &amp;= t_{11} T_{11} + t_{12}T_{12} + \cdots + t_{1n}T_{1n} &amp; \text{(Laplace expansion by row 1)}\\

&amp;= t_{i1} T_{i1} + t_{i2}T_{i2} + \cdots + t_{in}T_{in} &amp;\text{(Laplace expansion by row 1)}\\

&amp;= t_{11} T_{11} + t_{21}T_{21} + \cdots + t_{n1}T_{n1} &amp;\text{(Laplace expansion by column 1)}\\

&amp;= t_{1j} T_{1j} + t_{2j}T_{2j} + \cdots + t_{nj}T_{nj} &amp; \text{(Laplace expansion by column j)}
\end{aligned}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Baligned%257D%250A%255Cdet%2520T%2520%2526%253D%2520t_%257B11%257D%2520T_%257B11%257D%2520%252B%2520t_%257B12%257DT_%257B12%257D%2520%252B%2520%255Ccdots%2520%252B%2520t_%257B1n%257DT_%257B1n%257D%2520%2526%2520%255Ctext%257B(Laplace%2520expansion%2520by%2520row%25201)%257D%255C%255C%250A%250A%2526%253D%2520t_%257Bi1%257D%2520T_%257Bi1%257D%2520%252B%2520t_%257Bi2%257DT_%257Bi2%257D%2520%252B%2520%255Ccdots%2520%252B%2520t_%257Bin%257DT_%257Bin%257D%2520%2526%255Ctext%257B(Laplace%2520expansion%2520by%2520row%25201)%257D%255C%255C%250A%250A%2526%253D%2520t_%257B11%257D%2520T_%257B11%257D%2520%252B%2520t_%257B21%257DT_%257B21%257D%2520%252B%2520%255Ccdots%2520%252B%2520t_%257Bn1%257DT_%257Bn1%257D%2520%2526%255Ctext%257B(Laplace%2520expansion%2520by%2520column%25201)%257D%255C%255C%250A%250A%2526%253D%2520t_%257B1j%257D%2520T_%257B1j%257D%2520%252B%2520t_%257B2j%257DT_%257B2j%257D%2520%252B%2520%255Ccdots%2520%252B%2520t_%257Bnj%257DT_%257Bnj%257D%2520%2526%2520%255Ctext%257B(Laplace%2520expansion%2520by%2520column%2520j)%257D%250A%255Cend%257Baligned%257D?scale=1" alt="LaTeX: \begin{aligned}
\det T &amp;= t_{11} T_{11} + t_{12}T_{12} + \cdots + t_{1n}T_{1n} &amp; \text{(Laplace expansion by row 1)}\\

&amp;= t_{i1} T_{i1} + t_{i2}T_{i2} + \cdots + t_{in}T_{in} &amp;\text{(Laplace expansion by row 1)}\\

&amp;= t_{11} T_{11} + t_{21}T_{21} + \cdots + t_{n1}T_{n1} &amp;\text{(Laplace expansion by column 1)}\\

&amp;= t_{1j} T_{1j} + t_{2j}T_{2j} + \cdots + t_{nj}T_{nj} &amp; \text{(Laplace expansion by column j)}
\end{aligned}" data-equation-content="\begin{aligned}
\det T &amp;= t_{11} T_{11} + t_{12}T_{12} + \cdots + t_{1n}T_{1n} &amp; \text{(Laplace expansion by row 1)}\\

&amp;= t_{i1} T_{i1} + t_{i2}T_{i2} + \cdots + t_{in}T_{in} &amp;\text{(Laplace expansion by row 1)}\\

&amp;= t_{11} T_{11} + t_{21}T_{21} + \cdots + t_{n1}T_{n1} &amp;\text{(Laplace expansion by column 1)}\\

&amp;= t_{1j} T_{1j} + t_{2j}T_{2j} + \cdots + t_{nj}T_{nj} &amp; \text{(Laplace expansion by column j)}
\end{aligned}" data-ignore-a11y-check="" /><br />This allows us to compute the determinant by expanding the row (or column) with the most zeros. <br />Example: For <img class="equation_image" title="\begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bvmatrix%257D%250A%250A3%2520%2526%25201%2520%2526%25202%255C%255C%250A%250A5%2520%2526%25204%2520%2526%2520-1%255C%255C%250A%250A7%2520%2526%25200%2520%2526%2520-3%250A%250A%255Cend%257Bvmatrix%257D?scale=1" alt="LaTeX: \begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix}" data-equation-content="\begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix}" data-ignore-a11y-check="" />, we expand the third row: <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="\begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix} = 7 \cdot (-1)^{3+1} \begin{vmatrix} 1 &amp; 2 \\ 4 &amp; -1 \end{vmatrix} + (-3) \cdot (-1)^{3+3} \begin{vmatrix} 3 &amp; 1 \\ 5 &amp; 4 \end{vmatrix} = 7 \cdot (1\cdot (-1) - 2\cdot 4) - 3\cdot (3\cdot 4 - 5 \cdot 1) = 84.&nbsp;" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bvmatrix%257D%250A%250A3%2520%2526%25201%2520%2526%25202%255C%255C%250A%250A5%2520%2526%25204%2520%2526%2520-1%255C%255C%250A%250A7%2520%2526%25200%2520%2526%2520-3%250A%250A%255Cend%257Bvmatrix%257D%2520%253D%25207%2520%255Ccdot%2520(-1)%255E%257B3%252B1%257D%2520%255Cbegin%257Bvmatrix%257D%25201%2520%2526%25202%2520%255C%255C%25204%2520%2526%2520-1%2520%255Cend%257Bvmatrix%257D%2520%252B%2520(-3)%2520%255Ccdot%2520(-1)%255E%257B3%252B3%257D%2520%255Cbegin%257Bvmatrix%257D%25203%2520%2526%25201%2520%255C%255C%25205%2520%2526%25204%2520%255Cend%257Bvmatrix%257D%2520%253D%25207%2520%255Ccdot%2520(1%255Ccdot%2520(-1)%2520-%25202%255Ccdot%25204)%2520-%25203%255Ccdot%2520(3%255Ccdot%25204%2520-%25205%2520%255Ccdot%25201)%2520%253D%252084.%25C2%25A0?scale=1" alt="LaTeX: \begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix} = 7 \cdot (-1)^{3+1} \begin{vmatrix} 1 &amp; 2 \\ 4 &amp; -1 \end{vmatrix} + (-3) \cdot (-1)^{3+3} \begin{vmatrix} 3 &amp; 1 \\ 5 &amp; 4 \end{vmatrix} = 7 \cdot (1\cdot (-1) - 2\cdot 4) - 3\cdot (3\cdot 4 - 5 \cdot 1) = 84.&nbsp;" data-equation-content="\begin{vmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{vmatrix} = 7 \cdot (-1)^{3+1} \begin{vmatrix} 1 &amp; 2 \\ 4 &amp; -1 \end{vmatrix} + (-3) \cdot (-1)^{3+3} \begin{vmatrix} 3 &amp; 1 \\ 5 &amp; 4 \end{vmatrix} = 7 \cdot (1\cdot (-1) - 2\cdot 4) - 3\cdot (3\cdot 4 - 5 \cdot 1) = 84.&nbsp;" data-ignore-a11y-check="" /></li>
        </ol>
    </li>
    <li>Adjoint matrix and the inverse<br />adj(T) is defined as the transpose of the matrix formed by the cofactor. <br />More precisely <br /><img class="equation_image" title="\text{adj}(T) = \begin{pmatrix}&nbsp;

T_{11} &amp; T_{21} &amp; \cdots &amp; T_{n1} \\&nbsp;

T_{12} &amp; T_{22} &amp; \cdots &amp; T_{n2} \\

\vdots &amp; \vdots &amp; &amp; \vdots\\

T_{1n} &amp; T_{2n} &amp; \cdots &amp; T_{nn}&nbsp;

\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Ctext%257Badj%257D(T)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25C2%25A0%250A%250AT_%257B11%257D%2520%2526%2520T_%257B21%257D%2520%2526%2520%255Ccdots%2520%2526%2520T_%257Bn1%257D%2520%255C%255C%25C2%25A0%250A%250AT_%257B12%257D%2520%2526%2520T_%257B22%257D%2520%2526%2520%255Ccdots%2520%2526%2520T_%257Bn2%257D%2520%255C%255C%250A%250A%255Cvdots%2520%2526%2520%255Cvdots%2520%2526%2520%2526%2520%255Cvdots%255C%255C%250A%250AT_%257B1n%257D%2520%2526%2520T_%257B2n%257D%2520%2526%2520%255Ccdots%2520%2526%2520T_%257Bnn%257D%25C2%25A0%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \text{adj}(T) = \begin{pmatrix}&nbsp;

T_{11} &amp; T_{21} &amp; \cdots &amp; T_{n1} \\&nbsp;

T_{12} &amp; T_{22} &amp; \cdots &amp; T_{n2} \\

\vdots &amp; \vdots &amp; &amp; \vdots\\

T_{1n} &amp; T_{2n} &amp; \cdots &amp; T_{nn}&nbsp;

\end{pmatrix}" data-equation-content="\text{adj}(T) = \begin{pmatrix}&nbsp;

T_{11} &amp; T_{21} &amp; \cdots &amp; T_{n1} \\&nbsp;

T_{12} &amp; T_{22} &amp; \cdots &amp; T_{n2} \\

\vdots &amp; \vdots &amp; &amp; \vdots\\

T_{1n} &amp; T_{2n} &amp; \cdots &amp; T_{nn}&nbsp;

\end{pmatrix}" data-ignore-a11y-check="" /><br />Theorem: <img class="equation_image" title="T \cdot adj(T) = adj(T) \cdot T = |T|\cdot I_{n\times n} =&nbsp;


\begin{pmatrix}

|T| &amp; &amp; &amp; \\

&amp; |T| &amp; &amp; \\

&amp; &amp; \ddots &amp; \\

&amp; &amp; &amp; |T|

\end{pmatrix}" src="https://canvas.du.edu/equation_images/T%2520%255Ccdot%2520adj(T)%2520%253D%2520adj(T)%2520%255Ccdot%2520T%2520%253D%2520%257CT%257C%255Ccdot%2520I_%257Bn%255Ctimes%2520n%257D%2520%253D%25C2%25A0%250A%250A%250A%255Cbegin%257Bpmatrix%257D%250A%250A%257CT%257C%2520%2526%2520%2526%2520%2526%2520%255C%255C%250A%250A%2526%2520%257CT%257C%2520%2526%2520%2526%2520%255C%255C%250A%250A%2526%2520%2526%2520%255Cddots%2520%2526%2520%255C%255C%250A%250A%2526%2520%2526%2520%2526%2520%257CT%257C%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: T \cdot adj(T) = adj(T) \cdot T = |T|\cdot I_{n\times n} =&nbsp;


\begin{pmatrix}

|T| &amp; &amp; &amp; \\

&amp; |T| &amp; &amp; \\

&amp; &amp; \ddots &amp; \\

&amp; &amp; &amp; |T|

\end{pmatrix}" data-equation-content="T \cdot adj(T) = adj(T) \cdot T = |T|\cdot I_{n\times n} =&nbsp;


\begin{pmatrix}

|T| &amp; &amp; &amp; \\

&amp; |T| &amp; &amp; \\

&amp; &amp; \ddots &amp; \\

&amp; &amp; &amp; |T|

\end{pmatrix}" data-ignore-a11y-check="" /><br />Corollary: <img class="equation_image" title="|T^{-1}| = \frac{1}{|T|} adj(T)" src="https://canvas.du.edu/equation_images/%257CT%255E%257B-1%257D%257C%2520%253D%2520%255Cfrac%257B1%257D%257B%257CT%257C%257D%2520adj(T)?scale=1" alt="LaTeX: |T^{-1}| = \frac{1}{|T|} adj(T)" data-equation-content="|T^{-1}| = \frac{1}{|T|} adj(T)" data-ignore-a11y-check="" />.<br />Corollary: <img class="equation_image" title="|adj(T)| = |T|^{n-1}" src="https://canvas.du.edu/equation_images/%257Cadj(T)%257C%2520%253D%2520%257CT%257C%255E%257Bn-1%257D?scale=1" alt="LaTeX: |adj(T)| = |T|^{n-1}" data-equation-content="|adj(T)| = |T|^{n-1}" data-ignore-a11y-check="" />. <br />Example: <img class="equation_image" style="color: var(--ic-brand-font-color-dark); font-size: 1rem;" title="T = \begin{pmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{pmatrix}, 

adj(T) = \begin{pmatrix}

-12 &amp; 3 &amp; -9\\

8 &amp; -23 &amp; 13\\

-28 &amp; 7 &amp; 7

\end{pmatrix}, 

T^{-1} = \frac 1 {84} \begin{pmatrix}

-12 &amp; 3 &amp; -9\\

8 &amp; -23 &amp; 13\\

-28 &amp; 7 &amp; 7

\end{pmatrix}" src="https://canvas.du.edu/equation_images/T%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A%250A3%2520%2526%25201%2520%2526%25202%255C%255C%250A%250A5%2520%2526%25204%2520%2526%2520-1%255C%255C%250A%250A7%2520%2526%25200%2520%2526%2520-3%250A%250A%255Cend%257Bpmatrix%257D%252C%2520%250A%250Aadj(T)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A%250A-12%2520%2526%25203%2520%2526%2520-9%255C%255C%250A%250A8%2520%2526%2520-23%2520%2526%252013%255C%255C%250A%250A-28%2520%2526%25207%2520%2526%25207%250A%250A%255Cend%257Bpmatrix%257D%252C%2520%250A%250AT%255E%257B-1%257D%2520%253D%2520%255Cfrac%25201%2520%257B84%257D%2520%255Cbegin%257Bpmatrix%257D%250A%250A-12%2520%2526%25203%2520%2526%2520-9%255C%255C%250A%250A8%2520%2526%2520-23%2520%2526%252013%255C%255C%250A%250A-28%2520%2526%25207%2520%2526%25207%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: T = \begin{pmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{pmatrix}, 

adj(T) = \begin{pmatrix}

-12 &amp; 3 &amp; -9\\

8 &amp; -23 &amp; 13\\

-28 &amp; 7 &amp; 7

\end{pmatrix}, 

T^{-1} = \frac 1 {84} \begin{pmatrix}

-12 &amp; 3 &amp; -9\\

8 &amp; -23 &amp; 13\\

-28 &amp; 7 &amp; 7

\end{pmatrix}" data-equation-content="T = \begin{pmatrix}

3 &amp; 1 &amp; 2\\

5 &amp; 4 &amp; -1\\

7 &amp; 0 &amp; -3

\end{pmatrix}, 

adj(T) = \begin{pmatrix}

-12 &amp; 3 &amp; -9\\

8 &amp; -23 &amp; 13\\

-28 &amp; 7 &amp; 7

\end{pmatrix}, 

T^{-1} = \frac 1 {84} \begin{pmatrix}

-12 &amp; 3 &amp; -9\\

8 &amp; -23 &amp; 13\\

-28 &amp; 7 &amp; 7

\end{pmatrix}" data-ignore-a11y-check="" /></li>
</ol>
<p>&nbsp;</p>
    </ul></details>
	
	<details><summary>Lecture 19 on 11/15/2022</summary>
        <ul>
            <p><span data-preserver-spaces="true">Recall: Two matrices <img class="equation_image" title="A, \hat{A}" src="https://canvas.du.edu/equation_images/A%252C%2520%255Chat%257BA%257D?scale=1" alt="LaTeX: A, \hat{A}" data-equation-content="A, \hat{A}" data-ignore-a11y-check="" /> are row equivalent if there exists nonsingular matrix <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" />, such that <img class="equation_image" title="\hat{A} = BA" src="https://canvas.du.edu/equation_images/%255Chat%257BA%257D%2520%253D%2520BA?scale=1" alt="LaTeX: \hat{A} = BA" data-equation-content="\hat{A} = BA" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Every matrix is row-equivalent to a matrix of reduced echelon form. The reduced echelon form is the canonical form of this equivalence relation.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Recall: Two matrices <img class="equation_image" title="H, \hat{H}" src="https://canvas.du.edu/equation_images/H%252C%2520%255Chat%257BH%257D?scale=1" alt="LaTeX: H, \hat{H}" data-equation-content="H, \hat{H}" data-ignore-a11y-check="" /> are matrix equivalent, if there exists nonsingular matrices <img class="equation_image" title="B, D" src="https://canvas.du.edu/equation_images/B%252C%2520D?scale=1" alt="LaTeX: B, D" data-equation-content="B, D" data-ignore-a11y-check="" />, such that <img class="equation_image" title="\hat{H} = B H D" src="https://canvas.du.edu/equation_images/%255Chat%257BH%257D%2520%253D%2520B%2520H%2520D?scale=1" alt="LaTeX: \hat{H} = B H D" data-equation-content="\hat{H} = B H D" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Every matrix is matrix-equivalent to a block partial-identity matrix <img class="equation_image" title="\begin{pmatrix}

I_{k\times k} &amp; \text{zeros} \\

\text{zeros} &amp; \text{zeros}

\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%250AI_%257Bk%255Ctimes%2520k%257D%2520%2526%2520%255Ctext%257Bzeros%257D%2520%255C%255C%250A%250A%255Ctext%257Bzeros%257D%2520%2526%2520%255Ctext%257Bzeros%257D%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix}

I_{k\times k} &amp; \text{zeros} \\

\text{zeros} &amp; \text{zeros}

\end{pmatrix}" data-equation-content="\begin{pmatrix}

I_{k\times k} &amp; \text{zeros} \\

\text{zeros} &amp; \text{zeros}

\end{pmatrix}" data-ignore-a11y-check="" />, where k is the rank. The block partial-identity matrix is the canonical form of this equivalent relation.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Every homomorphism <img class="equation_image" title="h: V \to W" src="https://canvas.du.edu/equation_images/h%253A%2520V%2520%255Cto%2520W?scale=1" alt="LaTeX: h: V \to W" data-equation-content="h: V \to W" data-ignore-a11y-check="" /> can be represented by a block-partial identity matrix with respect to appropriately chosen bases of V and W.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Today: Refine the matrix equivalence to similarity and find its canonical form.&nbsp;</span></p>
<p><span data-preserver-spaces="true">In more detail, we restrict our attention to the following case</span></p>
<ol>
    <li><span data-preserver-spaces="true"><img class="equation_image" title="t: V\to V" src="https://canvas.du.edu/equation_images/t%253A%2520V%255Cto%2520V?scale=1" alt="LaTeX: t: V\to V" data-equation-content="t: V\to V" data-ignore-a11y-check="" /> is an endomorphism of V, i.e., a linear map whose domain and codomain coincide.&nbsp;</span></li>
    <li><span data-preserver-spaces="true">The basis of the domain <strong>equals </strong>the basis of the codomain.&nbsp;</span></li>
</ol>
<p><span data-preserver-spaces="true">So if <img class="equation_image" title="B" src="https://canvas.du.edu/equation_images/B?scale=1" alt="LaTeX: B" data-equation-content="B" data-ignore-a11y-check="" /> and <img class="equation_image" title="\hat{B}" src="https://canvas.du.edu/equation_images/%255Chat%257BB%257D?scale=1" alt="LaTeX: \hat{B}" data-equation-content="\hat{B}" data-ignore-a11y-check="" /> are two bases of V, then we have</span></p>
<p><img class="equation_image" title="\begin{aligned}
\text{Rep}_{\hat{B}, \hat{B}}(h) &amp;= \text{Rep}_{B, \hat{B}}(id) \text{Rep}_{B, B}(h) \text{Rep}_{\hat{B}, B}(id) 
\\
&amp; = \text{Rep}_{B, \hat{B}}(id) \text{Rep}_{B, B}(h) (\text{Rep}_{B, \hat{B}}(id) )^{-1}
\end{aligned}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Baligned%257D%250A%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520%255Chat%257BB%257D%257D(h)%2520%2526%253D%2520%255Ctext%257BRep%257D_%257BB%252C%2520%255Chat%257BB%257D%257D(id)%2520%255Ctext%257BRep%257D_%257BB%252C%2520B%257D(h)%2520%255Ctext%257BRep%257D_%257B%255Chat%257BB%257D%252C%2520B%257D(id)%2520%250A%255C%255C%250A%2526%2520%253D%2520%255Ctext%257BRep%257D_%257BB%252C%2520%255Chat%257BB%257D%257D(id)%2520%255Ctext%257BRep%257D_%257BB%252C%2520B%257D(h)%2520(%255Ctext%257BRep%257D_%257BB%252C%2520%255Chat%257BB%257D%257D(id)%2520)%255E%257B-1%257D%250A%255Cend%257Baligned%257D?scale=1" alt="LaTeX: \begin{aligned}
\text{Rep}_{\hat{B}, \hat{B}}(h) &amp;= \text{Rep}_{B, \hat{B}}(id) \text{Rep}_{B, B}(h) \text{Rep}_{\hat{B}, B}(id) 
\\
&amp; = \text{Rep}_{B, \hat{B}}(id) \text{Rep}_{B, B}(h) (\text{Rep}_{B, \hat{B}}(id) )^{-1}
\end{aligned}" data-equation-content="\begin{aligned}
\text{Rep}_{\hat{B}, \hat{B}}(h) &amp;= \text{Rep}_{B, \hat{B}}(id) \text{Rep}_{B, B}(h) \text{Rep}_{\hat{B}, B}(id) 
\\
&amp; = \text{Rep}_{B, \hat{B}}(id) \text{Rep}_{B, B}(h) (\text{Rep}_{B, \hat{B}}(id) )^{-1}
\end{aligned}" data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">Definition: Two matrices <img class="equation_image" title="T, \hat{T}" src="https://canvas.du.edu/equation_images/T%252C%2520%255Chat%257BT%257D?scale=1" alt="LaTeX: T, \hat{T}" data-equation-content="T, \hat{T}" data-ignore-a11y-check="" /> are similar if there exists a nonsingular matrix <img class="equation_image" title="P" src="https://canvas.du.edu/equation_images/P?scale=1" alt="LaTeX: P" data-equation-content="P" data-ignore-a11y-check="" /> such that <img class="equation_image" title="\hat{T} = P T P^{-1}" src="https://canvas.du.edu/equation_images/%255Chat%257BT%257D%2520%253D%2520P%2520T%2520P%255E%257B-1%257D?scale=1" alt="LaTeX: \hat{T} = P T P^{-1}" data-equation-content="\hat{T} = P T P^{-1}" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Note: Similar matrices are matrix equivalent. But the converse does not hold. In other words, s</span><span data-preserver-spaces="true">imilarity is finer than matrix equivalence.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Example: The identity matrix <img class="equation_image" title="\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%25201%2520%2526%25200%2520%255C%255C%25200%2520%2526%25201%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}" data-equation-content="\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}" data-ignore-a11y-check="" /> is matrix equivalent to any other <img class="equation_image" title="2\times 2" src="https://canvas.du.edu/equation_images/2%255Ctimes%25202?scale=1" alt="LaTeX: 2\times 2" data-equation-content="2\times 2" data-ignore-a11y-check="" /> matrix with rank 2, but is similar only to itself, because <br /></span><span data-preserver-spaces="true"><img class="equation_image" title="P\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}P^{-1} = PP^{-1} = I = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}" src="https://canvas.du.edu/equation_images/P%255Cbegin%257Bpmatrix%257D%25201%2520%2526%25200%2520%255C%255C%25200%2520%2526%25201%255Cend%257Bpmatrix%257DP%255E%257B-1%257D%2520%253D%2520PP%255E%257B-1%257D%2520%253D%2520I%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%2526%25200%2520%255C%255C%25200%2520%2526%25201%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: P\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}P^{-1} = PP^{-1} = I = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}" data-equation-content="P\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}P^{-1} = PP^{-1} = I = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix}" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">To find the canonical form of the similarity relation, we </span><span data-preserver-spaces="true">start with finding the eigenvalues and eigenvectors.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Definition: A transformation <img class="equation_image" title="t: V\to V" src="https://canvas.du.edu/equation_images/t%253A%2520V%255Cto%2520V?scale=1" alt="LaTeX: t: V\to V" data-equation-content="t: V\to V" data-ignore-a11y-check="" /> has a scalar eigenvalue <img class="equation_image" title="\lambda" src="https://canvas.du.edu/equation_images/%255Clambda?scale=1" alt="LaTeX: \lambda" data-equation-content="\lambda" data-ignore-a11y-check="" /> if there exists a <strong>nonzero </strong>eigenvector <img class="equation_image" title="\vec{\zeta}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: \vec{\zeta}" data-equation-content="\vec{\zeta}" data-ignore-a11y-check="" />, such that <img class="equation_image" title=" t(\vec{\zeta}) = \lambda\vec{\zeta}" src="https://canvas.du.edu/equation_images/%2520t(%255Cvec%257B%255Czeta%257D)%2520%253D%2520%255Clambda%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX:  t(\vec{\zeta}) = \lambda\vec{\zeta}" data-equation-content=" t(\vec{\zeta}) = \lambda\vec{\zeta}" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Definition: A square matrix <img class="equation_image" title="T" src="https://canvas.du.edu/equation_images/T?scale=1" alt="LaTeX: T" data-equation-content="T" data-ignore-a11y-check="" /> has a scalar eigenvalue <img class="equation_image" title="\lambda" src="https://canvas.du.edu/equation_images/%255Clambda?scale=1" alt="LaTeX: \lambda" data-equation-content="\lambda" data-ignore-a11y-check="" /> associated with the <strong>nonzero </strong>eigenvector <img class="equation_image" title="\vec{\zeta}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: \vec{\zeta}" data-equation-content="\vec{\zeta}" data-ignore-a11y-check="" /> if <img class="equation_image" title="T\vec{\zeta} = \lambda \vec{\zeta}" src="https://canvas.du.edu/equation_images/T%255Cvec%257B%255Czeta%257D%2520%253D%2520%255Clambda%2520%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: T\vec{\zeta} = \lambda \vec{\zeta}" data-equation-content="T\vec{\zeta} = \lambda \vec{\zeta}" data-ignore-a11y-check="" />.</span></p>
<p><span data-preserver-spaces="true">Equivalently: <img class="equation_image" title="(T-\lambda I)\vec{\zeta}= \vec{0}" src="https://canvas.du.edu/equation_images/(T-%255Clambda%2520I)%255Cvec%257B%255Czeta%257D%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: (T-\lambda I)\vec{\zeta}= \vec{0}" data-equation-content="(T-\lambda I)\vec{\zeta}= \vec{0}" data-ignore-a11y-check="" />, i.e., <img class="equation_image" title="\vec{\zeta}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: \vec{\zeta}" data-equation-content="\vec{\zeta}" data-ignore-a11y-check="" /> is the solution to the homogeneous equation with coefficient matrix <img class="equation_image" title="T-\lambda I" src="https://canvas.du.edu/equation_images/T-%255Clambda%2520I?scale=1" alt="LaTeX: T-\lambda I" data-equation-content="T-\lambda I" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">The homogeneous equation has nonzero solutions only when <img class="equation_image" title="T-\lambda I" src="https://canvas.du.edu/equation_images/T-%255Clambda%2520I?scale=1" alt="LaTeX: T-\lambda I" data-equation-content="T-\lambda I" data-ignore-a11y-check="" /> is singular, i.e., <img class="equation_image" title="\det(T-\lambda I) = 0." src="https://canvas.du.edu/equation_images/%255Cdet(T-%255Clambda%2520I)%2520%253D%25200.?scale=1" alt="LaTeX: \det(T-\lambda I) = 0." data-equation-content="\det(T-\lambda I) = 0." data-ignore-a11y-check="" />&nbsp;</span></p>
<p><span data-preserver-spaces="true">Standard procedure:&nbsp;</span></p>
<ol>
    <li><span data-preserver-spaces="true">Compute <img class="equation_image" title="\det(T-\lambda I)" src="https://canvas.du.edu/equation_images/%255Cdet(T-%255Clambda%2520I)?scale=1" alt="LaTeX: \det(T-\lambda I)" data-equation-content="\det(T-\lambda I)" data-ignore-a11y-check="" />,&nbsp;then set it as zero to find the eigenvalues.&nbsp;</span></li>
    <li><span data-preserver-spaces="true">For each eigenvalue found in Step 1, solve the system <img class="equation_image" title="(T-\lambda I)\vec{\zeta}= \vec{0}" src="https://canvas.du.edu/equation_images/(T-%255Clambda%2520I)%255Cvec%257B%255Czeta%257D%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: (T-\lambda I)\vec{\zeta}= \vec{0}" data-equation-content="(T-\lambda I)\vec{\zeta}= \vec{0}" data-ignore-a11y-check="" /> and parametrize the solution. The eigenvectors are simply the basis of the solution.&nbsp;</span></li>
</ol>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Three examples.&nbsp;</span></p>
<p><span data-preserver-spaces="true">1.<img class="equation_image" title=" \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}" src="https://canvas.du.edu/equation_images/%2520%255Cbegin%257Bpmatrix%257D%25202%2520%2526%25201%2520%255C%255C%25201%2520%2526%25202%2520%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX:  \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}" data-equation-content=" \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}" data-ignore-a11y-check="" />, </span><img class="equation_image" title="\lambda_1 = 3, \lambda_2 = 1.&nbsp;" src="https://canvas.du.edu/equation_images/%255Clambda_1%2520%253D%25203%252C%2520%255Clambda_2%2520%253D%25201.%25C2%25A0?scale=1" alt="LaTeX: \lambda_1 = 3, \lambda_2 = 1.&nbsp;" data-equation-content="\lambda_1 = 3, \lambda_2 = 1.&nbsp;" data-ignore-a11y-check="" /> <img class="equation_image" title="\vec{\zeta}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ -1\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%2520-1%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \vec{\zeta}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ -1\end{pmatrix}" data-equation-content="\vec{\zeta}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ -1\end{pmatrix}" data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">2. <img class="equation_image" title="\begin{pmatrix}&nbsp;

5 &amp; 2 &amp; 1\\

1 &amp; 4 &amp; -1\\

-1 &amp; -2 &amp; 3

\end{pmatrix}, \lambda_1 = 2, \lambda_2 = 4, \lambda_3 = 6, 

\vec{\zeta}_1 = \begin{pmatrix} 1 \\ -1 \\ -1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} -1 \\ 1 \\ -1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix} -1 \\ -1 \\ 1\end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%25C2%25A0%250A%250A5%2520%2526%25202%2520%2526%25201%255C%255C%250A%250A1%2520%2526%25204%2520%2526%2520-1%255C%255C%250A%250A-1%2520%2526%2520-2%2520%2526%25203%250A%250A%255Cend%257Bpmatrix%257D%252C%2520%255Clambda_1%2520%253D%25202%252C%2520%255Clambda_2%2520%253D%25204%252C%2520%255Clambda_3%2520%253D%25206%252C%2520%250A%250A%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%2520-1%2520%255C%255C%2520-1%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520-1%2520%255C%255C%25201%2520%255C%255C%2520-1%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_3%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520-1%2520%255C%255C%2520-1%2520%255C%255C%25201%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: \begin{pmatrix}&nbsp;

5 &amp; 2 &amp; 1\\

1 &amp; 4 &amp; -1\\

-1 &amp; -2 &amp; 3

\end{pmatrix}, \lambda_1 = 2, \lambda_2 = 4, \lambda_3 = 6, 

\vec{\zeta}_1 = \begin{pmatrix} 1 \\ -1 \\ -1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} -1 \\ 1 \\ -1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix} -1 \\ -1 \\ 1\end{pmatrix}.&nbsp;" data-equation-content="\begin{pmatrix}&nbsp;

5 &amp; 2 &amp; 1\\

1 &amp; 4 &amp; -1\\

-1 &amp; -2 &amp; 3

\end{pmatrix}, \lambda_1 = 2, \lambda_2 = 4, \lambda_3 = 6, 

\vec{\zeta}_1 = \begin{pmatrix} 1 \\ -1 \\ -1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} -1 \\ 1 \\ -1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix} -1 \\ -1 \\ 1\end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">3. <img class="equation_image" title="\begin{pmatrix}&nbsp;

1 &amp; -4 &amp; 2\\

-2 &amp; -1 &amp; 2\\

-4 &amp; -8 &amp; 7

\end{pmatrix}, 

\lambda_1 = 3, \lambda_2 = 3, \lambda_3 = 1, 

\vec{\zeta}_1 = \begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ 0 \\ 1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix} 1 \\ 1 \\ 2\end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%25C2%25A0%250A%250A1%2520%2526%2520-4%2520%2526%25202%255C%255C%250A%250A-2%2520%2526%2520-1%2520%2526%25202%255C%255C%250A%250A-4%2520%2526%2520-8%2520%2526%25207%250A%250A%255Cend%257Bpmatrix%257D%252C%2520%250A%250A%255Clambda_1%2520%253D%25203%252C%2520%255Clambda_2%2520%253D%25203%252C%2520%255Clambda_3%2520%253D%25201%252C%2520%250A%250A%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cbegin%257Bpmatrix%257D%2520-2%2520%255C%255C%25201%2520%255C%255C%25200%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25200%2520%255C%255C%25201%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_3%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25201%2520%255C%255C%25202%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: \begin{pmatrix}&nbsp;

1 &amp; -4 &amp; 2\\

-2 &amp; -1 &amp; 2\\

-4 &amp; -8 &amp; 7

\end{pmatrix}, 

\lambda_1 = 3, \lambda_2 = 3, \lambda_3 = 1, 

\vec{\zeta}_1 = \begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ 0 \\ 1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix} 1 \\ 1 \\ 2\end{pmatrix}.&nbsp;" data-equation-content="\begin{pmatrix}&nbsp;

1 &amp; -4 &amp; 2\\

-2 &amp; -1 &amp; 2\\

-4 &amp; -8 &amp; 7

\end{pmatrix}, 

\lambda_1 = 3, \lambda_2 = 3, \lambda_3 = 1, 

\vec{\zeta}_1 = \begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ 0 \\ 1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix} 1 \\ 1 \\ 2\end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /></span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Proposition: If the eigenvectors span the domain, then T is similar to the diagonal matrix consisting of the eigenvalues, i.e.,&nbsp;</span></p>
<p><span data-preserver-spaces="true"><img class="equation_image" title="T = P \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix} P^{-1}" src="https://canvas.du.edu/equation_images/T%2520%253D%2520P%2520%255Cbegin%257Bpmatrix%257D%2520%255Clambda_1%2520%2526%2520%2526%2520%255C%255C%2520%2526%2520%255Cddots%2520%2526%2520%255C%255C%2520%2526%2520%2526%2520%255Clambda_n%255Cend%257Bpmatrix%257D%2520P%255E%257B-1%257D?scale=1" alt="LaTeX: T = P \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix} P^{-1}" data-equation-content="T = P \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix} P^{-1}" data-ignore-a11y-check="" />, where P is the nonsingular matrix whose columns are given by the eigenvectors.&nbsp;&nbsp;</span></p>
<p><span data-preserver-spaces="true">Equivalently, if the eigenvectors for the endomorphism t span V, then t is represented by a diagonal matrix under the basis formed by the eigenvectors.&nbsp;</span></p>
<p><span data-preserver-spaces="true">We say T is <strong>diagonalizable </strong>if this is the case.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Proof:<br /><img class="equation_image" title=" \begin{aligned}
T P &amp;= T (\vec{\zeta}_1, ..., \vec{\zeta}_n) \\
&amp;= (T \vec{\zeta}_1, ..., T\vec{\zeta}_n) \\
&amp; = (\lambda_1 \vec{\zeta}_1, ..., \lambda_n\vec{\zeta}_n) \\
&amp; = (\vec{\zeta}_1, ..., \vec{\zeta}_n) \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix} \\
&amp;= P \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix}.&nbsp;
\end{aligned}" src="https://canvas.du.edu/equation_images/%2520%255Cbegin%257Baligned%257D%250AT%2520P%2520%2526%253D%2520T%2520(%255Cvec%257B%255Czeta%257D_1%252C%2520...%252C%2520%255Cvec%257B%255Czeta%257D_n)%2520%255C%255C%250A%2526%253D%2520(T%2520%255Cvec%257B%255Czeta%257D_1%252C%2520...%252C%2520T%255Cvec%257B%255Czeta%257D_n)%2520%255C%255C%250A%2526%2520%253D%2520(%255Clambda_1%2520%255Cvec%257B%255Czeta%257D_1%252C%2520...%252C%2520%255Clambda_n%255Cvec%257B%255Czeta%257D_n)%2520%255C%255C%250A%2526%2520%253D%2520(%255Cvec%257B%255Czeta%257D_1%252C%2520...%252C%2520%255Cvec%257B%255Czeta%257D_n)%2520%255Cbegin%257Bpmatrix%257D%2520%255Clambda_1%2520%2526%2520%2526%2520%255C%255C%2520%2526%2520%255Cddots%2520%2526%2520%255C%255C%2520%2526%2520%2526%2520%255Clambda_n%255Cend%257Bpmatrix%257D%2520%255C%255C%250A%2526%253D%2520P%2520%255Cbegin%257Bpmatrix%257D%2520%255Clambda_1%2520%2526%2520%2526%2520%255C%255C%2520%2526%2520%255Cddots%2520%2526%2520%255C%255C%2520%2526%2520%2526%2520%255Clambda_n%255Cend%257Bpmatrix%257D.%25C2%25A0%250A%255Cend%257Baligned%257D?scale=1" alt="LaTeX:  \begin{aligned}
T P &amp;= T (\vec{\zeta}_1, ..., \vec{\zeta}_n) \\
&amp;= (T \vec{\zeta}_1, ..., T\vec{\zeta}_n) \\
&amp; = (\lambda_1 \vec{\zeta}_1, ..., \lambda_n\vec{\zeta}_n) \\
&amp; = (\vec{\zeta}_1, ..., \vec{\zeta}_n) \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix} \\
&amp;= P \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix}.&nbsp;
\end{aligned}" data-equation-content=" \begin{aligned}
T P &amp;= T (\vec{\zeta}_1, ..., \vec{\zeta}_n) \\
&amp;= (T \vec{\zeta}_1, ..., T\vec{\zeta}_n) \\
&amp; = (\lambda_1 \vec{\zeta}_1, ..., \lambda_n\vec{\zeta}_n) \\
&amp; = (\vec{\zeta}_1, ..., \vec{\zeta}_n) \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix} \\
&amp;= P \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n\end{pmatrix}.&nbsp;
\end{aligned}" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">Right multiply both sides by <img class="equation_image" title="P^{-1}" src="https://canvas.du.edu/equation_images/P%255E%257B-1%257D?scale=1" alt="LaTeX: P^{-1}" data-equation-content="P^{-1}" data-ignore-a11y-check="" /> and use associativity of matrix multiplications, we see the result.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Proposition: For each eigenvalue <img class="equation_image" title="\lambda" src="https://canvas.du.edu/equation_images/%255Clambda?scale=1" alt="LaTeX: \lambda" data-equation-content="\lambda" data-ignore-a11y-check="" />, the set of eigenvectors with eigenvalue <img class="equation_image" title=" \lambda " src="https://canvas.du.edu/equation_images/%2520%255Clambda%2520?scale=1" alt="LaTeX:  \lambda " data-equation-content=" \lambda " data-ignore-a11y-check="" /> form a vector space, called the <strong>eigenspace </strong>with respect to <img class="equation_image" title=" \lambda " src="https://canvas.du.edu/equation_images/%2520%255Clambda%2520?scale=1" alt="LaTeX:  \lambda " data-equation-content=" \lambda " data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Proof: Let <img class="equation_image" title="\vec{\zeta}_1, \vec{\zeta}_2" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D_1%252C%2520%255Cvec%257B%255Czeta%257D_2?scale=1" alt="LaTeX: \vec{\zeta}_1, \vec{\zeta}_2" data-equation-content="\vec{\zeta}_1, \vec{\zeta}_2" data-ignore-a11y-check="" /> be two eigenvectors with eigenvalue <img class="equation_image" title=" \lambda " src="https://canvas.du.edu/equation_images/%2520%255Clambda%2520?scale=1" alt="LaTeX:  \lambda " data-equation-content=" \lambda " data-ignore-a11y-check="" />. Then for every <img class="equation_image" title="r_1, r_2 \in \mathbb{R}" src="https://canvas.du.edu/equation_images/r_1%252C%2520r_2%2520%255Cin%2520%255Cmathbb%257BR%257D?scale=1" alt="LaTeX: r_1, r_2 \in \mathbb{R}" data-equation-content="r_1, r_2 \in \mathbb{R}" data-ignore-a11y-check="" />,&nbsp;</span></p>
<p><img class="equation_image" title="T (r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2) =&nbsp;r_1 T(\vec{\zeta}_1) + r_2 T(\vec{\zeta}_2) = r_1 \lambda \vec{\zeta}_1 + r_2 \lambda \vec{\zeta}_2 = \lambda (r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2)" src="https://canvas.du.edu/equation_images/T%2520(r_1%2520%255Cvec%257B%255Czeta%257D_1%2520%252B%2520r_2%2520%255Cvec%257B%255Czeta%257D_2)%2520%253D%25C2%25A0r_1%2520T(%255Cvec%257B%255Czeta%257D_1)%2520%252B%2520r_2%2520T(%255Cvec%257B%255Czeta%257D_2)%2520%253D%2520r_1%2520%255Clambda%2520%255Cvec%257B%255Czeta%257D_1%2520%252B%2520r_2%2520%255Clambda%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Clambda%2520(r_1%2520%255Cvec%257B%255Czeta%257D_1%2520%252B%2520r_2%2520%255Cvec%257B%255Czeta%257D_2)?scale=1" alt="LaTeX: T (r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2) =&nbsp;r_1 T(\vec{\zeta}_1) + r_2 T(\vec{\zeta}_2) = r_1 \lambda \vec{\zeta}_1 + r_2 \lambda \vec{\zeta}_2 = \lambda (r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2)" data-equation-content="T (r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2) =&nbsp;r_1 T(\vec{\zeta}_1) + r_2 T(\vec{\zeta}_2) = r_1 \lambda \vec{\zeta}_1 + r_2 \lambda \vec{\zeta}_2 = \lambda (r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2)" data-ignore-a11y-check="" /></p>
<p>So <img class="equation_image" title="r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2" src="https://canvas.du.edu/equation_images/r_1%2520%255Cvec%257B%255Czeta%257D_1%2520%252B%2520r_2%2520%255Cvec%257B%255Czeta%257D_2?scale=1" alt="LaTeX: r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2" data-equation-content="r_1 \vec{\zeta}_1 + r_2 \vec{\zeta}_2" data-ignore-a11y-check="" /> is also an eigenvector with eigenvalue <span data-preserver-spaces="true"><img class="equation_image" title=" \lambda " src="https://canvas.du.edu/equation_images/%2520%255Clambda%2520?scale=1" alt="LaTeX:  \lambda " data-equation-content=" \lambda " data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Remark: Generally, the sum of two eigenvectors with different eigenvalues is no longer an eigenvector.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">In engineering practices, many matrices have <strong>complex eigenvalues</strong>. Consequently, the eigenvectors are also <strong>complex</strong>.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Recall: <img class="equation_image" title="i = \sqrt{-1}, (a+bi) + (c+di) = (a+c) + (b+d)i, " src="https://canvas.du.edu/equation_images/i%2520%253D%2520%255Csqrt%257B-1%257D%252C%2520(a%252Bbi)%2520%252B%2520(c%252Bdi)%2520%253D%2520(a%252Bc)%2520%252B%2520(b%252Bd)i%252C%2520?scale=1" alt="LaTeX: i = \sqrt{-1}, (a+bi) + (c+di) = (a+c) + (b+d)i, " data-equation-content="i = \sqrt{-1}, (a+bi) + (c+di) = (a+c) + (b+d)i, " data-ignore-a11y-check="" />&nbsp;</span></p>
<p><span data-preserver-spaces="true"><img class="equation_image" title="(a+bi)(c+di) = ac-bd + (ad + bc)i. \overline{a+bi} = a-bi." src="https://canvas.du.edu/equation_images/(a%252Bbi)(c%252Bdi)%2520%253D%2520ac-bd%2520%252B%2520(ad%2520%252B%2520bc)i.%2520%255Coverline%257Ba%252Bbi%257D%2520%253D%2520a-bi.?scale=1" alt="LaTeX: (a+bi)(c+di) = ac-bd + (ad + bc)i. \overline{a+bi} = a-bi." data-equation-content="(a+bi)(c+di) = ac-bd + (ad + bc)i. \overline{a+bi} = a-bi." data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">A <strong>complex vector space</strong> is a vector space with complex scalar multiplication.&nbsp;</span></p>
<p>Example:&nbsp;</p>
<p><img class="equation_image" title="\mathbb{C}^n = \left\{\left.\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n\end{pmatrix} \right| a_1, a_2, ..., a_n \in \mathbb{C}\right\}

= \left\{ \left. a_1 \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} + a_2 \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix} +\cdots + \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}\right| a_1, ..., a_n \in \mathbb{C}\right\}" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BC%257D%255En%2520%253D%2520%255Cleft%255C%257B%255Cleft.%255Cbegin%257Bpmatrix%257D%2520a_1%2520%255C%255C%2520a_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_n%255Cend%257Bpmatrix%257D%2520%255Cright%257C%2520a_1%252C%2520a_2%252C%2520...%252C%2520a_n%2520%255Cin%2520%255Cmathbb%257BC%257D%255Cright%255C%257D%250A%250A%253D%2520%255Cleft%255C%257B%2520%255Cleft.%2520a_1%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%2520%252B%2520a_2%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25201%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%2520%252B%255Ccdots%2520%252B%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25201%255Cend%257Bpmatrix%257D%255Cright%257C%2520a_1%252C%2520...%252C%2520a_n%2520%255Cin%2520%255Cmathbb%257BC%257D%255Cright%255C%257D?scale=1" alt="LaTeX: \mathbb{C}^n = \left\{\left.\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n\end{pmatrix} \right| a_1, a_2, ..., a_n \in \mathbb{C}\right\}

= \left\{ \left. a_1 \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} + a_2 \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix} +\cdots + \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}\right| a_1, ..., a_n \in \mathbb{C}\right\}" data-equation-content="\mathbb{C}^n = \left\{\left.\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n\end{pmatrix} \right| a_1, a_2, ..., a_n \in \mathbb{C}\right\}

= \left\{ \left. a_1 \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} + a_2 \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix} +\cdots + \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}\right| a_1, ..., a_n \in \mathbb{C}\right\}" data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">So <img class="equation_image" title=" \mathcal{E}_n =&nbsp;

&nbsp;\left\langle \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} , \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix}, \cdots, \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}\right\rangle " src="https://canvas.du.edu/equation_images/%2520%255Cmathcal%257BE%257D_n%2520%253D%25C2%25A0%250A%250A%25C2%25A0%255Cleft%255Clangle%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%2520%252C%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25201%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%252C%2520%255Ccdots%252C%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25201%255Cend%257Bpmatrix%257D%255Cright%255Crangle%2520?scale=1" alt="LaTeX:  \mathcal{E}_n =&nbsp;

&nbsp;\left\langle \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} , \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix}, \cdots, \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}\right\rangle " data-equation-content=" \mathcal{E}_n =&nbsp;

&nbsp;\left\langle \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} , \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0\end{pmatrix}, \cdots, \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1\end{pmatrix}\right\rangle " data-ignore-a11y-check="" /></span><span style="color: var(--ic-brand-font-color-dark); font-size: 1rem;">is still a basis for <img class="equation_image" title="\mathbb{C}^n" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BC%257D%255En?scale=1" alt="LaTeX: \mathbb{C}^n" data-equation-content="\mathbb{C}^n" data-ignore-a11y-check="" />, called the standard basis. So <img class="equation_image" title="\dim_\mathbb{C} \mathbb{C}^n = n" src="https://canvas.du.edu/equation_images/%255Cdim_%255Cmathbb%257BC%257D%2520%255Cmathbb%257BC%257D%255En%2520%253D%2520n?scale=1" alt="LaTeX: \dim_\mathbb{C} \mathbb{C}^n = n" data-equation-content="\dim_\mathbb{C} \mathbb{C}^n = n" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">If we view <img class="equation_image" title="\mathbb{C}^n" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BC%257D%255En?scale=1" alt="LaTeX: \mathbb{C}^n" data-equation-content="\mathbb{C}^n" data-ignore-a11y-check="" /> as a real vector space, then <img class="equation_image" title=" \dim_\mathbb{R} \mathbb{C}^n = 2n" src="https://canvas.du.edu/equation_images/%2520%255Cdim_%255Cmathbb%257BR%257D%2520%255Cmathbb%257BC%257D%255En%2520%253D%25202n?scale=1" alt="LaTeX:  \dim_\mathbb{R} \mathbb{C}^n = 2n" data-equation-content=" \dim_\mathbb{R} \mathbb{C}^n = 2n" data-ignore-a11y-check="" />, since.&nbsp;</span></p>
<p><img class="equation_image" title="\begin{aligned}
\mathbb{C}^n &amp;= \left\{\left.\begin{pmatrix} a_1+i b_1 \\ a_2 + ib_2 \\ \vdots \\ a_n + ib_n\end{pmatrix} \right| a_1, b_1, a_2, b_2, ..., a_n, b_n \in \mathbb{R}\right\}\\
&amp;=&nbsp; \left\{ \left. a_1 \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} + b_1 \begin{pmatrix} i \\ 0 \\ \vdots \\ 0\end{pmatrix}+ a_2 \begin{pmatrix} 0 \\ 1 \\\vdots \\ 0\end{pmatrix} +b_2 \begin{pmatrix} 0 \\ i \\ \vdots \\ 0\end{pmatrix} +\cdots + a_n \begin{pmatrix} 0 \\ 0\\ \vdots \\ 1\end{pmatrix} + b_n \begin{pmatrix} 0 \\ 0 \\\vdots \\ i\end{pmatrix} \right| a_1, b_1,  ..., a_n, b_n \in \mathbb{R}\right \}.&nbsp;
\end{aligned}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Baligned%257D%250A%255Cmathbb%257BC%257D%255En%2520%2526%253D%2520%255Cleft%255C%257B%255Cleft.%255Cbegin%257Bpmatrix%257D%2520a_1%252Bi%2520b_1%2520%255C%255C%2520a_2%2520%252B%2520ib_2%2520%255C%255C%2520%255Cvdots%2520%255C%255C%2520a_n%2520%252B%2520ib_n%255Cend%257Bpmatrix%257D%2520%255Cright%257C%2520a_1%252C%2520b_1%252C%2520a_2%252C%2520b_2%252C%2520...%252C%2520a_n%252C%2520b_n%2520%255Cin%2520%255Cmathbb%257BR%257D%255Cright%255C%257D%255C%255C%250A%2526%253D%25C2%25A0%2520%255Cleft%255C%257B%2520%255Cleft.%2520a_1%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%2520%252B%2520b_1%2520%255Cbegin%257Bpmatrix%257D%2520i%2520%255C%255C%25200%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%252B%2520a_2%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25201%2520%255C%255C%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%2520%252Bb_2%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%2520i%2520%255C%255C%2520%255Cvdots%2520%255C%255C%25200%255Cend%257Bpmatrix%257D%2520%252B%255Ccdots%2520%252B%2520a_n%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25200%255C%255C%2520%255Cvdots%2520%255C%255C%25201%255Cend%257Bpmatrix%257D%2520%252B%2520b_n%2520%255Cbegin%257Bpmatrix%257D%25200%2520%255C%255C%25200%2520%255C%255C%255Cvdots%2520%255C%255C%2520i%255Cend%257Bpmatrix%257D%2520%255Cright%257C%2520a_1%252C%2520b_1%252C%2520%2520...%252C%2520a_n%252C%2520b_n%2520%255Cin%2520%255Cmathbb%257BR%257D%255Cright%2520%255C%257D.%25C2%25A0%250A%255Cend%257Baligned%257D?scale=1" alt="LaTeX: \begin{aligned}
\mathbb{C}^n &amp;= \left\{\left.\begin{pmatrix} a_1+i b_1 \\ a_2 + ib_2 \\ \vdots \\ a_n + ib_n\end{pmatrix} \right| a_1, b_1, a_2, b_2, ..., a_n, b_n \in \mathbb{R}\right\}\\
&amp;=&nbsp; \left\{ \left. a_1 \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} + b_1 \begin{pmatrix} i \\ 0 \\ \vdots \\ 0\end{pmatrix}+ a_2 \begin{pmatrix} 0 \\ 1 \\\vdots \\ 0\end{pmatrix} +b_2 \begin{pmatrix} 0 \\ i \\ \vdots \\ 0\end{pmatrix} +\cdots + a_n \begin{pmatrix} 0 \\ 0\\ \vdots \\ 1\end{pmatrix} + b_n \begin{pmatrix} 0 \\ 0 \\\vdots \\ i\end{pmatrix} \right| a_1, b_1,  ..., a_n, b_n \in \mathbb{R}\right \}.&nbsp;
\end{aligned}" data-equation-content="\begin{aligned}
\mathbb{C}^n &amp;= \left\{\left.\begin{pmatrix} a_1+i b_1 \\ a_2 + ib_2 \\ \vdots \\ a_n + ib_n\end{pmatrix} \right| a_1, b_1, a_2, b_2, ..., a_n, b_n \in \mathbb{R}\right\}\\
&amp;=&nbsp; \left\{ \left. a_1 \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix} + b_1 \begin{pmatrix} i \\ 0 \\ \vdots \\ 0\end{pmatrix}+ a_2 \begin{pmatrix} 0 \\ 1 \\\vdots \\ 0\end{pmatrix} +b_2 \begin{pmatrix} 0 \\ i \\ \vdots \\ 0\end{pmatrix} +\cdots + a_n \begin{pmatrix} 0 \\ 0\\ \vdots \\ 1\end{pmatrix} + b_n \begin{pmatrix} 0 \\ 0 \\\vdots \\ i\end{pmatrix} \right| a_1, b_1,  ..., a_n, b_n \in \mathbb{R}\right \}.&nbsp;
\end{aligned}" data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">The most significant advantage of introducing complex numbers is that every polynomial of degree n has n roots (counting multiplicities of the repeated roots). Consequently, an eigenvalue exists for every matrix.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Example:&nbsp;</span></p>
<p>1. <img class="equation_image" title="\begin{pmatrix}

2 &amp; -1\\

1 &amp; 2

\end{pmatrix}.&nbsp;

\lambda_1 = 2 + i, \lambda_2 = 2 - i.&nbsp;

\vec{\zeta}_1 = \begin{pmatrix} 1 \\ i \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ -i\end{pmatrix}. " src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%250A2%2520%2526%2520-1%255C%255C%250A%250A1%2520%2526%25202%250A%250A%255Cend%257Bpmatrix%257D.%25C2%25A0%250A%250A%255Clambda_1%2520%253D%25202%2520%252B%2520i%252C%2520%255Clambda_2%2520%253D%25202%2520-%2520i.%25C2%25A0%250A%250A%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%2520i%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%2520-i%255Cend%257Bpmatrix%257D.%2520?scale=1" alt="LaTeX: \begin{pmatrix}

2 &amp; -1\\

1 &amp; 2

\end{pmatrix}.&nbsp;

\lambda_1 = 2 + i, \lambda_2 = 2 - i.&nbsp;

\vec{\zeta}_1 = \begin{pmatrix} 1 \\ i \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ -i\end{pmatrix}. " data-equation-content="\begin{pmatrix}

2 &amp; -1\\

1 &amp; 2

\end{pmatrix}.&nbsp;

\lambda_1 = 2 + i, \lambda_2 = 2 - i.&nbsp;

\vec{\zeta}_1 = \begin{pmatrix} 1 \\ i \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1 \\ -i\end{pmatrix}. " data-ignore-a11y-check="" /></p>
<p>2. <img class="equation_image" title="\begin{pmatrix}

4 &amp; -5&nbsp; \\

1 &amp; 2

\end{pmatrix}

\lambda_1 = 3 + 2i, \lambda_2 = 3 - 2i.&nbsp;

\vec{\zeta}_1 = \begin{pmatrix} 1-2i \\ 1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1+2i\\ 1\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%250A4%2520%2526%2520-5%25C2%25A0%2520%255C%255C%250A%250A1%2520%2526%25202%250A%250A%255Cend%257Bpmatrix%257D%250A%250A%255Clambda_1%2520%253D%25203%2520%252B%25202i%252C%2520%255Clambda_2%2520%253D%25203%2520-%25202i.%25C2%25A0%250A%250A%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201-2i%2520%255C%255C%25201%2520%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%252B2i%255C%255C%25201%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix}

4 &amp; -5&nbsp; \\

1 &amp; 2

\end{pmatrix}

\lambda_1 = 3 + 2i, \lambda_2 = 3 - 2i.&nbsp;

\vec{\zeta}_1 = \begin{pmatrix} 1-2i \\ 1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1+2i\\ 1\end{pmatrix}" data-equation-content="\begin{pmatrix}

4 &amp; -5&nbsp; \\

1 &amp; 2

\end{pmatrix}

\lambda_1 = 3 + 2i, \lambda_2 = 3 - 2i.&nbsp;

\vec{\zeta}_1 = \begin{pmatrix} 1-2i \\ 1 \end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix} 1+2i\\ 1\end{pmatrix}" data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">Proposition: Let <img class="equation_image" title="T" src="https://canvas.du.edu/equation_images/T?scale=1" alt="LaTeX: T" data-equation-content="T" data-ignore-a11y-check="" /> be a matrix <strong>with real entries</strong>. If <img class="equation_image" title="\alpha + \beta i" src="https://canvas.du.edu/equation_images/%255Calpha%2520%252B%2520%255Cbeta%2520i?scale=1" alt="LaTeX: \alpha + \beta i" data-equation-content="\alpha + \beta i" data-ignore-a11y-check="" /> is an eigenvalue, so is <img class="equation_image" title="\alpha - \beta i" src="https://canvas.du.edu/equation_images/%255Calpha%2520-%2520%255Cbeta%2520i?scale=1" alt="LaTeX: \alpha - \beta i" data-equation-content="\alpha - \beta i" data-ignore-a11y-check="" />. The eigenvectors for these conjugate eigenvalues are conjugate to each other as well.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Proof: Let <img class="equation_image" title="\vec{\zeta}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: \vec{\zeta}" data-equation-content="\vec{\zeta}" data-ignore-a11y-check="" /> be a complex eigenvector of eigenvalue <img class="equation_image" title="\alpha + \beta i" src="https://canvas.du.edu/equation_images/%255Calpha%2520%252B%2520%255Cbeta%2520i?scale=1" alt="LaTeX: \alpha + \beta i" data-equation-content="\alpha + \beta i" data-ignore-a11y-check="" />. Then <img class="equation_image" title="T \vec{\zeta} = (\alpha + \beta i) \vec{\zeta}" src="https://canvas.du.edu/equation_images/T%2520%255Cvec%257B%255Czeta%257D%2520%253D%2520(%255Calpha%2520%252B%2520%255Cbeta%2520i)%2520%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: T \vec{\zeta} = (\alpha + \beta i) \vec{\zeta}" data-equation-content="T \vec{\zeta} = (\alpha + \beta i) \vec{\zeta}" data-ignore-a11y-check="" />. Take the conjugation on both sides, so </span></p>
<p><span data-preserver-spaces="true"><img class="equation_image" title="\overline{T} \overline{\vec{\zeta}} = \overline{(\alpha + \beta i)} \overline{\vec{\zeta}} = (\alpha - \beta i) \overline{\vec{\zeta}}." src="https://canvas.du.edu/equation_images/%255Coverline%257BT%257D%2520%255Coverline%257B%255Cvec%257B%255Czeta%257D%257D%2520%253D%2520%255Coverline%257B(%255Calpha%2520%252B%2520%255Cbeta%2520i)%257D%2520%255Coverline%257B%255Cvec%257B%255Czeta%257D%257D%2520%253D%2520(%255Calpha%2520-%2520%255Cbeta%2520i)%2520%255Coverline%257B%255Cvec%257B%255Czeta%257D%257D.?scale=1" alt="LaTeX: \overline{T} \overline{\vec{\zeta}} = \overline{(\alpha + \beta i)} \overline{\vec{\zeta}} = (\alpha - \beta i) \overline{\vec{\zeta}}." data-equation-content="\overline{T} \overline{\vec{\zeta}} = \overline{(\alpha + \beta i)} \overline{\vec{\zeta}} = (\alpha - \beta i) \overline{\vec{\zeta}}." data-ignore-a11y-check="" /><br />Since T has real entries, <img class="equation_image" title="\overline{T} = T" src="https://canvas.du.edu/equation_images/%255Coverline%257BT%257D%2520%253D%2520T?scale=1" alt="LaTeX: \overline{T} = T" data-equation-content="\overline{T} = T" data-ignore-a11y-check="" />. So <img class="equation_image" title=" \overline{\vec{\zeta}} " src="https://canvas.du.edu/equation_images/%2520%255Coverline%257B%255Cvec%257B%255Czeta%257D%257D%2520?scale=1" alt="LaTeX:  \overline{\vec{\zeta}} " data-equation-content=" \overline{\vec{\zeta}} " data-ignore-a11y-check="" /> is an eigenvector with eigenvalue <img class="equation_image" title="\alpha - \beta i" src="https://canvas.du.edu/equation_images/%255Calpha%2520-%2520%255Cbeta%2520i?scale=1" alt="LaTeX: \alpha - \beta i" data-equation-content="\alpha - \beta i" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Example:&nbsp;</span></p>
<p>3. <img class="equation_image" title="\begin{pmatrix}-1&amp;8&amp;2\\ -3&amp;8&amp;3\\ 3&amp;-7&amp;-2\end{pmatrix}.  \lambda_1 = 2+3i, \lambda_2 = 2-3i, \lambda_3 = 1." src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D-1%25268%25262%255C%255C%2520-3%25268%25263%255C%255C%25203%2526-7%2526-2%255Cend%257Bpmatrix%257D.%2520%2520%255Clambda_1%2520%253D%25202%252B3i%252C%2520%255Clambda_2%2520%253D%25202-3i%252C%2520%255Clambda_3%2520%253D%25201.?scale=1" alt="LaTeX: \begin{pmatrix}-1&amp;8&amp;2\\ -3&amp;8&amp;3\\ 3&amp;-7&amp;-2\end{pmatrix}.  \lambda_1 = 2+3i, \lambda_2 = 2-3i, \lambda_3 = 1." data-equation-content="\begin{pmatrix}-1&amp;8&amp;2\\ -3&amp;8&amp;3\\ 3&amp;-7&amp;-2\end{pmatrix}.  \lambda_1 = 2+3i, \lambda_2 = 2-3i, \lambda_3 = 1." data-ignore-a11y-check="" /></p>
<p><img class="equation_image" title=" \vec{\zeta}_1 = \begin{pmatrix}-1+i\\-1\\ 1\end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix}-1-i\\-1\\ 1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix}1\\0\\ 1\end{pmatrix}" src="https://canvas.du.edu/equation_images/%2520%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cbegin%257Bpmatrix%257D-1%252Bi%255C%255C-1%255C%255C%25201%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cbegin%257Bpmatrix%257D-1-i%255C%255C-1%255C%255C%25201%255Cend%257Bpmatrix%257D%252C%2520%255Cvec%257B%255Czeta%257D_3%2520%253D%2520%255Cbegin%257Bpmatrix%257D1%255C%255C0%255C%255C%25201%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX:  \vec{\zeta}_1 = \begin{pmatrix}-1+i\\-1\\ 1\end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix}-1-i\\-1\\ 1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix}1\\0\\ 1\end{pmatrix}" data-equation-content=" \vec{\zeta}_1 = \begin{pmatrix}-1+i\\-1\\ 1\end{pmatrix}, \vec{\zeta}_2 = \begin{pmatrix}-1-i\\-1\\ 1\end{pmatrix}, \vec{\zeta}_3 = \begin{pmatrix}1\\0\\ 1\end{pmatrix}" data-ignore-a11y-check="" /></p>
<p>&nbsp;</p>
<p><strong>Not every matrix is diagonalizable.&nbsp;</strong></p>
<p><span data-preserver-spaces="true">Example: <img class="equation_image" title="\begin{pmatrix}

1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%250A1%2520%2526%25201%2520%255C%255C%25200%2520%2526%25201%2520%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: \begin{pmatrix}

1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.&nbsp;" data-equation-content="\begin{pmatrix}

1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">Clearly, the eigenvalue is 1 with multiplicity 2. But the eigenspace is one-dimensional, spanned by <img class="equation_image" title="\begin{pmatrix} 1 \\ 0\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25200%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix} 1 \\ 0\end{pmatrix}" data-equation-content="\begin{pmatrix} 1 \\ 0\end{pmatrix}" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Remark: This only happens when we have repeated eigenvalues.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Proposition: If a matrix T does not have repeated eigenvalues, then T is diagonalizable.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Proof: Shown by induction. I only show the case where n=2.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Suppose <img class="equation_image" title=" \lambda_1 " src="https://canvas.du.edu/equation_images/%2520%255Clambda_1%2520?scale=1" alt="LaTeX:  \lambda_1 " data-equation-content=" \lambda_1 " data-ignore-a11y-check="" /> and <img class="equation_image" title="\lambda_2" src="https://canvas.du.edu/equation_images/%255Clambda_2?scale=1" alt="LaTeX: \lambda_2" data-equation-content="\lambda_2" data-ignore-a11y-check="" /> are eigenvalues, <img class="equation_image" title="\vec{\zeta}_1, \vec{\zeta}_2" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D_1%252C%2520%255Cvec%257B%255Czeta%257D_2?scale=1" alt="LaTeX: \vec{\zeta}_1, \vec{\zeta}_2" data-equation-content="\vec{\zeta}_1, \vec{\zeta}_2" data-ignore-a11y-check="" /> are eigenvectors. It suffices to show that <img class="equation_image" title="\vec{\zeta}_1, \vec{\zeta}_2" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D_1%252C%2520%255Cvec%257B%255Czeta%257D_2?scale=1" alt="LaTeX: \vec{\zeta}_1, \vec{\zeta}_2" data-equation-content="\vec{\zeta}_1, \vec{\zeta}_2" data-ignore-a11y-check="" /> are linearly independent.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Suppose <img class="equation_image" title="c_1 \vec{\zeta}_1+ c_2 \vec{\zeta}_2 = \vec{0}" src="https://canvas.du.edu/equation_images/c_1%2520%255Cvec%257B%255Czeta%257D_1%252B%2520c_2%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: c_1 \vec{\zeta}_1+ c_2 \vec{\zeta}_2 = \vec{0}" data-equation-content="c_1 \vec{\zeta}_1+ c_2 \vec{\zeta}_2 = \vec{0}" data-ignore-a11y-check="" />. </span><span data-preserver-spaces="true">Apply T on both sides, we see that </span><img class="equation_image" title="c_1 \lambda_1 \vec{\zeta}_1+ c_2 \lambda_2 \vec{\zeta}_2 = \vec{0}." src="https://canvas.du.edu/equation_images/c_1%2520%255Clambda_1%2520%255Cvec%257B%255Czeta%257D_1%252B%2520c_2%2520%255Clambda_2%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cvec%257B0%257D.?scale=1" alt="LaTeX: c_1 \lambda_1 \vec{\zeta}_1+ c_2 \lambda_2 \vec{\zeta}_2 = \vec{0}." data-equation-content="c_1 \lambda_1 \vec{\zeta}_1+ c_2 \lambda_2 \vec{\zeta}_2 = \vec{0}." data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">On the other hand, <img class="equation_image" title="c_1 \lambda_1 \vec{\zeta}_1+ c_2 \lambda_1 \vec{\zeta}_2 = \vec{0}." src="https://canvas.du.edu/equation_images/c_1%2520%255Clambda_1%2520%255Cvec%257B%255Czeta%257D_1%252B%2520c_2%2520%255Clambda_1%2520%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cvec%257B0%257D.?scale=1" alt="LaTeX: c_1 \lambda_1 \vec{\zeta}_1+ c_2 \lambda_1 \vec{\zeta}_2 = \vec{0}." data-equation-content="c_1 \lambda_1 \vec{\zeta}_1+ c_2 \lambda_1 \vec{\zeta}_2 = \vec{0}." data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">Subtraction yields <img class="equation_image" title="c_2 (\lambda_2 - \lambda_1)\vec{\zeta}_2 = \vec{0}" src="https://canvas.du.edu/equation_images/c_2%2520(%255Clambda_2%2520-%2520%255Clambda_1)%255Cvec%257B%255Czeta%257D_2%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: c_2 (\lambda_2 - \lambda_1)\vec{\zeta}_2 = \vec{0}" data-equation-content="c_2 (\lambda_2 - \lambda_1)\vec{\zeta}_2 = \vec{0}" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">Since <img class="equation_image" title="\vec{\zeta}_2 \neq \vec{0}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D_2%2520%255Cneq%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: \vec{\zeta}_2 \neq \vec{0}" data-equation-content="\vec{\zeta}_2 \neq \vec{0}" data-ignore-a11y-check="" />,<img class="equation_image" title=" (\lambda_2 - \lambda_1)\neq 0" src="https://canvas.du.edu/equation_images/%2520(%255Clambda_2%2520-%2520%255Clambda_1)%255Cneq%25200?scale=1" alt="LaTeX:  (\lambda_2 - \lambda_1)\neq 0" data-equation-content=" (\lambda_2 - \lambda_1)\neq 0" data-ignore-a11y-check="" />, it forces <img class="equation_image" title="c_2 = 0" src="https://canvas.du.edu/equation_images/c_2%2520%253D%25200?scale=1" alt="LaTeX: c_2 = 0" data-equation-content="c_2 = 0" data-ignore-a11y-check="" />. thus <img class="equation_image" title="c_2 = 0" src="https://canvas.du.edu/equation_images/c_2%2520%253D%25200?scale=1" alt="LaTeX: c_2 = 0" data-equation-content="c_2 = 0" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Definition: A linear transformation <img class="equation_image" title="t" src="https://canvas.du.edu/equation_images/t?scale=1" alt="LaTeX: t" data-equation-content="t" data-ignore-a11y-check="" /> has a generalized eigenvector <img class="equation_image" title="\vec{\zeta}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: \vec{\zeta}" data-equation-content="\vec{\zeta}" data-ignore-a11y-check="" /> with respect to an eigenvalue<img class="equation_image" title=" \lambda \in \mathbb{C}" src="https://canvas.du.edu/equation_images/%2520%255Clambda%2520%255Cin%2520%255Cmathbb%257BC%257D?scale=1" alt="LaTeX:  \lambda \in \mathbb{C}" data-equation-content=" \lambda \in \mathbb{C}" data-ignore-a11y-check="" />, if <img class="equation_image" title="(t - \lambda \cdot \text{id})^n \vec{\zeta} = 0" src="https://canvas.du.edu/equation_images/(t%2520-%2520%255Clambda%2520%255Ccdot%2520%255Ctext%257Bid%257D)%255En%2520%255Cvec%257B%255Czeta%257D%2520%253D%25200?scale=1" alt="LaTeX: (t - \lambda \cdot \text{id})^n \vec{\zeta} = 0" data-equation-content="(t - \lambda \cdot \text{id})^n \vec{\zeta} = 0" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">A matrix <img class="equation_image" title="T" src="https://canvas.du.edu/equation_images/T?scale=1" alt="LaTeX: T" data-equation-content="T" data-ignore-a11y-check="" /> of size <img class="equation_image" title="n\times n" src="https://canvas.du.edu/equation_images/n%255Ctimes%2520n?scale=1" alt="LaTeX: n\times n" data-equation-content="n\times n" data-ignore-a11y-check="" /> has a generalized eigenvector <img class="equation_image" title="\vec{\zeta}\in \mathbb{C}^n" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D%255Cin%2520%255Cmathbb%257BC%257D%255En?scale=1" alt="LaTeX: \vec{\zeta}\in \mathbb{C}^n" data-equation-content="\vec{\zeta}\in \mathbb{C}^n" data-ignore-a11y-check="" /> with respect to the eigenvalue <img class="equation_image" title="\lambda \in \mathbb{C}" src="https://canvas.du.edu/equation_images/%255Clambda%2520%255Cin%2520%255Cmathbb%257BC%257D?scale=1" alt="LaTeX: \lambda \in \mathbb{C}" data-equation-content="\lambda \in \mathbb{C}" data-ignore-a11y-check="" />, if <img class="equation_image" title="(T - \lambda I)^n \vec{\zeta} = \vec{0}" src="https://canvas.du.edu/equation_images/(T%2520-%2520%255Clambda%2520I)%255En%2520%255Cvec%257B%255Czeta%257D%2520%253D%2520%255Cvec%257B0%257D?scale=1" alt="LaTeX: (T - \lambda I)^n \vec{\zeta} = \vec{0}" data-equation-content="(T - \lambda I)^n \vec{\zeta} = \vec{0}" data-ignore-a11y-check="" /> for some <img class="equation_image" title="n \geq 1" src="https://canvas.du.edu/equation_images/n%2520%255Cgeq%25201?scale=1" alt="LaTeX: n \geq 1" data-equation-content="n \geq 1" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Example: <img class="equation_image" title="T = \begin{pmatrix}

1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.&nbsp;" src="https://canvas.du.edu/equation_images/T%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A%250A1%2520%2526%25201%2520%255C%255C%25200%2520%2526%25201%2520%255Cend%257Bpmatrix%257D.%25C2%25A0?scale=1" alt="LaTeX: T = \begin{pmatrix}

1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.&nbsp;" data-equation-content="T = \begin{pmatrix}

1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}.&nbsp;" data-ignore-a11y-check="" /> </span><span data-preserver-spaces="true">Then <img class="equation_image" title="(T-I)^2 = \begin{pmatrix}

0 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}^2 = \begin{pmatrix}

0 &amp; 0 \\ 0 &amp; 0 \end{pmatrix}" src="https://canvas.du.edu/equation_images/(T-I)%255E2%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A%250A0%2520%2526%25200%2520%255C%255C%25200%2520%2526%25201%2520%255Cend%257Bpmatrix%257D%255E2%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A%250A0%2520%2526%25200%2520%255C%255C%25200%2520%2526%25200%2520%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: (T-I)^2 = \begin{pmatrix}

0 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}^2 = \begin{pmatrix}

0 &amp; 0 \\ 0 &amp; 0 \end{pmatrix}" data-equation-content="(T-I)^2 = \begin{pmatrix}

0 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}^2 = \begin{pmatrix}

0 &amp; 0 \\ 0 &amp; 0 \end{pmatrix}" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">So <img class="equation_image" title="(T-I)^2" src="https://canvas.du.edu/equation_images/(T-I)%255E2?scale=1" alt="LaTeX: (T-I)^2" data-equation-content="(T-I)^2" data-ignore-a11y-check="" /> annihilates every vector in <img class="equation_image" title="\mathbb{C}^2" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BC%257D%255E2?scale=1" alt="LaTeX: \mathbb{C}^2" data-equation-content="\mathbb{C}^2" data-ignore-a11y-check="" />. So every vector in <img class="equation_image" title="\mathbb{C}^2" src="https://canvas.du.edu/equation_images/%255Cmathbb%257BC%257D%255E2?scale=1" alt="LaTeX: \mathbb{C}^2" data-equation-content="\mathbb{C}^2" data-ignore-a11y-check="" /> is a generalized eigenvector.&nbsp;</span></p>
<p>&nbsp;</p>
<p><span data-preserver-spaces="true">Proposition: Generalized eigenvectors with respect to an eigenvalue <img class="equation_image" title="\lambda\in \mathbb{C}" src="https://canvas.du.edu/equation_images/%255Clambda%255Cin%2520%255Cmathbb%257BC%257D?scale=1" alt="LaTeX: \lambda\in \mathbb{C}" data-equation-content="\lambda\in \mathbb{C}" data-ignore-a11y-check="" /> form a vector subspace of <img class="equation_image" title="V" src="https://canvas.du.edu/equation_images/V?scale=1" alt="LaTeX: V" data-equation-content="V" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Theorem: Let V be a complex vector space. Let <img class="equation_image" title="t: V \to V" src="https://canvas.du.edu/equation_images/t%253A%2520V%2520%255Cto%2520V?scale=1" alt="LaTeX: t: V \to V" data-equation-content="t: V \to V" data-ignore-a11y-check="" /> be a linear transformation.&nbsp;</span></p>
<ol>
    <li><span data-preserver-spaces="true">V is spanned by generalized eigenvectors.&nbsp;</span></li>
    <li><span data-preserver-spaces="true">There exists a basis consisting of generalized eigenvectors, such that <img class="equation_image" title="t" src="https://canvas.du.edu/equation_images/t?scale=1" alt="LaTeX: t" data-equation-content="t" data-ignore-a11y-check="" /> can be represented by a block-matrix of the form&nbsp;</span></li>
</ol>
<p><img class="equation_image" title="J = \begin{pmatrix}

J(\lambda_1, n_1) &amp; &amp; \\

&amp; \ddots&amp; \\

&amp; &amp; J(\lambda_k, n_k)

\end{pmatrix}" src="https://canvas.du.edu/equation_images/J%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A%250AJ(%255Clambda_1%252C%2520n_1)%2520%2526%2520%2526%2520%255C%255C%250A%250A%2526%2520%255Cddots%2526%2520%255C%255C%250A%250A%2526%2520%2526%2520J(%255Clambda_k%252C%2520n_k)%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: J = \begin{pmatrix}

J(\lambda_1, n_1) &amp; &amp; \\

&amp; \ddots&amp; \\

&amp; &amp; J(\lambda_k, n_k)

\end{pmatrix}" data-equation-content="J = \begin{pmatrix}

J(\lambda_1, n_1) &amp; &amp; \\

&amp; \ddots&amp; \\

&amp; &amp; J(\lambda_k, n_k)

\end{pmatrix}" data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">where&nbsp;</span></p>
<p><img class="equation_image" title="J(\lambda_i, n_i) = \begin{pmatrix}

\lambda_i &amp; 1 &amp; &amp; &amp;\\

&amp; \lambda_i &amp; 1 &amp; &amp; \\

&amp; &amp; \ddots &amp; \ddots&amp;\\

&amp; &amp; &amp; \lambda_i &amp; 1\\

&amp; &amp; &amp; &amp; \lambda_i

\end{pmatrix}" src="https://canvas.du.edu/equation_images/J(%255Clambda_i%252C%2520n_i)%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A%250A%255Clambda_i%2520%2526%25201%2520%2526%2520%2526%2520%2526%255C%255C%250A%250A%2526%2520%255Clambda_i%2520%2526%25201%2520%2526%2520%2526%2520%255C%255C%250A%250A%2526%2520%2526%2520%255Cddots%2520%2526%2520%255Cddots%2526%255C%255C%250A%250A%2526%2520%2526%2520%2526%2520%255Clambda_i%2520%2526%25201%255C%255C%250A%250A%2526%2520%2526%2520%2526%2520%2526%2520%255Clambda_i%250A%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: J(\lambda_i, n_i) = \begin{pmatrix}

\lambda_i &amp; 1 &amp; &amp; &amp;\\

&amp; \lambda_i &amp; 1 &amp; &amp; \\

&amp; &amp; \ddots &amp; \ddots&amp;\\

&amp; &amp; &amp; \lambda_i &amp; 1\\

&amp; &amp; &amp; &amp; \lambda_i

\end{pmatrix}" data-equation-content="J(\lambda_i, n_i) = \begin{pmatrix}

\lambda_i &amp; 1 &amp; &amp; &amp;\\

&amp; \lambda_i &amp; 1 &amp; &amp; \\

&amp; &amp; \ddots &amp; \ddots&amp;\\

&amp; &amp; &amp; \lambda_i &amp; 1\\

&amp; &amp; &amp; &amp; \lambda_i

\end{pmatrix}" data-ignore-a11y-check="" /></p>
<p><span data-preserver-spaces="true">is a Jordan block of size <img class="equation_image" title="n_i \times n_i" src="https://canvas.du.edu/equation_images/n_i%2520%255Ctimes%2520n_i?scale=1" alt="LaTeX: n_i \times n_i" data-equation-content="n_i \times n_i" data-ignore-a11y-check="" />, with diagonal entries all being <img class="equation_image" title="\lambda_i" src="https://canvas.du.edu/equation_images/%255Clambda_i?scale=1" alt="LaTeX: \lambda_i" data-equation-content="\lambda_i" data-ignore-a11y-check="" /> and the first superdiagonal entries all being 1. The matrix <img class="equation_image" title="J" src="https://canvas.du.edu/equation_images/J?scale=1" alt="LaTeX: J" data-equation-content="J" data-ignore-a11y-check="" /> is called the Jordan canonical form of the endomorphism <img class="equation_image" title="t" src="https://canvas.du.edu/equation_images/t?scale=1" alt="LaTeX: t" data-equation-content="t" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">3. Every matrix <img class="equation_image" title="T" src="https://canvas.du.edu/equation_images/T?scale=1" alt="LaTeX: T" data-equation-content="T" data-ignore-a11y-check="" /> is similar to its Jordan canonical form. Two matrices are similar if they have the same Jordan canonical forms.&nbsp;</span></p>
<p><strong><span data-preserver-spaces="true">Note:</span></strong><span data-preserver-spaces="true">&nbsp;The Hefferon textbook defines the Jordan block as a lower triangular matrix, while the upper triangular version is more common in engineering practices.&nbsp;</span></p>
<p><span data-preserver-spaces="true">For <img class="equation_image" title="2\times 2" src="https://canvas.du.edu/equation_images/2%255Ctimes%25202?scale=1" alt="LaTeX: 2\times 2" data-equation-content="2\times 2" data-ignore-a11y-check="" /> matrices, here are all the possible Jordan canonical forms:</span></p>
<p><span data-preserver-spaces="true"><img class="equation_image" title="\begin{pmatrix}
\lambda_1 &amp; 0 \\
0 &amp;\lambda_2
\end{pmatrix}, \lambda_1, \lambda_2\in \mathbb{C};
\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp;\lambda
\end{pmatrix}, \lambda\in \mathbb{C}. 

" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%255Clambda_1%2520%2526%25200%2520%255C%255C%250A0%2520%2526%255Clambda_2%250A%255Cend%257Bpmatrix%257D%252C%2520%255Clambda_1%252C%2520%255Clambda_2%255Cin%2520%255Cmathbb%257BC%257D%253B%250A%255Cbegin%257Bpmatrix%257D%250A%255Clambda%2520%2526%25201%2520%255C%255C%250A0%2520%2526%255Clambda%250A%255Cend%257Bpmatrix%257D%252C%2520%255Clambda%255Cin%2520%255Cmathbb%257BC%257D.%2520%250A%250A?scale=1" alt="LaTeX: \begin{pmatrix}
\lambda_1 &amp; 0 \\
0 &amp;\lambda_2
\end{pmatrix}, \lambda_1, \lambda_2\in \mathbb{C};
\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp;\lambda
\end{pmatrix}, \lambda\in \mathbb{C}. 

" data-equation-content="\begin{pmatrix}
\lambda_1 &amp; 0 \\
0 &amp;\lambda_2
\end{pmatrix}, \lambda_1, \lambda_2\in \mathbb{C};
\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp;\lambda
\end{pmatrix}, \lambda\in \mathbb{C}. 

" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">Let T be a <img class="equation_image" title="2\times 2" src="https://canvas.du.edu/equation_images/2%255Ctimes%25202?scale=1" alt="LaTeX: 2\times 2" data-equation-content="2\times 2" data-ignore-a11y-check="" /> matrix. The Jordan canonical form and the transformation matrix P can be obtained by the following procedure: </span></p>
<p><span data-preserver-spaces="true">1. Find the eigenvalues and solve the eigenvectors. If the eigenvalues are distinct, or the eigenvalues are repeated with 2-dimensional eigenspace, the Jordan canonical form is <img class="equation_image" title="\begin{pmatrix}
\lambda_1 &amp; 0 \\
0 &amp;\lambda_2
\end{pmatrix}

" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%255Clambda_1%2520%2526%25200%2520%255C%255C%250A0%2520%2526%255Clambda_2%250A%255Cend%257Bpmatrix%257D%250A%250A?scale=1" alt="LaTeX: \begin{pmatrix}
\lambda_1 &amp; 0 \\
0 &amp;\lambda_2
\end{pmatrix}

" data-equation-content="\begin{pmatrix}
\lambda_1 &amp; 0 \\
0 &amp;\lambda_2
\end{pmatrix}

" data-ignore-a11y-check="" />. The change of basis matrix is simply given by the eigenvectors.&nbsp;</span></p>
<p><span data-preserver-spaces="true">2. If the eigenvalues are repeated <img class="equation_image" title="\lambda" src="https://canvas.du.edu/equation_images/%255Clambda?scale=1" alt="LaTeX: \lambda" data-equation-content="\lambda" data-ignore-a11y-check="" /> but the eigenspace is only one-dimension spanned by <img class="equation_image" title="\vec{\zeta}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: \vec{\zeta}" data-equation-content="\vec{\zeta}" data-ignore-a11y-check="" />, then the Jordan canonical form is <img class="equation_image" title="\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp;\lambda
\end{pmatrix}.

" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A%255Clambda%2520%2526%25201%2520%255C%255C%250A0%2520%2526%255Clambda%250A%255Cend%257Bpmatrix%257D.%250A%250A?scale=1" alt="LaTeX: \begin{pmatrix}
\lambda &amp; 1 \\
0 &amp;\lambda
\end{pmatrix}.

" data-equation-content="\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp;\lambda
\end{pmatrix}.

" data-ignore-a11y-check="" /> We compute </span><span data-preserver-spaces="true">a generalized eigenvector by solving <img class="equation_image" title="(T-\lambda I) \vec{\zeta}_1 = \vec{\zeta}" src="https://canvas.du.edu/equation_images/(T-%255Clambda%2520I)%2520%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cvec%257B%255Czeta%257D?scale=1" alt="LaTeX: (T-\lambda I) \vec{\zeta}_1 = \vec{\zeta}" data-equation-content="(T-\lambda I) \vec{\zeta}_1 = \vec{\zeta}" data-ignore-a11y-check="" />. Any particular solution of <img class="equation_image" title="\vec{\zeta}_1" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D_1?scale=1" alt="LaTeX: \vec{\zeta}_1" data-equation-content="\vec{\zeta}_1" data-ignore-a11y-check="" /> serves as a generalized eigenvector. The change of basis matrix is simply <img class="equation_image" title="\begin{pmatrix} \vec{\zeta} &amp; \vec{\zeta}_1\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%2520%255Cvec%257B%255Czeta%257D%2520%2526%2520%255Cvec%257B%255Czeta%257D_1%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \begin{pmatrix} \vec{\zeta} &amp; \vec{\zeta}_1\end{pmatrix}" data-equation-content="\begin{pmatrix} \vec{\zeta} &amp; \vec{\zeta}_1\end{pmatrix}" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Example: <img class="equation_image" title="\begin{pmatrix}
2 &amp; 1 \\
-1 &amp; 4
\end{pmatrix}, \lambda_1 = \lambda_2 = 3, \vec{\zeta} = \begin{pmatrix} 1 \\ 1\end{pmatrix}. " src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A2%2520%2526%25201%2520%255C%255C%250A-1%2520%2526%25204%250A%255Cend%257Bpmatrix%257D%252C%2520%255Clambda_1%2520%253D%2520%255Clambda_2%2520%253D%25203%252C%2520%255Cvec%257B%255Czeta%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%255C%255C%25201%255Cend%257Bpmatrix%257D.%2520?scale=1" alt="LaTeX: \begin{pmatrix}
2 &amp; 1 \\
-1 &amp; 4
\end{pmatrix}, \lambda_1 = \lambda_2 = 3, \vec{\zeta} = \begin{pmatrix} 1 \\ 1\end{pmatrix}. " data-equation-content="\begin{pmatrix}
2 &amp; 1 \\
-1 &amp; 4
\end{pmatrix}, \lambda_1 = \lambda_2 = 3, \vec{\zeta} = \begin{pmatrix} 1 \\ 1\end{pmatrix}. " data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">We solve the system <img class="equation_image" title="\begin{pmatrix}
2-3 &amp; 1  &amp;|&amp; 1\\
-1 &amp; 4-3 &amp;|&amp; 1
\end{pmatrix} = \begin{pmatrix}
-1 &amp; 1  &amp;|&amp; 1\\
-1 &amp; 1 &amp;|&amp; 1
\end{pmatrix}\Rightarrow -x_1 + x_2 = 1. " src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A2-3%2520%2526%25201%2520%2520%2526%257C%2526%25201%255C%255C%250A-1%2520%2526%25204-3%2520%2526%257C%2526%25201%250A%255Cend%257Bpmatrix%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A-1%2520%2526%25201%2520%2520%2526%257C%2526%25201%255C%255C%250A-1%2520%2526%25201%2520%2526%257C%2526%25201%250A%255Cend%257Bpmatrix%257D%255CRightarrow%2520-x_1%2520%252B%2520x_2%2520%253D%25201.%2520?scale=1" alt="LaTeX: \begin{pmatrix}
2-3 &amp; 1  &amp;|&amp; 1\\
-1 &amp; 4-3 &amp;|&amp; 1
\end{pmatrix} = \begin{pmatrix}
-1 &amp; 1  &amp;|&amp; 1\\
-1 &amp; 1 &amp;|&amp; 1
\end{pmatrix}\Rightarrow -x_1 + x_2 = 1. " data-equation-content="\begin{pmatrix}
2-3 &amp; 1  &amp;|&amp; 1\\
-1 &amp; 4-3 &amp;|&amp; 1
\end{pmatrix} = \begin{pmatrix}
-1 &amp; 1  &amp;|&amp; 1\\
-1 &amp; 1 &amp;|&amp; 1
\end{pmatrix}\Rightarrow -x_1 + x_2 = 1. " data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true">Pick <img class="equation_image" title="x_1 = 0" src="https://canvas.du.edu/equation_images/x_1%2520%253D%25200?scale=1" alt="LaTeX: x_1 = 0" data-equation-content="x_1 = 0" data-ignore-a11y-check="" />. Then <img class="equation_image" title="x_2 = 1" src="https://canvas.du.edu/equation_images/x_2%2520%253D%25201?scale=1" alt="LaTeX: x_2 = 1" data-equation-content="x_2 = 1" data-ignore-a11y-check="" />. So a particular solution is precisely <img class="equation_image" title="\vec{\zeta}_1 = \begin{pmatrix}
0 \\ 1
\end{pmatrix}" src="https://canvas.du.edu/equation_images/%255Cvec%257B%255Czeta%257D_1%2520%253D%2520%255Cbegin%257Bpmatrix%257D%250A0%2520%255C%255C%25201%250A%255Cend%257Bpmatrix%257D?scale=1" alt="LaTeX: \vec{\zeta}_1 = \begin{pmatrix}
0 \\ 1
\end{pmatrix}" data-equation-content="\vec{\zeta}_1 = \begin{pmatrix}
0 \\ 1
\end{pmatrix}" data-ignore-a11y-check="" />.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Then we have <img class="equation_image" title="\begin{pmatrix}
2 &amp; 1 \\
-1 &amp; 4
\end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 1\end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 0 &amp; 3\end{pmatrix} \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 1\end{pmatrix}^{-1}" src="https://canvas.du.edu/equation_images/%255Cbegin%257Bpmatrix%257D%250A2%2520%2526%25201%2520%255C%255C%250A-1%2520%2526%25204%250A%255Cend%257Bpmatrix%257D%2520%253D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%2526%25200%2520%255C%255C%25201%2520%2526%25201%255Cend%257Bpmatrix%257D%2520%255Cbegin%257Bpmatrix%257D%25203%2520%2526%25201%2520%255C%255C%25200%2520%2526%25203%255Cend%257Bpmatrix%257D%2520%255Cbegin%257Bpmatrix%257D%25201%2520%2526%25200%2520%255C%255C%25201%2520%2526%25201%255Cend%257Bpmatrix%257D%255E%257B-1%257D?scale=1" alt="LaTeX: \begin{pmatrix}
2 &amp; 1 \\
-1 &amp; 4
\end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 1\end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 0 &amp; 3\end{pmatrix} \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 1\end{pmatrix}^{-1}" data-equation-content="\begin{pmatrix}
2 &amp; 1 \\
-1 &amp; 4
\end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 1\end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 0 &amp; 3\end{pmatrix} \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 1\end{pmatrix}^{-1}" data-ignore-a11y-check="" /></span></p>
<p><span data-preserver-spaces="true"><strong>Note:</strong> We will not require the computation of Jordan canonical forms for matrices with larger sizes in this course. The above procedure does not admit a straightforward generalization to larger size matrices and needs significant modification. The textbook contains an alternative algorithm that is slightly more complicated, but applies to all possible cases.&nbsp;</span></p>
        </ul></details>
    
    
</body>
</html>
